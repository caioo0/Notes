{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f3514b",
   "metadata": {},
   "source": [
    "# 第4章　RDD编程 :id=cp_4\n",
    "---\n",
    "\n",
    "## 本章学习内容：\n",
    "\n",
    "1、RDD的创建方法、各种操作APP以及持久化和分区方法；\n",
    "2、RDD的各种操作\n",
    "3、RDD编程案例实现\n",
    "\n",
    "**RDD是什么？**\n",
    "\n",
    "- Spark核心概念\n",
    "- 一个只读的、可分区的分布式数据集\n",
    "- 可全部或部分缓存在内存中，在多次计算间重用。\n",
    "- RDD API 采用scala语言实现\n",
    "\n",
    "## RDD编程基础 \n",
    "\n",
    "本节介绍RDD编程的基础知识、包括RDD的创建、操作、API、持久化和分区等。\n",
    "\n",
    "### RDD 创建\n",
    "\n",
    "1. 从文件系统中加载数据创建RDD\n",
    "\n",
    "1) 本地文件系统加载数据\n",
    "\n",
    "- pyspark交互式环境中，执行如下命令\n",
    "\n",
    "```\n",
    ">>> lines = sc.textFile(\"file:///opt/spark/data/word.txt\")\n",
    ">>> lines.foreach(print)\n",
    "Spark is better                                                     (0 + 2) / 2]\n",
    "Hadoop is good \n",
    "Spark is fast \n",
    "\n",
    "```\n",
    "在上述语句中，使用了 Spark 提供的 `SparkContext` 对象，名称为 `sc`，这是 `pyspark` 启动的时候自\n",
    "动创建的，在交互式编程环境中可以直接使用。如果是编写独立应用程序，则可以通过如下语句生成 `SparkContext` 对象：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
