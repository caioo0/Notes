{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5d4030",
   "metadata": {},
   "source": [
    "# 第4章　RDD编程 :id=cp_4\n",
    "---\n",
    "\n",
    "## 本章学习内容：\n",
    "\n",
    "1、RDD的创建方法、各种操作APP以及持久化和分区方法；\n",
    "2、RDD的各种操作\n",
    "3、RDD编程案例实现\n",
    "\n",
    "**RDD是什么？**\n",
    "\n",
    "- Spark核心概念\n",
    "- 一个只读的、可分区的分布式数据集\n",
    "- 可全部或部分缓存在内存中，在多次计算间重用。\n",
    "- RDD API 采用scala语言实现\n",
    "\n",
    "## RDD编程基础 \n",
    "\n",
    "本节介绍RDD编程的基础知识、包括RDD的创建、操作、API、持久化和分区等。\n",
    "\n",
    "### RDD 创建\n",
    "\n",
    "1. 从文件系统中加载数据创建RDD\n",
    "\n",
    "1) 本地文件系统加载数据\n",
    "\n",
    "- pyspark交互式环境中，执行如下命令\n",
    "\n",
    "```\n",
    ">>> lines = sc.textFile(\"file:///opt/spark/data/word.txt\")\n",
    ">>> lines.foreach(print)\n",
    "Spark is better                                                     (0 + 2) / 2]\n",
    "Hadoop is good \n",
    "Spark is fast \n",
    "\n",
    "```\n",
    "在上述语句中，使用了 Spark 提供的 `SparkContext` 对象，名称为 `sc`，这是 `pyspark` 启动的时候自\n",
    "动创建的，在交互式编程环境中可以直接使用。如果是编写独立应用程序，则可以通过如下语句生成 `SparkContext` 对象：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f06fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf,SparkContext\n\u001b[0;32m      2\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy App\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf \u001b[38;5;241m=\u001b[39m conf)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8773cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
