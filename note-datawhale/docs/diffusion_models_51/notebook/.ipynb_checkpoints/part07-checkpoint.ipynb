{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb41f4c7",
   "metadata": {},
   "source": [
    "# DDIM 反转\n",
    "---\n",
    "### 本章知识点\n",
    "\n",
    "- DDIM采样的工作原理\n",
    "- 确定性采样和随机性采样的比较\n",
    "- DDIM反转的理论支持\n",
    "- 使用反转来编辑图像\n",
    "\n",
    "## 实战：反转\n",
    "\n",
    "### 配置\n",
    "\n",
    "首先安装所需的库并且配置环境，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers diffusers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5115ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ae4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url, size=None):\n",
    "    response = requests.get(url,timeout=1)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    if size is not None:\n",
    "        img = img.resize(size)\n",
    "    return img\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edb91d",
   "metadata": {},
   "source": [
    "## 载入一个预训练过的管线\n",
    "\n",
    "首先：使用stable diffusion pipeline加载预训练模型并配置DIMM调度器。而后对预训练模型进行一次采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce57836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入一个管线\n",
    "mypipe = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(mypipe).to(device)\n",
    "# 配置DDIM调度器\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "# 从中采样一次，以保证代码运行正常\n",
    "prompt = 'Beautiful DSLR Photograph of a penguin on the beach, golden hour'\n",
    "\n",
    "negative_prompt = 'blurry, ugly, stock photo'\n",
    "im = pipe(prompt, negative_prompt=negative_prompt).images[0]\n",
    "im.resize((256, 256))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076bd79",
   "metadata": {},
   "source": [
    "## DIMM 采样\n",
    "\n",
    "在给定时刻$t$,带有噪声的图像$x_t$通过对原始图像$x_0$ 加上高斯噪声ϵ得到：\n",
    "$$x_t=\\sqrt{a_t x_o} + \\sqrt{1-a_t ϵ}$$\n",
    "\n",
    "ϵ是方差归一化后的高斯噪声，$a_t$在DDPM论文中被称为$a$,并被用于定义噪声调度器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b796b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用alphas_cumprod函数得到alphas\n",
    "timesteps = pipe.scheduler.timesteps.cpu()\n",
    "alphas = pipe.scheduler.alphas_cumprod[timesteps]\n",
    "plt.plot(timesteps, alphas, label='alpha_t');\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec37314",
   "metadata": {},
   "source": [
    "当时间步为0时，从一幅无噪声的干净图像开始，此时$\\alpha_t=1$，当达到更高的时间步，得到一幅几乎全是噪声的图像，$\\alpha_t$也几乎下降到0。\n",
    "\n",
    "> 前向过程在给定$x_{t-1}$和$x_0$的情况下变得更加确定。在生成过程中，随机噪声$\\epsilon_t$前面的系数变为0，可得到隐式概率模型，其中模型样本是更加固定的过程从隐变量生成的(从$x_T$到$x_0$)，该模型命名为去噪扩散隐式模型（DDIM）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(prompt, start_step=0, start_latents=None,\n",
    "           guidance_scale=3.5, num_inference_steps=30,\n",
    "           num_images_per_prompt=1, do_classifier_free_guidance=True,\n",
    "           negative_prompt='', device=device):\n",
    "    # 对文本提示语进行编码  \n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "    # 设置推理步数\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    # 创建随机起点\n",
    "    if start_latents is None:\n",
    "        start_latents = torch.randn(1, 4, 64, 64, device=device)\n",
    "        start_latents *= pipe.scheduler.init_noise_sigma\n",
    "    \n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    for i in tqdm(range(start_step, num_inference_steps)):\n",
    "    \n",
    "        t = pipe.scheduler.timesteps[i]\n",
    "        \n",
    "        # 如果正在进行CFG，则对隐层进行扩展\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "        \n",
    "        # 预测噪声\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # 进行引导\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "\n",
    "        # Normally we'd rely on the scheduler to handle the update step:\n",
    "        # latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # 自行实现调度器\n",
    "        prev_t = max(1, t.item() - (1000//num_inference_steps)) # t-1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[t.item()]\n",
    "        alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]\n",
    "        predicted_x0 = (latents - (1-alpha_t).sqrt()*noise_pred) / alpha_t.sqrt()\n",
    "        direction_pointing_to_xt = (1-alpha_t_prev).sqrt()*noise_pred\n",
    "        latents = alpha_t_prev.sqrt()*predicted_x0 + direction_pointing_to_xt\n",
    "    \n",
    "    # 进行后处理\n",
    "    images = pipe.decode_latents(latents)\n",
    "    images = pipe.numpy_to_pil(images)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b083899",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Watercolor painting of a beach sunset'\n",
    "sample(prompt, negative_prompt=negative_prompt, num_inference_steps=50)[0].resize((256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e263c4dd",
   "metadata": {},
   "source": [
    "## 反转\n",
    "\n",
    "反转的目标是得到“带噪”的隐式表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = load_image('https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg', size=(512, 512))\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_prompt = \"Photograph of a puppy on the grass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用VAE进行编码\n",
    "with torch.no_grad(): \n",
    "    latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)\n",
    "l = 0.18215 * latent.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ac70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反转\n",
    "@torch.no_grad()\n",
    "def invert(start_latents, prompt, guidance_scale=3.5, num_inference_steps=80,\n",
    "           num_images_per_prompt=1, do_classifier_free_guidance=True,\n",
    "           negative_prompt='', device=device):\n",
    "  \n",
    "    # 对提示文本进行编码\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "    \n",
    "    # 指定起点\n",
    "    latents = start_latents.clone()\n",
    "    \n",
    "    # 保存反转的隐层\n",
    "    intermediate_latents = []\n",
    "    \n",
    "    # 设置推理步数\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    \n",
    "    # 反转时间步\n",
    "    timesteps = reversed(pipe.scheduler.timesteps)\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps-1):\n",
    "        # 跳过最后一次迭代\n",
    "        if i >= num_inference_steps - 1: continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "        \n",
    "        # 如果正在进行CFG，则对隐层进行扩展\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "        \n",
    "        # 预测残留的噪声\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # 引导\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        current_t = max(0, t.item() - (1000//num_inference_steps)) # t\n",
    "        next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]\n",
    "        \n",
    "        # 反转的更新步（重新排列更新步，利用当前隐层得到新的隐层）\n",
    "        latents = (latents - (1-alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (1-alpha_t_next).sqrt() * noise_pred\n",
    "\n",
    "        # 保存隐层\n",
    "        intermediate_latents.append(latents)\n",
    "            \n",
    "    return torch.cat(intermediate_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7289b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_latents = invert(l, input_image_prompt,num_inference_steps=50)\n",
    "inverted_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c40848",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([48, 4, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码反转的最后一个隐层\n",
    "with torch.no_grad():\n",
    "    im = pipe.decode_latents(inverted_latents[-1].unsqueeze(0))\n",
    "pipe.numpy_to_pil(im)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30872c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过常规调用方法，将反转隐层传递给管线\n",
    "pipe(input_image_prompt, latents=inverted_latents[-1][None], num_inference_steps=50, guidance_scale=3.5).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bab6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从第20步的隐式表示开始，得到的结果距离最初的图片很近了！\n",
    "start_step=20\n",
    "sample(input_image_prompt, start_latents=inverted_latents[-(start_step+1)][None], \n",
    "       start_step=start_step, num_inference_steps=50)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把小狗换成小猫，从第10步的隐式表示开始\n",
    "start_step=10\n",
    "new_prompt = input_image_prompt.replace('puppy', 'cat')\n",
    "sample(new_prompt, start_latents=inverted_latents[-(start_step+1)][None], \n",
    "       start_step=start_step, num_inference_steps=50)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55172b76",
   "metadata": {},
   "source": [
    "##  组合封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit(input_image, input_image_prompt, edit_prompt, num_steps=100, start_step=30, guidance_scale=3.5):\n",
    "    with torch.no_grad(): latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)\n",
    "    l = 0.18215 * latent.latent_dist.sample()\n",
    "    inverted_latents = invert(l, input_image_prompt,num_inference_steps=num_steps)\n",
    "    final_im = sample(edit_prompt, start_latents=inverted_latents[-(start_step+1)][None], \n",
    "                      start_step=start_step, num_inference_steps=num_steps, guidance_scale=guidance_scale)[0]\n",
    "    return final_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cba330",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit(input_image, 'A puppy on the grass', 'an old grey dog on the grass', num_steps=50, start_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = load_image('https://images.pexels.com/photos/1493111/pexels-photo-1493111.jpeg', size=(512, 512))\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49838f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit(face, 'A photograph of a face', 'A photograph of a face with sunglasses', num_steps=250, start_step=30, guidance_scale=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a71f6",
   "metadata": {},
   "source": [
    "##  ControlNet的结构与训练过程\n",
    "\n",
    "**目标：** 主要解决图像细节的提示，包括人物四肢的角度、背景中物体的位置、每一缕光线照射的角度等。\n",
    "\n",
    "**方案：** ControlNet能够嵌入任意已经训练好的扩散模型，可以提供更多控制条件。\n",
    "\n",
    "> ControlNet的基本结构由一个对应的预训练网络的神经网络模块和两个“零卷积”层组成。在训练过程中，固定预训练网络的权重，只更新ControlNet基本结构中的网络“副本”和零卷积层的权重。网络“副本”将学会如何让模型按照新的控制条件来生成结果，被固定的网络会保留原先网络已经学会的所有知识。\n",
    "\n",
    "**ControlNet模型的训练过程：**\n",
    "1. 收集数据集，并包含对应的prompt。   \n",
    "2. 将prompt输入被固定的稳定扩散模型，并将标注好的图像控制条件输入ControlNet，然后按照稳定扩散模型的训练过程迭代ControlNet block的权重。\n",
    "3. 在训练过程中，随机将50%的文本提示语替换为空白字符串。\n",
    "4. 训练结束后，使用ControlNet对应的图像控制条件，来控制扩散模型生成符合条件的图像。\n",
    "\n",
    "## ControlNet实战\n",
    "\n",
    "###  生成人物的肖像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc533e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533297be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "# 提取图片边缘线条\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "canny_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用半精度节约计算资源，加快推理速度\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, \n",
    "                                                        torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c497a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用速度最快的扩散模型调度器UniPCMultistepScheduler\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafac085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44abbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \", best quality, extremely detailed\"\n",
    "prompt = [t + prompt for t in [\"Sandra Oh\", \"Kim Kardashian\", \"rihanna\", \"taylor swift\"]]\n",
    "generator = [torch.Generator(device=device).manual_seed(2) for i in range(len(prompt))]\n",
    "\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    canny_image,\n",
    "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * len(prompt),\n",
    "    generator=generator,\n",
    "    num_inference_steps=30\n",
    ")\n",
    "\n",
    "image_grid(output.images, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec681dd1",
   "metadata": {},
   "source": [
    "### 提取身体姿态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from controlnet_aux import OpenposeDetector\n",
    "\n",
    "model = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "\n",
    "poses = [model(img) for img in imgs]\n",
    "image_grid(poses, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, UniPCMultistepScheduler, StableDiffusionControlNetPipeline\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"fusing/stable-diffusion-v1-5-controlnet-openpose\", \n",
    "                                             torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    controlnet = controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2feed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = [torch.Generator(device=device).manual_seed(10) for i in range(4)]\n",
    "prompt = \"super-hero character, best quality, extremely detailed\"\n",
    "\n",
    "output = pipe(\n",
    "    [prompt] * 4,\n",
    "    poses,\n",
    "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
    "    generator=generator,\n",
    "    num_inference_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(output.images, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
