{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce7150e1",
   "metadata": {},
   "source": [
    "# 第六章 Stable Diffusion\n",
    "\n",
    "> stable diffusion 是一个强大的文本条件隐式扩散模型(Text-conditioned latent diffusion model),拥有出色的文字描述生成精美图片的能力\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "隐式扩散：图片通常包括大量冗余信息，通过训练一个VAE(对其使用大量的图片数据进行训练)，使其可以将图片映射到一个较小的隐式表征，并将这个较小的隐式表征映射到原始图片，通过在隐式表征上进行扩散，我们可以在使用更少的内存的同时减少UNet层数并加速图片的生成，与此同时，仍能把结果输入VAE的解码器，得到高分辨率图像。\n",
    "\n",
    "![img.png](img.png)\n",
    "\n",
    "以文本为生成条件：在推理阶段，输入期望图像的文本描述，将纯噪声数据作为起点，然后模型对噪声输入进行“去噪”，生成能匹配文本描述的图像。\n",
    "\n",
    "1. CLIP的文本编码器将文本描述转换为特征向量，该特征向量用于与图像特征向量进行相似度比较。输入的文本提示语进行分词，然后被输入CLIP的文本编码器。\n",
    "2. 使用交叉注意力机制，交叉注意力贯穿整个UNet结构，UNet中的每个空间位置都可以“注意”到文字条件中不同的token，从文本提示语中获取不同位置的相互关联信息。\n",
    "\n",
    "\n",
    "无分类引导：主要解决可能得到与文字描述根本不相关的图片，具体方法如下：\n",
    "\n",
    "1. 训练阶段，强制模型学习在无文字信息的情况下对图片“去噪”（无条件生成）。\n",
    "2. 推理阶段，进行有文字条件预测、无文字条件预测，利用两者差异建立最终结合版的预测。\n",
    "\n",
    "使用DreamBooth进行微调：用来微调文字到图像的生成模型，Google的Imagen Model 开发，是一种个性化训练一个文本到图像模型的方法，只需要提供一个主题的3~5张图像，就能教会模型有关这个主题的各种概念，从而在不同的场景和视图中生成这个主题的相关图像。\n",
    "\n",
    "## 环境准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -Uq diffusers ftfy accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d843a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -Uq git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a34b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline, \n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionInpaintPipeline, \n",
    "    StableDiffusionDepth2ImgPipeline\n",
    "    )       \n",
    "\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = download_image(img_url).resize((512, 512))\n",
    "mask_image = download_image(mask_url).resize((512, 512))\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8d065",
   "metadata": {},
   "source": [
    "## 从文本生成图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载管线\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给生成器设置一个随机种子\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "pipe_output = pipe(\n",
    "    prompt=\"Palette knife painting of an autumn cityscape\", \n",
    "    negative_prompt=\"Oversaturated, blurry, low quality\", \n",
    "    height=480, width=640,     # 图片大小\n",
    "    guidance_scale=8,          # 提示文字的影响程度\n",
    "    num_inference_steps=35,    # 推理步数\n",
    "    generator=generator        # 设置随机种子生成器\n",
    ")\n",
    "\n",
    "pipe_output.images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比不同的guidance_scale效果（该参数决定了无分类器引导的影响强度）\n",
    "cfg_scales = [1.1, 8, 12] \n",
    "prompt = \"A collie with a pink hat\" \n",
    "fig, axs = plt.subplots(1, len(cfg_scales), figsize=(16, 5))\n",
    "for i, ax in enumerate(axs):\n",
    "    im = pipe(prompt, height=480, width=480,\n",
    "        guidance_scale=cfg_scales[i], num_inference_steps=35,\n",
    "        generator=torch.Generator(device=device).manual_seed(42)).images[0]\n",
    "    ax.imshow(im); ax.set_title(f'CFG Scale {cfg_scales[i]}');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd93970",
   "metadata": {},
   "source": [
    "## Stable Diffusion Pipeline\n",
    "可变分自编码器：对输入图像进行VAE编码器，然后生成隐编码，在VAE解码器中进行解码，得到解码后的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建区间为(-1, 1)的伪数据\n",
    "images = torch.rand(1, 3, 512, 512).to(device) * 2 - 1 \n",
    "print(\"Input images shape:\", images.shape)\n",
    "\n",
    "# 编码到隐空间\n",
    "with torch.no_grad():\n",
    "    latents = 0.18215 * pipe.vae.encode(images).latent_dist.mean\n",
    "print(\"Encoded latents shape:\", latents.shape)\n",
    "\n",
    "# 解码\n",
    "with torch.no_grad():\n",
    "    decoded_images = pipe.vae.decode(latents / 0.18215).sample\n",
    "print(\"Decoded images shape:\", decoded_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input images shape: torch.Size([1, 3, 512, 512])\n",
    "Encoded latents shape: torch.Size([1, 4, 64, 64])\n",
    "Decoded images shape: torch.Size([1, 3, 512, 512])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af4912",
   "metadata": {},
   "source": [
    "分词器和文本编码器：将输入的字符串（文本提示语）转换成数值表示形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动对提示文字进行分词和编码\n",
    "input_ids = pipe.tokenizer([\"A painting of a flooble\"])['input_ids']\n",
    "print(\"Input ID -> decoded token\")\n",
    "for input_id in input_ids[0]:\n",
    "    print(f\"{input_id} -> {pipe.tokenizer.decode(input_id)}\")\n",
    "\n",
    "# 将分词结果输入CLIP\n",
    "input_ids = torch.tensor(input_ids).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embeddings = pipe.text_encoder(input_ids)['last_hidden_state']\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行编码\n",
    "text_embeddings = pipe.encode_prompt(\n",
    "    prompt=\"A painting of a flooble\", \n",
    "    device=device, \n",
    "    num_images_per_prompt=1, \n",
    "    do_classifier_free_guidance=False, \n",
    "    negative_prompt='')\n",
    "print(\"Text embeddings shape:\", text_embeddings[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c6e3a",
   "metadata": {},
   "source": [
    "UNet：主要作用是接收“带噪”的输入并预测噪声，实现“去噪”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c15e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建伪输入\n",
    "timestep = pipe.scheduler.timesteps[0]\n",
    "latents = torch.randn(1, 4, 64, 64).to(device)\n",
    "text_embeddings = torch.randn(1, 77, 1024).to(device)\n",
    "\n",
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    unet_output = pipe.unet(latents, timestep, text_embeddings).sample\n",
    "print('UNet output shape:', unet_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227aa04",
   "metadata": {},
   "source": [
    "调度器：保存关于添加噪声的信息，并管理如何基于模型的预测更新“带噪”样本。默认调度器是PNDMScheduler。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pipe.scheduler.alphas_cumprod, label=r'$\\bar{\\alpha}$')\n",
    "plt.xlabel('Timestep (high noise to low noise ->)')\n",
    "plt.title('Noise schedule')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "# 替换调度器\n",
    "pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "print('Scheduler config:', pipe.scheduler)\n",
    "\n",
    "# 使用新的调度器生成图像\n",
    "pipe(prompt=\"Palette knife painting of an winter cityscape\", height=480, width=480,\n",
    "     generator=torch.Generator(device=device).manual_seed(42)).images[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03bfe2",
   "metadata": {},
   "source": [
    "DIY采样循环：主要整合整个管线的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 8 \n",
    "num_inference_steps=30 \n",
    "prompt = \"Beautiful picture of a wave breaking\" \n",
    "negative_prompt = \"zoomed in, blurry, oversaturated, warped\" \n",
    "\n",
    "# 对提示文字进行编码\n",
    "text_embeddings = pipe._encode_prompt(prompt, device, 1, True, negative_prompt)\n",
    "\n",
    "# 创建随机噪声作为起点\n",
    "latents = torch.randn((1, 4, 64, 64), device=device, generator=generator)\n",
    "latents *= pipe.scheduler.init_noise_sigma\n",
    "\n",
    "# 设置调度器\n",
    "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "# 循环采样\n",
    "for i, t in enumerate(pipe.scheduler.timesteps):\n",
    "    \n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred = pipe.unet(latent_model_input, t, text_embeddings).sample\n",
    "\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# 将隐变量映射到图片\n",
    "with torch.no_grad():\n",
    "    image = pipe.decode_latents(latents.detach())\n",
    "\n",
    "pipe.numpy_to_pil(image)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他管线\n",
    "\n",
    "Img2Img：首先会对一张已有的图片进行编码，得到隐变量后添加随机噪声。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63179112",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = img2img_pipe(\n",
    "    prompt=\"An oil painting of a man on a bench\",\n",
    "    image = init_image,\n",
    "    strength = 0.6, # 强度：0表示完全不起作用，1表示作用强度最大\n",
    ").images[0]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].imshow(init_image);axs[0].set_title('Input Image')\n",
    "axs[1].imshow(result_image);axs[1].set_title('Result');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c49f0",
   "metadata": {},
   "source": [
    "Inpainting：接收一张掩模图片作为额外条件输入，该掩模图片与输入图片的尺寸一致，白色区域表示要替换的部分，黑色区域表示要保留的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb24901",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A small robot, high resolution, sitting on a park bench\"\n",
    "image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs[0].imshow(init_image);axs[0].set_title('Input Image')\n",
    "axs[1].imshow(mask_image);axs[1].set_title('Mask')\n",
    "axs[2].imshow(image);axs[2].set_title('Result');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a0c76",
   "metadata": {},
   "source": [
    "Depth2Image：采用深度预测模型来预测一个深度图，该深度图被输入为跳过的UNet以生成图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab700dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-depth\")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"An oil painting of a man on a bench\"\n",
    "image = pipe(prompt=prompt, image=init_image).images[0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "axs[0].imshow(init_image);axs[0].set_title('Input Image')\n",
    "axs[1].imshow(image);axs[1].set_title('Result');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91539e38",
   "metadata": {},
   "source": [
    "对比Img2Img生成的图片，Depth2Img生成的图片有丰富的色彩变化，整体结构更忠于原图。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
