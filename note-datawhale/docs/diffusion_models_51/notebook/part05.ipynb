{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5008d82",
   "metadata": {},
   "source": [
    "# 第5章 微调和引导\n",
    "\n",
    "基于现有模型实现改造的主要方法：\n",
    "\n",
    "- 微调： 可以在新的数据集上重新训练已有的模型，以改变原有的输出类型\n",
    "- 引导： 在推理阶段引导现有模型的生成过程，以获取额外的控制\n",
    "- 条件生成：在训练过程中产生的额外信息，把这些信息输入到模型中进行预测，通过输入相关信息作为条件来控制模型的生成。\n",
    "\n",
    "## 环境准备\n",
    "\n",
    "1. 安装python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq diffusers datasets accelerate wandb open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870c384",
   "metadata": {},
   "source": [
    "2. 登录hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf26734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token: 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\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367e24b",
   "metadata": {},
   "source": [
    "引入需要的库和计算设备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98701ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from diffusers import DDIMScheduler,DDPMPipeline\n",
    "from matplotlib import pyplot as plt \n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c50777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658baaf",
   "metadata": {},
   "source": [
    "## 载入训练过的管线\n",
    "\n",
    "首先载入一个现有的管线，看看能用它做些什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32569a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pipe = \"google/ddpm-cecebahq-256\"\n",
    "image_pipe = DDPMPipeline.from_pretrained(_pipe)\n",
    "image_pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = image_pipe().images # 调用函数生成图片\n",
    "images[0]  # 过程可能有点慢，请耐心等待"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a243c",
   "metadata": {},
   "source": [
    "## DDIM - 更快的采样过程\n",
    "\n",
    "生成图片过程中，每一步模型都会接收一个带噪声的输入，并且预测这个噪声，以此估算没有噪声的完整图像。\n",
    "\n",
    "Diffusers库通过调度器进行控制采样方法，每次更新由`step()`函数完成，最后，将返回的输出命名为`pre_sample`,其中，prev表示previous,在时间上是倒退的，整个过程是从高噪声到低噪声（与前向扩散过程相反）。\n",
    "\n",
    "\n",
    "载入调度器 DDIMScheduler （论文“[Denoising Diffusion Implicit\n",
    "Models](https://arxiv.org/abs/2010.02502)”）,实现代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新的调度器并设置推理迭代次数\n",
    "scheduler = DDIMScheduler.from_pretrained(_pipe)\n",
    "scheduler.set_timesteps(num_inference_steps=40)   \n",
    "\n",
    "scheduler.timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机噪声\n",
    "x = torch.randn(4, 3, 256, 256).to(device)\n",
    "# batch_size=4，三通道，长宽均为256像素的一组图像\n",
    "# 循环时间步\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # 模型输入：给“带噪”图像加上时间步信息\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    \n",
    "    # 预测噪声\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "    \n",
    "    # 使用调度器计算更新后的样本\n",
    "    scheduler_output = scheduler.step(noise_pred, t, x)\n",
    "\n",
    "    # 更新输入图像\n",
    "    x = scheduler_output.prev_sample\n",
    "    \n",
    "    # 时不时看一下输入图像和预测的“去噪”图像\n",
    "    if i % 10 == 0 or i == len(scheduler.timesteps) - 1:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        grid = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0)\n",
    "        axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[0].set_title(f\"Current x (step {i})\")\n",
    "\n",
    "        pred_x0 = scheduler_output.pred_original_sample  \n",
    "        grid = torchvision.utils.make_grid(pred_x0, nrow=4).permute(1, 2, 0)\n",
    "        axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[1].set_title(f\"Predicted denoised images (step {i})\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795f867",
   "metadata": {},
   "source": [
    "随着过程的推进，预测的输出逐步得到改善。通过新的调度器替换原有管线中的调度器，然后进行采用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe.sheduler = scheduler\n",
    "images = image_pipe(num_inference_steps = 40).images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d2e05",
   "metadata": {},
   "source": [
    "**这里简单说一下DDIM:** :\n",
    "DDIM和DDPM有相同的训练目标，但是它不再限制扩散过程必须是一个马尔卡夫链，这使得DDIM可以采用更小的采样步数来加速生成过程，DDIM的另外是一个特点是从一个随机噪音生成样本的过程是一个确定的过程（中间没有加入随机噪音）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e3770",
   "metadata": {},
   "source": [
    "## 扩散模型之微调\n",
    "\n",
    "### 实战：微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aa5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(dataset_name,split=\"train\")\n",
    "image_size = 256\n",
    "batch_size = 4\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((iamge_size,image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0,5],[0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in \n",
    "             examples[image]]\n",
    "    return {\"images\":images}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader =torch.utils.data.DataLoader(\n",
    "    dataset,batch_size = batch_size,shuffle = True\n",
    ")\n",
    "\n",
    "print(\"Previewing batch:\")\n",
    "batch = next(iter(train_dataloader))\n",
    "grid = torchvision.utils.make_grid(batch[\"image\"],nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "需要考虑的4个额外因素：\n",
    "\n",
    "额外因素1：适应GPU显存，需要权衡好batch size和图像尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa72c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "lr = 1e-5\n",
    "grad_accumulation_steps = 2  \n",
    "\n",
    "optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # 随机生成一个噪声,稍后加到图像上\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "        # 随机选取一个时间步\n",
    "        timesteps = torch.randint(\n",
    "            0,\n",
    "            image_pipe.scheduler.conf.num_train_timesteps,\n",
    "            (bs,),\n",
    "            device=clean_images.device,\n",
    "        ).long()\n",
    "        \n",
    "        # 根据选中的时间步和确定的幅值，添加噪声，并进行前向扩散过程\n",
    "        \n",
    "        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        # 使用“带噪”图像进行网络预测\n",
    "        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # 对真正的噪声和预测的结果进行比较，注意这里是预测噪声\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # 保存损失值\n",
    "        losses.append(loss.item())\n",
    "        # 更新梯度\n",
    "        loss.backward(loss)\n",
    "\n",
    "        # 进行梯度累积，在累积到一定步数后更新模型参数 \n",
    "        if (step + 1) % grad_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}\")\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfca2b",
   "metadata": {},
   "source": [
    "额外因素2： 通过观测图，损失曲线跟噪声一样混乱，可以通过进行梯度累积（gradient accumulation），来得到与使用更大batch size一样的收益，又不会造成内存溢出。\n",
    "\n",
    "具体做法：\n",
    "\n",
    "多运行几次loss.backward()再调用optimizer.step() 和 optimizer.zero_grad() .\n",
    "\n",
    "> 练习5-1：思考是否可以把梯度累积加到训练循环中呢？如果可以，具体该怎么做呢？如何基于梯度累积的步数调整学习率？学习率与之前一样吗？\n",
    "\n",
    "额外因素3： 频率不足以及时反映我们的训练进展。因此为了更好地了解训练情况，我们应该采取以下两个步骤。\n",
    "(1)在训练过程中，时不时生成一些图像样本，供我们检查模型性能。  \n",
    "(2)在训练过程中，将损失值、生成的图像样本等信息记录到日志中，可使用Weights and Biases、TensorBoard等功能或工具。 \n",
    "\n",
    "为了方便你了解训练效果，本书提供了一个快速的脚本程序，该脚本程序使用上述训练代码并加入了少量日志记录功能。\n",
    "\n",
    "通过观察这些信息，你可以发现，尽管从损失值的角度看，训练似乎没有得到改进，但你可以看到一个从原有图像分布到新的数据集逐渐演变的过程。\n",
    "\n",
    "> 练习5-2：尝试修改第4章的示例训练脚本程序。你可以使用预训练好的模型，而不是从头开始训练。对比一下这里提供的脚本程序，看看有哪些额外功能是这里的脚本程序所没有的。\n",
    "\n",
    "接下来，我们可以使用模型生成一些图像样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c26547",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 3, 256, 256).to(device)\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac7af5",
   "metadata": {},
   "source": [
    "额外因素4：微调过程可能是难以预测的。如果训练时间很长，则我们有可能看到一些完美的蝴蝶图像，但模型的中间过程也非常有趣，特别是对于那些对不同艺术风格感兴趣的人来说！你还可以试着\n",
    "短时间或长时间观察一下训练过程，并试着改变学习率，看看这会如何影响模型最终的输出\n",
    "\n",
    "### 使用最小化实例程序来微调模型："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e9d5c",
   "metadata": {},
   "source": [
    "一个最小化示例程序，用于微调相关模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7479efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 下载微调用的脚本\n",
    "!wget https://github.com/huggingface/diffusion-models￾class/raw/main/unit2/finetune_model.py\n",
    "## 在终端运行脚本：在Vintage Face数据集上训练脚本\n",
    "python finetune_model.py\n",
    " --image_size 128 --batch_size 8 --num_epochs 16 \\\n",
    " --grad_accumulation_steps 2 --start_model \"google/ddpm-celebahq-256\" \\\n",
    " --dataset_name \"Norod78/Vintage-Faces-FFHQAligned\" \\\n",
    " --wandb_project 'dm-finetune' \\\n",
    " --log_samples_every 100 --save_model_every 1000 \\\n",
    " --model_save_name 'vintageface'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134baf",
   "metadata": {},
   "source": [
    "### 保存和载入微调过的管线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe.save_pretrained(\"my-finetuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d769e6",
   "metadata": {},
   "source": [
    "## 引导\n",
    "\n",
    "使用LSUM bedrooms 数据集上训练并在wikiart数据集上进行一轮微调的新模型进行引导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载一个预训练的管线\n",
    "pipeline_name = \"johnowhitaker/sd-class-wikiart-from-bedrooms\"\n",
    "image_pipe = DDPMPipeline.from_pretrained(pipeline_name).to(device)\n",
    "\n",
    "# 使用DDIM调度器，用40步生成图片\n",
    "scheduler = DDIMScheduler.from_pretrained(pipeline_name)\n",
    "scheduler.set_timesteps(num_inference_steps=40)\n",
    "\n",
    "# 从随机噪声开始\n",
    "x = torch.randn(8, 3, 256, 256).to(device)\n",
    "\n",
    "# 使用一个最简单的采样循环\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "# 生成图片\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843563a1",
   "metadata": {},
   "source": [
    "需要引导的几种情况：\n",
    "\n",
    "1. 在一个很小的数据集上微调文本条件模型，希望模型尽可能保留其原始训练所学习的内容，以便能够理解数据集中未涵盖的各种文本提示语。同时，希望它能适应我们的数据集，以便生成的内容与原有的数据风格一致，需要使用很小的学习率并对模型执行指数平均操作。\n",
    "\n",
    "2. 可能需要重新训练一个模型以适应新的数据集，需要使用较大的学习率并进行长时间的训练。\n",
    "\n",
    "\n",
    "### 实战：引导\n",
    "\n",
    "如果想要对生成的样本施加控制，该怎么做呢？如果想让生成的图片偏向于靠近某种颜色，又该怎么做呢？这时我们可以利用引导(guidance)，在采样过程中施加额外的控制。\n",
    "\n",
    "首先，创建一个函数，用于定义希望优化的指标（损失值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_loss(images, target_color=(0.1, 0.9, 0.5)):\n",
    "    \"\"\"给定一个RGB值，返回一个损失值，用于衡量图片的像素值与目标颜色之差\"\"\"\n",
    "    # 归一化\n",
    "    target = torch.tensor(target_color).to(images.device) * 2 - 1  \n",
    "    # 将目标张量的形状改未(b, c, h, w)\n",
    "    target = target[None, :, None, None]  \n",
    "    # 计算图片的像素值以及目标颜色的均方误差\n",
    "    error = torch.abs(images - target).mean()  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0670c9",
   "metadata": {},
   "source": [
    "接下来，需要修改采样循环并执行以下操作：\n",
    "\n",
    "(1)创建新的输入图像$x$，将它的requires_grad属性设置为True。  \n",
    "(2)计算“去噪”后的图像$x_0$。  \n",
    "(3)将“去噪”后的图像$x_0$传递给损失函数。  \n",
    "(4)计算损失函数对输入图像$x$的梯度。  \n",
    "(5)在使用调度器之前，先用计算出来的梯度修改输入图像$x$，使输入图像$x$朝着减少损失值的方向改进。  \n",
    "\n",
    "实现方法有两种：\n",
    "\n",
    "**方法1：** 从UNet中获取噪声预测，将其设置为输入图像x的requires_grad属性，可以更高效地使用内存。缺点是导致梯度的精度降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 用于决定引导的强度有多大，可设置5~100之间的数\n",
    "guidance_loss_scale = 40  \n",
    "\n",
    "x = torch.randn(8, 3, 256, 256).to(device)\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    x = x.detach().requires_grad_()\n",
    "    \n",
    "    # 得到去噪后的图像\n",
    "    x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "    loss = color_loss(x0) * guidance_loss_scale\n",
    "    if i % 10 == 0:\n",
    "        print(i, \"loss:\", loss.item())\n",
    "    \n",
    "    # 获取梯度\n",
    "    cond_grad = -torch.autograd.grad(loss, x)[0]\n",
    "    # 梯度更新x\n",
    "    x = x.detach() + cond_grad\n",
    "    # 使用调度器更新x\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5\n",
    "Image.fromarray(np.array(im * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a479dc",
   "metadata": {},
   "source": [
    "方法2：先将输入图像x\n",
    "x的requires_grad属性设置为True，然后传递给UNet并计算“去噪”后的图像$x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_loss_scale = 40\n",
    "x = torch.randn(4, 3, 256, 256).to(device)\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "    # 设置requires_grad\n",
    "    x = x.detach().requires_grad_()\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "    # 预测\n",
    "    noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "    # 得到“去噪”后的图像\n",
    "    x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "    loss = color_loss(x0) * guidance_loss_scale\n",
    "    if i % 10 == 0:\n",
    "        print(i, \"loss:\", loss.item())\n",
    "    # 获取梯度\n",
    "    cond_grad = -torch.autograd.grad(loss, x)[0]\n",
    "    # 更新x\n",
    "    x = x.detach() + cond_grad\n",
    "    # 使用调度器更新x\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "grid = torchvision.utils.make_grid(x, nrow=4)\n",
    "im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5\n",
    "Image.fromarray(np.array(im * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008c68e",
   "metadata": {},
   "source": [
    "第二种方法对GPU显存的要求更高了，但更接近于训练模型所使用的数据。也可以通过增大guidance_loss_scale来增强颜色迁移的效果。\n",
    "\n",
    "> 练习5-3：选出你最喜欢的颜色并找出相应的RGB值，然后修改上述代码中的color_loss()函数，将目标颜色改成你最喜欢的颜色并检查输出。观察输出和你预期的结果匹配吗？\n",
    "\n",
    "###  CLIP引导\n",
    "\n",
    "CLIP是一个由OpenAI开发的模型，它使得我们能够对图片和文字说明进行比较\n",
    "\n",
    "基本流程：\n",
    "\n",
    "对文本提示语进行embedding，为 CLIP 获取一个512维的embedding。\n",
    "在扩散模型的生成过程中的每一步进行如下操作：\n",
    "- 制作多个不同版本的预测出来的“去噪”图片。\n",
    "- 对预测出来的每一张“去噪”图片，用CLIP给图片做embedding，并对图片和文字的embedding做对比。\n",
    "- 计算损失对于当前“带噪”的输入x的梯度，在使用调度器更新x之前先用这个梯度修改它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "clip_model.to(device)\n",
    "\n",
    "tfms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomResizedCrop(224), \n",
    "        torchvision.transforms.RandomAffine(5),  \n",
    "        torchvision.transforms.RandomHorizontalFlip(),  \n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "            std=(0.26862954, 0.26130258, 0.27577711),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 定义一个损失函数，用于获取图片的特征，然后与提示文字的特征进行对比\n",
    "def clip_loss(image, text_features):\n",
    "    image_features = clip_model.encode_image(\n",
    "        tfms(image)\n",
    "    )  # Note: applies the above transforms\n",
    "    input_normed = torch.nn.functional.normalize(image_features.unsqueeze(1), dim=2)\n",
    "    embed_normed = torch.nn.functional.normalize(text_features.unsqueeze(0), dim=2)\n",
    "    dists = (\n",
    "        input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "    )  # Squared Great Circle Distance\n",
    "    return dists.mean()\n",
    "\n",
    "prompt = \"Red Rose (still life), red flower painting\"\n",
    "\n",
    "guidance_scale = 8  \n",
    "n_cuts = 4  \n",
    "\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "# 使用CLIP从提示文字中提取特征\n",
    "text = open_clip.tokenize([prompt]).to(device)\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = clip_model.encode_text(text)\n",
    "\n",
    "\n",
    "x = torch.randn(4, 3, 256, 256).to(device)\n",
    "\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    cond_grad = 0\n",
    "\n",
    "    for cut in range(n_cuts):\n",
    "\n",
    "        x = x.detach().requires_grad_()\n",
    "        # 获取“去噪”后的图像\n",
    "        x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "\n",
    "        loss = clip_loss(x0, text_features) * guidance_scale\n",
    "        # 获取梯度并使用n_cuts平均\n",
    "        cond_grad -= torch.autograd.grad(loss, x)[0] / n_cuts\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(\"Step:\", i, \", Guidance loss:\", loss.item())\n",
    "\n",
    "    alpha_bar = scheduler.alphas_cumprod[i]\n",
    "    x = x.detach() + cond_grad * alpha_bar.sqrt()\n",
    "\n",
    "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "\n",
    "grid = torchvision.utils.make_grid(x.detach(), nrow=4)\n",
    "im = grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5\n",
    "Image.fromarray(np.array(im * 255).astype(np.uint8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233628cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1 for a in scheduler.alphas_cumprod], label=\"no\n",
    "scaling\")\n",
    "plt.plot([a for a in scheduler.alphas_cumprod],\n",
    "label=\"alpha_bar\")\n",
    "plt.plot([a.sqrt() for a in scheduler.alphas_cumprod],\n",
    " label=\"alpha_bar.sqrt()\")\n",
    "plt.plot(\n",
    " [(1 - a).sqrt() for a in scheduler.alphas_cumprod], label=\"\n",
    "(1-\n",
    " alpha_bar).sqrt()\"\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43a6b9",
   "metadata": {},
   "source": [
    "> 如需分享你的自定义采样训练，可以关注扩展工具gradio的使用 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623cc28",
   "metadata": {},
   "source": [
    "## 实战：创建一个类别条件扩散模型\n",
    "\n",
    "### 1. 配置和数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载MNIST数据集\n",
    "dataset = torchvision.datasets.MNIST(root=\"./data/mnist/\", train=True, \n",
    "                                     download=True, \n",
    "                                     transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535e090",
   "metadata": {},
   "source": [
    "### 2. 创建一个以类别为条件的UNet模型\n",
    "\n",
    "\n",
    "输入类别的流程：\n",
    "\n",
    "1. 创建一个标准的UNet2DModel，加入一些额外的输入通道。\n",
    "2. 通过一个嵌入层，把类别标签映射到一个长度为class_emb_size的特征向量上。\n",
    "3. 把这个信息作为额外通道和原有的输入向量拼接起来。\n",
    "4. 将net_input（其中包含class_emb_size + 1个通道）输入UNet模型，得到最终的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "    def __init__(self, num_classes=10, class_emb_size=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=28,           \n",
    "            in_channels=1 + class_emb_size, # 加入额外的输入通道\n",
    "            out_channels=1,           # 输出结果的通道数\n",
    "            layers_per_block=2,       # 残差层个数\n",
    "            block_out_channels=(32, 64, 64), \n",
    "            down_block_types=( \n",
    "                \"DownBlock2D\",        # 下采样模块\n",
    "                \"AttnDownBlock2D\",    # 含有spatil self-attention的ResNet下采样模块\n",
    "                \"AttnDownBlock2D\",\n",
    "            ), \n",
    "            up_block_types=(\n",
    "                \"AttnUpBlock2D\", \n",
    "                \"AttnUpBlock2D\",      # 含有spatil self-attention的ResNet上采样模块\n",
    "                \"UpBlock2D\",          # 上采样模块\n",
    "              ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, class_labels):\n",
    "        bs, ch, w, h = x.shape\n",
    "        # 类别条件将会以额外通道的形式输入\n",
    "        class_cond = self.class_emb(class_labels) \n",
    "        class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "\n",
    "        net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "        return self.model(net_input, t).sample # (bs, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cf38d",
   "metadata": {},
   "source": [
    "###  3.训练和采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03543b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "net = ClassConditionedUnet().to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        x = x.to(device) * 2 - 1 \n",
    "        y = y.to(device)\n",
    "        noise = torch.randn_like(x)\n",
    "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "\n",
    "        pred = net(noisy_x, timesteps, y) \n",
    "\n",
    "        loss = loss_fn(pred, noise) \n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccefe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备一个随机噪声作为起点，并准备想要的图片标签\n",
    "x = torch.randn(80, 1, 28, 28).to(device)\n",
    "y = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)\n",
    "\n",
    "# 循环采样\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        residual = net(x, t, y)\n",
    "\n",
    "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0], cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fed34b",
   "metadata": {},
   "source": [
    "> 练习5-4（选做）：采用同样的方法在FashionMNIST数据集上进行尝试，调节学习率、batch size和训练周期数。看你能否使用比上面的示例更少的训练时间，最终得到一些看起来不错的与时尚相关的图片吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e268b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc1f9331",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "\n",
    "[1].[扩散模型之DDIM](https://zhuanlan.zhihu.com/p/565698027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474b863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
