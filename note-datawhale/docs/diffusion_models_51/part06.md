# ç¬¬å…­ç«  Stable Diffusion

> stable diffusion æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬æ¡ä»¶éšå¼æ‰©æ•£æ¨¡å‹(Text-conditioned latent diffusion model),æ‹¥æœ‰å‡ºè‰²çš„æ–‡å­—æè¿°ç”Ÿæˆç²¾ç¾å›¾ç‰‡çš„èƒ½åŠ›

## åŸºæœ¬æ¦‚å¿µ

éšå¼æ‰©æ•£ï¼šå›¾ç‰‡é€šå¸¸åŒ…æ‹¬å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªVAE(å¯¹å…¶ä½¿ç”¨å¤§é‡çš„å›¾ç‰‡æ•°æ®è¿›è¡Œè®­ç»ƒ)ï¼Œä½¿å…¶å¯ä»¥å°†å›¾ç‰‡æ˜ å°„åˆ°ä¸€ä¸ªè¾ƒå°çš„éšå¼è¡¨å¾ï¼Œå¹¶å°†è¿™ä¸ªè¾ƒå°çš„éšå¼è¡¨å¾æ˜ å°„åˆ°åŸå§‹å›¾ç‰‡ï¼Œé€šè¿‡åœ¨éšå¼è¡¨å¾ä¸Šè¿›è¡Œæ‰©æ•£ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä½¿ç”¨æ›´å°‘çš„å†…å­˜çš„åŒæ—¶å‡å°‘UNetå±‚æ•°å¹¶åŠ é€Ÿå›¾ç‰‡çš„ç”Ÿæˆï¼Œä¸æ­¤åŒæ—¶ï¼Œä»èƒ½æŠŠç»“æœè¾“å…¥VAEçš„è§£ç å™¨ï¼Œå¾—åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒã€‚

![img.png](img.png)

ä»¥æ–‡æœ¬ä¸ºç”Ÿæˆæ¡ä»¶ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œè¾“å…¥æœŸæœ›å›¾åƒçš„æ–‡æœ¬æè¿°ï¼Œå°†çº¯å™ªå£°æ•°æ®ä½œä¸ºèµ·ç‚¹ï¼Œç„¶åæ¨¡å‹å¯¹å™ªå£°è¾“å…¥è¿›è¡Œâ€œå»å™ªâ€ï¼Œç”Ÿæˆèƒ½åŒ¹é…æ–‡æœ¬æè¿°çš„å›¾åƒã€‚

1. CLIPçš„æ–‡æœ¬ç¼–ç å™¨å°†æ–‡æœ¬æè¿°è½¬æ¢ä¸ºç‰¹å¾å‘é‡ï¼Œè¯¥ç‰¹å¾å‘é‡ç”¨äºä¸å›¾åƒç‰¹å¾å‘é‡è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒã€‚è¾“å…¥çš„æ–‡æœ¬æç¤ºè¯­è¿›è¡Œåˆ†è¯ï¼Œç„¶åè¢«è¾“å…¥CLIPçš„æ–‡æœ¬ç¼–ç å™¨ã€‚
2. ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œäº¤å‰æ³¨æ„åŠ›è´¯ç©¿æ•´ä¸ªUNetç»“æ„ï¼ŒUNetä¸­çš„æ¯ä¸ªç©ºé—´ä½ç½®éƒ½å¯ä»¥â€œæ³¨æ„â€åˆ°æ–‡å­—æ¡ä»¶ä¸­ä¸åŒçš„tokenï¼Œä»æ–‡æœ¬æç¤ºè¯­ä¸­è·å–ä¸åŒä½ç½®çš„ç›¸äº’å…³è”ä¿¡æ¯ã€‚


æ— åˆ†ç±»å¼•å¯¼ï¼šä¸»è¦è§£å†³å¯èƒ½å¾—åˆ°ä¸æ–‡å­—æè¿°æ ¹æœ¬ä¸ç›¸å…³çš„å›¾ç‰‡ï¼Œå…·ä½“æ–¹æ³•å¦‚ä¸‹ï¼š

1. è®­ç»ƒé˜¶æ®µï¼Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ åœ¨æ— æ–‡å­—ä¿¡æ¯çš„æƒ…å†µä¸‹å¯¹å›¾ç‰‡â€œå»å™ªâ€ï¼ˆæ— æ¡ä»¶ç”Ÿæˆï¼‰ã€‚
2. æ¨ç†é˜¶æ®µï¼Œè¿›è¡Œæœ‰æ–‡å­—æ¡ä»¶é¢„æµ‹ã€æ— æ–‡å­—æ¡ä»¶é¢„æµ‹ï¼Œåˆ©ç”¨ä¸¤è€…å·®å¼‚å»ºç«‹æœ€ç»ˆç»“åˆç‰ˆçš„é¢„æµ‹ã€‚

ä½¿ç”¨DreamBoothè¿›è¡Œå¾®è°ƒï¼šç”¨æ¥å¾®è°ƒæ–‡å­—åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼ŒGoogleçš„Imagen Model å¼€å‘ï¼Œæ˜¯ä¸€ç§ä¸ªæ€§åŒ–è®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ–¹æ³•ï¼Œåªéœ€è¦æä¾›ä¸€ä¸ªä¸»é¢˜çš„3~5å¼ å›¾åƒï¼Œå°±èƒ½æ•™ä¼šæ¨¡å‹æœ‰å…³è¿™ä¸ªä¸»é¢˜çš„å„ç§æ¦‚å¿µï¼Œä»è€Œåœ¨ä¸åŒçš„åœºæ™¯å’Œè§†å›¾ä¸­ç”Ÿæˆè¿™ä¸ªä¸»é¢˜çš„ç›¸å…³å›¾åƒã€‚

## ç¯å¢ƒå‡†å¤‡



```python
pip install -Uq diffusers ftfy accelerate
```

    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.5/1.5 MB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m53.1/53.1 kB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m261.4/261.4 kB[0m [31m11.9 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m302.0/302.0 kB[0m [31m13.4 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.3/1.3 MB[0m [31m20.4 MB/s[0m eta [36m0:00:00[0m
    [?25h


```python
pip install -Uq git+https://github.com/huggingface/transformers
```

      Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.8/3.8 MB[0m [31m16.7 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m295.0/295.0 kB[0m [31m21.8 MB/s[0m eta [36m0:00:00[0m
    [?25h  Building wheel for transformers (pyproject.toml) ... [?25l[?25hdone
    


```python
import torch
import requests
from PIL import Image
from io import BytesIO
from matplotlib import pyplot as plt

from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionImg2ImgPipeline,
    StableDiffusionInpaintPipeline,
    StableDiffusionDepth2ImgPipeline
    )

def download_image(url):
    response = requests.get(url)
    return Image.open(BytesIO(response.content)).convert("RGB")

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

init_image = download_image(img_url).resize((512, 512))
mask_image = download_image(mask_url).resize((512, 512))

device =  "cuda" if torch.cuda.is_available() else "cpu"

```

    The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
    


    0it [00:00, ?it/s]


## ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒ


```python
# åŠ è½½ç®¡çº¿
model_id = "stabilityai/stable-diffusion-2-1-base"
pipe = StableDiffusionPipeline.from_pretrained(model_id).to(device)

```


    Downloading (â€¦)ain/model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]



    Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]



    Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]



    Downloading (â€¦)_encoder/config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]



    Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/807 [00:00<?, ?B/s]



    Downloading (â€¦)cheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]



    Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]



    Downloading (â€¦)f06/unet/config.json:   0%|          | 0.00/911 [00:00<?, ?B/s]



    Downloading (â€¦)df06/vae/config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]



    Downloading model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]



    Downloading (â€¦)ch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]



    Downloading (â€¦)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]



    Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]



```python
# ç»™ç”Ÿæˆå™¨è®¾ç½®ä¸€ä¸ªéšæœºç§å­
generator = torch.Generator(device=device).manual_seed(42)

pipe_output = pipe(
    prompt="Palette knife painting of an autumn cityscape",
    negative_prompt="Oversaturated, blurry, low quality",
    height=480, width=640,     # å›¾ç‰‡å¤§å°
    guidance_scale=8,          # æç¤ºæ–‡å­—çš„å½±å“ç¨‹åº¦
    num_inference_steps=35,    # æ¨ç†æ­¥æ•°
    generator=generator        # è®¾ç½®éšæœºç§å­ç”Ÿæˆå™¨
)

pipe_output.images[0]

```


      0%|          | 0/35 [00:00<?, ?it/s]





    
![png](img/output_6_1.png)
    




```python
pipe_output = pipe(
    prompt="Palette knife painting of an winter cityscape, Romance and snow",
    negative_prompt="Oversaturated, blurry, low quality",
    height=480, width=640,       # å›¾ç‰‡å¤§å°
    guidance_scale=8,          # æç¤ºæ–‡å­—çš„å½±å“ç¨‹åº¦
    num_inference_steps=35,       # æ¨ç†æ­¥æ•°
    generator=generator         # è®¾ç½®éšæœºç§å­ç”Ÿæˆå™¨
)

pipe_output.images[0]
```


      0%|          | 0/35 [00:00<?, ?it/s]





    
![png](img/output_6_7_1.png)
    




```python
# å¯¹æ¯”ä¸åŒçš„guidance_scaleæ•ˆæœï¼ˆè¯¥å‚æ•°å†³å®šäº†æ— åˆ†ç±»å™¨å¼•å¯¼çš„å½±å“å¼ºåº¦ï¼‰
cfg_scales = [1.1, 8, 12]
prompt = "A collie with a pink hat"
fig, axs = plt.subplots(1, len(cfg_scales), figsize=(16, 5))
for i, ax in enumerate(axs):
    im = pipe(prompt, height=480, width=480,
        guidance_scale=cfg_scales[i], num_inference_steps=35,
        generator=torch.Generator(device=device).manual_seed(42)).images[0]
    ax.imshow(im); ax.set_title(f'CFG Scale {cfg_scales[i]}');

```


      0%|          | 0/35 [00:00<?, ?it/s]



      0%|          | 0/35 [00:00<?, ?it/s]



      0%|          | 0/35 [00:00<?, ?it/s]



    
![png](img/output_8_3.png)
    


## Stable Diffusion Pipeline
å¯å˜åˆ†è‡ªç¼–ç å™¨ï¼šå¯¹è¾“å…¥å›¾åƒè¿›è¡ŒVAEç¼–ç å™¨ï¼Œç„¶åç”Ÿæˆéšç¼–ç ï¼Œåœ¨VAEè§£ç å™¨ä¸­è¿›è¡Œè§£ç ï¼Œå¾—åˆ°è§£ç åçš„å›¾åƒã€‚


```python
# åˆ›å»ºåŒºé—´ä¸º(-1, 1)çš„ä¼ªæ•°æ®
images = torch.rand(1, 3, 512, 512).to(device) * 2 - 1
print("Input images shape:", images.shape)

# ç¼–ç åˆ°éšç©ºé—´
with torch.no_grad():
    latents = 0.18215 * pipe.vae.encode(images).latent_dist.mean
print("Encoded latents shape:", latents.shape)

# è§£ç 
with torch.no_grad():
    decoded_images = pipe.vae.decode(latents / 0.18215).sample
print("Decoded images shape:", decoded_images.shape)

```

    Input images shape: torch.Size([1, 3, 512, 512])
    Encoded latents shape: torch.Size([1, 4, 64, 64])
    Decoded images shape: torch.Size([1, 3, 512, 512])
    

åˆ†è¯å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼šå°†è¾“å…¥çš„å­—ç¬¦ä¸²ï¼ˆæ–‡æœ¬æç¤ºè¯­ï¼‰è½¬æ¢æˆæ•°å€¼è¡¨ç¤ºå½¢å¼ã€‚


```python
# æ‰‹åŠ¨å¯¹æç¤ºæ–‡å­—è¿›è¡Œåˆ†è¯å’Œç¼–ç 
input_ids = pipe.tokenizer(["A painting of a flooble"])['input_ids']
print("Input ID -> decoded token")
for input_id in input_ids[0]:
    print(f"{input_id} -> {pipe.tokenizer.decode(input_id)}")

# å°†åˆ†è¯ç»“æœè¾“å…¥CLIP
input_ids = torch.tensor(input_ids).to(device)
with torch.no_grad():
    text_embeddings = pipe.text_encoder(input_ids)['last_hidden_state']
print("Text embeddings shape:", text_embeddings.shape)

```

    Input ID -> decoded token
    49406 -> <|startoftext|>
    320 -> a
    3086 -> painting
    539 -> of
    320 -> a
    4062 -> floo
    1059 -> ble
    49407 -> <|endoftext|>
    Text embeddings shape: torch.Size([1, 8, 1024])
    


```python
# è¿›è¡Œç¼–ç 
text_embeddings = pipe.encode_prompt(
    prompt="A painting of a flooble",
    device=device,
    num_images_per_prompt=1,
    do_classifier_free_guidance=False,
    negative_prompt='')
print("Text embeddings shape:", text_embeddings[0].shape)

```

    Text embeddings shape: torch.Size([1, 77, 1024])
    

UNetï¼šä¸»è¦ä½œç”¨æ˜¯æ¥æ”¶â€œå¸¦å™ªâ€çš„è¾“å…¥å¹¶é¢„æµ‹å™ªå£°ï¼Œå®ç°â€œå»å™ªâ€ã€‚


```python
# åˆ›å»ºä¼ªè¾“å…¥
timestep = pipe.scheduler.timesteps[0]
latents = torch.randn(1, 4, 64, 64).to(device)
text_embeddings = torch.randn(1, 77, 1024).to(device)

# æ¨¡å‹é¢„æµ‹
with torch.no_grad():
    unet_output = pipe.unet(latents, timestep, text_embeddings).sample
print('UNet output shape:', unet_output.shape)

```

    UNet output shape: torch.Size([1, 4, 64, 64])
    

è°ƒåº¦å™¨ï¼šä¿å­˜å…³äºæ·»åŠ å™ªå£°çš„ä¿¡æ¯ï¼Œå¹¶ç®¡ç†å¦‚ä½•åŸºäºæ¨¡å‹çš„é¢„æµ‹æ›´æ–°â€œå¸¦å™ªâ€æ ·æœ¬ã€‚é»˜è®¤è°ƒåº¦å™¨æ˜¯PNDMSchedulerã€‚


```python
plt.plot(pipe.scheduler.alphas_cumprod, label=r'$\bar{\alpha}$')
plt.xlabel('Timestep (high noise to low noise ->)')
plt.title('Noise schedule')
plt.legend();

```


    
![png](img/output_17_0.png)
    



```python
from diffusers import LMSDiscreteScheduler

# æ›¿æ¢è°ƒåº¦å™¨
pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)

print('Scheduler config:', pipe.scheduler)

# ä½¿ç”¨æ–°çš„è°ƒåº¦å™¨ç”Ÿæˆå›¾åƒ
pipe(prompt="Palette knife painting of an winter cityscape", height=480, width=480,
     generator=torch.Generator(device=device).manual_seed(42)).images[0]

```

    Scheduler config: LMSDiscreteScheduler {
      "_class_name": "LMSDiscreteScheduler",
      "_diffusers_version": "0.21.4",
      "beta_end": 0.012,
      "beta_schedule": "scaled_linear",
      "beta_start": 0.00085,
      "clip_sample": false,
      "num_train_timesteps": 1000,
      "prediction_type": "epsilon",
      "set_alpha_to_one": false,
      "skip_prk_steps": true,
      "steps_offset": 1,
      "timestep_spacing": "linspace",
      "trained_betas": null,
      "use_karras_sigmas": false
    }
    
    


      0%|          | 0/50 [00:00<?, ?it/s]





    
![png](img/output_18_2.png)
    



DIYé‡‡æ ·å¾ªç¯ï¼šä¸»è¦æ•´åˆæ•´ä¸ªç®¡çº¿çš„åŠŸèƒ½ã€‚


```python
guidance_scale = 8
num_inference_steps=30
prompt = "Beautiful picture of a wave breaking"
negative_prompt = "zoomed in, blurry, oversaturated, warped"

# å¯¹æç¤ºæ–‡å­—è¿›è¡Œç¼–ç 
text_embeddings = pipe._encode_prompt(prompt, device, 1, True, negative_prompt)

# åˆ›å»ºéšæœºå™ªå£°ä½œä¸ºèµ·ç‚¹
latents = torch.randn((1, 4, 64, 64), device=device, generator=generator)
latents *= pipe.scheduler.init_noise_sigma

# è®¾ç½®è°ƒåº¦å™¨
pipe.scheduler.set_timesteps(num_inference_steps, device=device)

# å¾ªç¯é‡‡æ ·
for i, t in enumerate(pipe.scheduler.timesteps):

    latent_model_input = torch.cat([latents] * 2)

    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

    with torch.no_grad():
        noise_pred = pipe.unet(latent_model_input, t, text_embeddings).sample

    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

    # compute the previous noisy sample x_t -> x_t-1
    latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample

# å°†éšå˜é‡æ˜ å°„åˆ°å›¾ç‰‡
with torch.no_grad():
    image = pipe.decode_latents(latents.detach())

pipe.numpy_to_pil(image)[0]

```

## å…¶ä»–ç®¡çº¿

Img2Imgï¼šé¦–å…ˆä¼šå¯¹ä¸€å¼ å·²æœ‰çš„å›¾ç‰‡è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°éšå˜é‡åæ·»åŠ éšæœºå™ªå£°ã€‚


```python
model_id = "stabilityai/stable-diffusion-2-1-base"
img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id).to(device)

```


    Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]



```python
result_image = img2img_pipe(
    prompt="An oil painting of a beatiful girl on a bench",
    image = init_image,
    strength = 0.6, # å¼ºåº¦ï¼š0è¡¨ç¤ºå®Œå…¨ä¸èµ·ä½œç”¨ï¼Œ1è¡¨ç¤ºä½œç”¨å¼ºåº¦æœ€å¤§
).images[0]


fig, axs = plt.subplots(1, 2, figsize=(12, 5))
axs[0].imshow(init_image);axs[0].set_title('Input Image')
axs[1].imshow(result_image);axs[1].set_title('Result');

```


      0%|          | 0/30 [00:00<?, ?it/s]



    
![png](img/output_6_23_1.png)
    


Inpaintingï¼šæ¥æ”¶ä¸€å¼ æ©æ¨¡å›¾ç‰‡ä½œä¸ºé¢å¤–æ¡ä»¶è¾“å…¥ï¼Œè¯¥æ©æ¨¡å›¾ç‰‡ä¸è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸ä¸€è‡´ï¼Œç™½è‰²åŒºåŸŸè¡¨ç¤ºè¦æ›¿æ¢çš„éƒ¨åˆ†ï¼Œé»‘è‰²åŒºåŸŸè¡¨ç¤ºè¦ä¿ç•™çš„éƒ¨åˆ†ã€‚


```python
pipe = StableDiffusionInpaintPipeline.from_pretrained("runwayml/stable-diffusion-inpainting")
pipe = pipe.to(device)
```


    Downloading (â€¦)ain/model_index.json:   0%|          | 0.00/548 [00:00<?, ?B/s]


    vae/diffusion_pytorch_model.safetensors not found
    


    Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]



    Downloading (â€¦)_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]



    Downloading (â€¦)_checker/config.json:   0%|          | 0.00/4.78k [00:00<?, ?B/s]



    Downloading (â€¦)cheduler_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]



    Downloading (â€¦)e8e7b49d/config.json:   0%|          | 0.00/748 [00:00<?, ?B/s]



    Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]



    Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]



    Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]



    Downloading (â€¦)b49d/vae/config.json:   0%|          | 0.00/552 [00:00<?, ?B/s]



    Downloading pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]



    Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]



    Downloading (â€¦)on_pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]



    Downloading (â€¦)on_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]



    Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]


    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
    


```python
prompt = "A small robot, high resolution, sitting on a park bench"
image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]

fig, axs = plt.subplots(1, 3, figsize=(16, 5))
axs[0].imshow(init_image);axs[0].set_title('Input Image')
axs[1].imshow(mask_image);axs[1].set_title('Mask')
axs[2].imshow(image);axs[2].set_title('Result');

```


      0%|          | 0/50 [00:00<?, ?it/s]



    
![png](img/output_26_1.png)
    


Depth2Imageï¼šé‡‡ç”¨æ·±åº¦é¢„æµ‹æ¨¡å‹æ¥é¢„æµ‹ä¸€ä¸ªæ·±åº¦å›¾ï¼Œè¯¥æ·±åº¦å›¾è¢«è¾“å…¥ä¸ºè·³è¿‡çš„UNetä»¥ç”Ÿæˆå›¾ç‰‡ã€‚


```python
pipe = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth")
pipe = pipe.to(device)
```


    Downloading (â€¦)ain/model_index.json:   0%|          | 0.00/545 [00:00<?, ?B/s]



    Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]



    Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/382 [00:00<?, ?B/s]



    Downloading (â€¦)cheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]



    Downloading (â€¦)_encoder/config.json:   0%|          | 0.00/732 [00:00<?, ?B/s]



    Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]



    Downloading (â€¦)stimator/config.json:   0%|          | 0.00/9.96k [00:00<?, ?B/s]



    Downloading model.safetensors:   0%|          | 0.00/490M [00:00<?, ?B/s]



    Downloading (â€¦)ch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]



    Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/923 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]



    Downloading (â€¦)6ef/unet/config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]



    Downloading (â€¦)d6ef/vae/config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]



    Downloading (â€¦)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]



    Downloading (â€¦)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]



    Downloading model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]



    Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]



```python
prompt = "An oil painting of a man on a bench"
image = pipe(prompt=prompt, image=init_image).images[0]

fig, axs = plt.subplots(1, 2, figsize=(16, 5))
axs[0].imshow(init_image);axs[0].set_title('Input Image')
axs[1].imshow(image);axs[1].set_title('Result');

```


      0%|          | 0/40 [00:00<?, ?it/s]



    
![png](img/output_29_1.png)
    


å¯¹æ¯”Img2Imgç”Ÿæˆçš„å›¾ç‰‡ï¼ŒDepth2Imgç”Ÿæˆçš„å›¾ç‰‡æœ‰ä¸°å¯Œçš„è‰²å½©å˜åŒ–ï¼Œæ•´ä½“ç»“æ„æ›´å¿ äºåŸå›¾ã€‚
