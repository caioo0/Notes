# DDIM åè½¬
---
### æœ¬ç« çŸ¥è¯†ç‚¹

- DDIMé‡‡æ ·çš„å·¥ä½œåŸç†
- ç¡®å®šæ€§é‡‡æ ·å’Œéšæœºæ€§é‡‡æ ·çš„æ¯”è¾ƒ
- DDIMåè½¬çš„ç†è®ºæ”¯æŒ
- ä½¿ç”¨åè½¬æ¥ç¼–è¾‘å›¾åƒ

## å®æˆ˜ï¼šåè½¬

### é…ç½®

é¦–å…ˆå®‰è£…æ‰€éœ€çš„åº“å¹¶ä¸”é…ç½®ç¯å¢ƒï¼Œä»£ç å¦‚ä¸‹ï¼š


```python
!pip install -q transformers diffusers accelerate
```


```python
!pip install -q chardet
```


```python
import torch
import requests
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image
from io import BytesIO
from tqdm.auto import tqdm
from matplotlib import pyplot as plt
from torchvision import transforms as tfms
from diffusers import StableDiffusionPipeline, DDIMScheduler

```


```python
def load_image(url, size=None):
    response = requests.get(url,timeout=1)
    img = Image.open(BytesIO(response.content)).convert('RGB')
    if size is not None:
        img = img.resize(size)
    return img

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

## è½½å…¥ä¸€ä¸ªé¢„è®­ç»ƒè¿‡çš„ç®¡çº¿

é¦–å…ˆï¼šä½¿ç”¨stable diffusion pipelineåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é…ç½®DIMMè°ƒåº¦å™¨ã€‚è€Œåå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¸€æ¬¡é‡‡æ ·ï¼š


```python
# è½½å…¥ä¸€ä¸ªç®¡çº¿
mypipe = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(mypipe).to(device)
# é…ç½®DDIMè°ƒåº¦å™¨
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
# ä»ä¸­é‡‡æ ·ä¸€æ¬¡ï¼Œä»¥ä¿è¯ä»£ç è¿è¡Œæ­£å¸¸
prompt = 'Beautiful DSLR Photograph of a penguin on the beach, golden hour'

negative_prompt = 'blurry, ugly, stock photo'
im = pipe(prompt, negative_prompt=negative_prompt).images[0]
im.resize((256, 256))

```


    Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]


    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
    


      0%|          | 0/50 [00:00<?, ?it/s]





    
![png](img/output_6_3.png)
    




```python
prompt = 'Beautiful DSLR Photograph of a girl on the beach, golden hour'

negative_prompt = 'blurry, lightness, stock photo'
im = pipe(prompt, negative_prompt=negative_prompt).images[0]
im.resize((256, 256))
```


      0%|          | 0/50 [00:00<?, ?it/s]





    
![png](img/output_7_1.png)
    



## DIMM é‡‡æ ·

åœ¨ç»™å®šæ—¶åˆ»$t$,å¸¦æœ‰å™ªå£°çš„å›¾åƒ$x_t$é€šè¿‡å¯¹åŸå§‹å›¾åƒ$x_0$ åŠ ä¸Šé«˜æ–¯å™ªå£°Ïµå¾—åˆ°ï¼š
$$x_t=\sqrt{a_t x_o} + \sqrt{1-a_t Ïµ}$$

Ïµæ˜¯æ–¹å·®å½’ä¸€åŒ–åçš„é«˜æ–¯å™ªå£°ï¼Œ$a_t$åœ¨DDPMè®ºæ–‡ä¸­è¢«ç§°ä¸º$a$,å¹¶è¢«ç”¨äºå®šä¹‰å™ªå£°è°ƒåº¦å™¨ã€‚



```python
# ä½¿ç”¨alphas_cumprodå‡½æ•°å¾—åˆ°alphas
timesteps = pipe.scheduler.timesteps.cpu()
alphas = pipe.scheduler.alphas_cumprod[timesteps]
plt.plot(timesteps, alphas, label='alpha_t');
plt.legend();

```


    
![png](img/output_9_0.png)
    


å½“æ—¶é—´æ­¥ä¸º0æ—¶ï¼Œä»ä¸€å¹…æ— å™ªå£°çš„å¹²å‡€å›¾åƒå¼€å§‹ï¼Œæ­¤æ—¶$\alpha_t=1$ï¼Œå½“è¾¾åˆ°æ›´é«˜çš„æ—¶é—´æ­¥ï¼Œå¾—åˆ°ä¸€å¹…å‡ ä¹å…¨æ˜¯å™ªå£°çš„å›¾åƒï¼Œ$\alpha_t$ä¹Ÿå‡ ä¹ä¸‹é™åˆ°0ã€‚

> å‰å‘è¿‡ç¨‹åœ¨ç»™å®š$x_{t-1}$å’Œ$x_0$çš„æƒ…å†µä¸‹å˜å¾—æ›´åŠ ç¡®å®šã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œéšæœºå™ªå£°$\epsilon_t$å‰é¢çš„ç³»æ•°å˜ä¸º0ï¼Œå¯å¾—åˆ°éšå¼æ¦‚ç‡æ¨¡å‹ï¼Œå…¶ä¸­æ¨¡å‹æ ·æœ¬æ˜¯æ›´åŠ å›ºå®šçš„è¿‡ç¨‹ä»éšå˜é‡ç”Ÿæˆçš„(ä»$x_T$åˆ°$x_0$)ï¼Œè¯¥æ¨¡å‹å‘½åä¸ºå»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰ã€‚


```python
@torch.no_grad()
def sample(prompt, start_step=0, start_latents=None,
           guidance_scale=3.5, num_inference_steps=30,
           num_images_per_prompt=1, do_classifier_free_guidance=True,
           negative_prompt='', device=device):
    # å¯¹æ–‡æœ¬æç¤ºè¯­è¿›è¡Œç¼–ç 
    text_embeddings = pipe._encode_prompt(
            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt
    )
    # è®¾ç½®æ¨ç†æ­¥æ•°
    pipe.scheduler.set_timesteps(num_inference_steps, device=device)
    # åˆ›å»ºéšæœºèµ·ç‚¹
    if start_latents is None:
        start_latents = torch.randn(1, 4, 64, 64, device=device)
        start_latents *= pipe.scheduler.init_noise_sigma

    latents = start_latents.clone()

    for i in tqdm(range(start_step, num_inference_steps)):

        t = pipe.scheduler.timesteps[i]

        # å¦‚æœæ­£åœ¨è¿›è¡ŒCFGï¼Œåˆ™å¯¹éšå±‚è¿›è¡Œæ‰©å±•
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # é¢„æµ‹å™ªå£°
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # è¿›è¡Œå¼•å¯¼
        if do_classifier_free_guidance:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)


        # Normally we'd rely on the scheduler to handle the update step:
        # latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample

        # è‡ªè¡Œå®ç°è°ƒåº¦å™¨
        prev_t = max(1, t.item() - (1000//num_inference_steps)) # t-1
        alpha_t = pipe.scheduler.alphas_cumprod[t.item()]
        alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]
        predicted_x0 = (latents - (1-alpha_t).sqrt()*noise_pred) / alpha_t.sqrt()
        direction_pointing_to_xt = (1-alpha_t_prev).sqrt()*noise_pred
        latents = alpha_t_prev.sqrt()*predicted_x0 + direction_pointing_to_xt

    # è¿›è¡Œåå¤„ç†
    images = pipe.decode_latents(latents)
    images = pipe.numpy_to_pil(images)

    return images
```


```python
prompt = 'Watercolor painting of a beach sunset'
sample(prompt, negative_prompt=negative_prompt, num_inference_steps=50)[0].resize((256, 256))
```

    /usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:237: FutureWarning: `_encode_prompt()` is deprecated and it will be removed in a future version. Use `encode_prompt()` instead. Also, be aware that the output format changed from a concatenated tensor to a tuple.
      deprecate("_encode_prompt()", "1.0.0", deprecation_message, standard_warn=False)
    


      0%|          | 0/50 [00:00<?, ?it/s]


    /usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:430: FutureWarning: The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead
      deprecate("decode_latents", "1.0.0", deprecation_message, standard_warn=False)
    




    
![png](img/output_12_3.png)
    




```python
prompt = 'Van Gogh watercolor starry night'
sample(prompt, negative_prompt=negative_prompt, num_inference_steps=50)[0].resize((256, 256))
```


      0%|          | 0/50 [00:00<?, ?it/s]





    
![png](img/output_13_1.png)
    



## åè½¬

åè½¬çš„ç›®æ ‡æ˜¯å¾—åˆ°â€œå¸¦å™ªâ€çš„éšå¼è¡¨ç¤ºã€‚


```python
input_image = load_image('https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg', size=(512, 512))
input_image
```




    
![png](img/output_15_0.png)
    




```python
input_image_prompt = "Photograph of a puppy on the grass"
```


```python
# ä½¿ç”¨VAEè¿›è¡Œç¼–ç 
with torch.no_grad():
    latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)
l = 0.18215 * latent.latent_dist.sample()
```


```python
# åè½¬
@torch.no_grad()
def invert(start_latents, prompt, guidance_scale=3.5, num_inference_steps=80,
           num_images_per_prompt=1, do_classifier_free_guidance=True,
           negative_prompt='', device=device):

    # å¯¹æç¤ºæ–‡æœ¬è¿›è¡Œç¼–ç 
    text_embeddings = pipe._encode_prompt(
            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt
    )

    # æŒ‡å®šèµ·ç‚¹
    latents = start_latents.clone()

    # ä¿å­˜åè½¬çš„éšå±‚
    intermediate_latents = []

    # è®¾ç½®æ¨ç†æ­¥æ•°
    pipe.scheduler.set_timesteps(num_inference_steps, device=device)

    # åè½¬æ—¶é—´æ­¥
    timesteps = reversed(pipe.scheduler.timesteps)


    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps-1):
        # è·³è¿‡æœ€åä¸€æ¬¡è¿­ä»£
        if i >= num_inference_steps - 1: continue

        t = timesteps[i]

        # å¦‚æœæ­£åœ¨è¿›è¡ŒCFGï¼Œåˆ™å¯¹éšå±‚è¿›è¡Œæ‰©å±•
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # é¢„æµ‹æ®‹ç•™çš„å™ªå£°
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # å¼•å¯¼
        if do_classifier_free_guidance:
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        current_t = max(0, t.item() - (1000//num_inference_steps)) # t
        next_t = t # min(999, t.item() + (1000//num_inference_steps)) # t+1
        alpha_t = pipe.scheduler.alphas_cumprod[current_t]
        alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]

        # åè½¬çš„æ›´æ–°æ­¥ï¼ˆé‡æ–°æ’åˆ—æ›´æ–°æ­¥ï¼Œåˆ©ç”¨å½“å‰éšå±‚å¾—åˆ°æ–°çš„éšå±‚ï¼‰
        latents = (latents - (1-alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (1-alpha_t_next).sqrt() * noise_pred

        # ä¿å­˜éšå±‚
        intermediate_latents.append(latents)

    return torch.cat(intermediate_latents)
```


```python
inverted_latents = invert(l, input_image_prompt,num_inference_steps=50)
inverted_latents.shape
```


      0%|          | 0/49 [00:00<?, ?it/s]





    torch.Size([48, 4, 64, 64])




```python
torch.Size([48, 4, 64, 64])
```




    torch.Size([48, 4, 64, 64])




```python
# è§£ç åè½¬çš„æœ€åä¸€ä¸ªéšå±‚
with torch.no_grad():
    im = pipe.decode_latents(inverted_latents[-1].unsqueeze(0))
pipe.numpy_to_pil(im)[0]
```




    
![png](img/output_21_1_0.png)
    




```python
# å¯ä»¥é€šè¿‡å¸¸è§„è°ƒç”¨æ–¹æ³•ï¼Œå°†åè½¬éšå±‚ä¼ é€’ç»™ç®¡çº¿
pipe(input_image_prompt, latents=inverted_latents[-1][None], num_inference_steps=50, guidance_scale=3.5).images[0]
```


      0%|          | 0/50 [00:00<?, ?it/s]





    
![png](img/output_22_1.png)
    




```python
# ä»ç¬¬20æ­¥çš„éšå¼è¡¨ç¤ºå¼€å§‹ï¼Œå¾—åˆ°çš„ç»“æœè·ç¦»æœ€åˆçš„å›¾ç‰‡å¾ˆè¿‘äº†ï¼
start_step=20
sample(input_image_prompt, start_latents=inverted_latents[-(start_step+1)][None],
       start_step=start_step, num_inference_steps=50)[0]
```


      0%|          | 0/30 [00:00<?, ?it/s]





    
![png](img/output_23_1.png)
    




```python
# æŠŠå°ç‹—æ¢æˆå°çŒ«ï¼Œä»ç¬¬10æ­¥çš„éšå¼è¡¨ç¤ºå¼€å§‹
start_step=10
new_prompt = input_image_prompt.replace('puppy', 'cat')
sample(new_prompt, start_latents=inverted_latents[-(start_step+1)][None],
       start_step=start_step, num_inference_steps=50)[0]
```


      0%|          | 0/40 [00:00<?, ?it/s]





    
![png](img/output_24_1.png)
    




```python
start_step=10
new_prompt = input_image_prompt.replace('puppy', 'horse')
sample(new_prompt, start_latents=inverted_latents[-(start_step+1)][None],
       start_step=start_step, num_inference_steps=50)[0]
```


      0%|          | 0/40 [00:00<?, ?it/s]





    
![png](img/output_25_1.png)
    



##  ç»„åˆå°è£…


```python
def edit(input_image, input_image_prompt, edit_prompt, num_steps=100, start_step=30, guidance_scale=3.5):
    with torch.no_grad(): latent = pipe.vae.encode(tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)*2-1)
    l = 0.18215 * latent.latent_dist.sample()
    inverted_latents = invert(l, input_image_prompt,num_inference_steps=num_steps)
    final_im = sample(edit_prompt, start_latents=inverted_latents[-(start_step+1)][None],
                      start_step=start_step, num_inference_steps=num_steps, guidance_scale=guidance_scale)[0]
    return final_im
```


```python
input_image
```




    
![png](img/output_28_0.png)
    




```python
edit(input_image, 'A puppy on the grass', 'an old grey dog on the grass', num_steps=50, start_step=10)
```


      0%|          | 0/49 [00:00<?, ?it/s]



      0%|          | 0/40 [00:00<?, ?it/s]





    
![png](img/output_29_2.png)
    




```python
face = load_image('https://cdn.pixabay.com/photo/2017/03/05/23/14/girl-2120196_640.jpg', size=(512, 512))
face
```




    
![png](img/output_30_0.png)
    




```python
edit(face, 'A photograph of a face', 'A photograph of a face with sunglasses', num_steps=250, start_step=30, guidance_scale=3.5)
```


      0%|          | 0/249 [00:00<?, ?it/s]



      0%|          | 0/220 [00:00<?, ?it/s]





    
![png](img/output_31_2.png)
    



##  ControlNetçš„ç»“æ„ä¸è®­ç»ƒè¿‡ç¨‹

**ç›®æ ‡ï¼š** ä¸»è¦è§£å†³å›¾åƒç»†èŠ‚çš„æç¤ºï¼ŒåŒ…æ‹¬äººç‰©å››è‚¢çš„è§’åº¦ã€èƒŒæ™¯ä¸­ç‰©ä½“çš„ä½ç½®ã€æ¯ä¸€ç¼•å…‰çº¿ç…§å°„çš„è§’åº¦ç­‰ã€‚

**æ–¹æ¡ˆï¼š** ControlNetèƒ½å¤ŸåµŒå…¥ä»»æ„å·²ç»è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥æä¾›æ›´å¤šæ§åˆ¶æ¡ä»¶ã€‚

> ControlNetçš„åŸºæœ¬ç»“æ„ç”±ä¸€ä¸ªå¯¹åº”çš„é¢„è®­ç»ƒç½‘ç»œçš„ç¥ç»ç½‘ç»œæ¨¡å—å’Œä¸¤ä¸ªâ€œé›¶å·ç§¯â€å±‚ç»„æˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå›ºå®šé¢„è®­ç»ƒç½‘ç»œçš„æƒé‡ï¼Œåªæ›´æ–°ControlNetåŸºæœ¬ç»“æ„ä¸­çš„ç½‘ç»œâ€œå‰¯æœ¬â€å’Œé›¶å·ç§¯å±‚çš„æƒé‡ã€‚ç½‘ç»œâ€œå‰¯æœ¬â€å°†å­¦ä¼šå¦‚ä½•è®©æ¨¡å‹æŒ‰ç…§æ–°çš„æ§åˆ¶æ¡ä»¶æ¥ç”Ÿæˆç»“æœï¼Œè¢«å›ºå®šçš„ç½‘ç»œä¼šä¿ç•™åŸå…ˆç½‘ç»œå·²ç»å­¦ä¼šçš„æ‰€æœ‰çŸ¥è¯†ã€‚

**ControlNetæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼š**
1. æ”¶é›†æ•°æ®é›†ï¼Œå¹¶åŒ…å«å¯¹åº”çš„promptã€‚   
2. å°†promptè¾“å…¥è¢«å›ºå®šçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å°†æ ‡æ³¨å¥½çš„å›¾åƒæ§åˆ¶æ¡ä»¶è¾“å…¥ControlNetï¼Œç„¶åæŒ‰ç…§ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹è¿­ä»£ControlNet blockçš„æƒé‡ã€‚
3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšæœºå°†50%çš„æ–‡æœ¬æç¤ºè¯­æ›¿æ¢ä¸ºç©ºç™½å­—ç¬¦ä¸²ã€‚
4. è®­ç»ƒç»“æŸåï¼Œä½¿ç”¨ControlNetå¯¹åº”çš„å›¾åƒæ§åˆ¶æ¡ä»¶ï¼Œæ¥æ§åˆ¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆæ¡ä»¶çš„å›¾åƒã€‚

## ControlNetå®æˆ˜

###  ç”Ÿæˆäººç‰©çš„è‚–åƒ


```python
!pip install -q diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git
```

      Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m737.4/737.4 kB[0m [31m6.6 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m211.8/211.8 MB[0m [31m2.2 MB/s[0m eta [36m0:00:00[0m
    [?25h  Building wheel for accelerate (pyproject.toml) ... [?25l[?25hdone
    


```python
!pip install -q opencv-contrib-python
```


```python
!pip install -q controlnet_aux
```


```python
from diffusers import StableDiffusionControlNetPipeline
from diffusers.utils import load_image

image = load_image("https://s.yimg.com/ny/api/res/1.2/uf5CdO4GWdFvAlccVVFd9g--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTk1NjtjZj13ZWJw/https://media.zenfs.com/zh-tw/entertainment.nownews.hk/c441463e5276b19ae4588c1690199852")
image
```




    
![png](img/output_36_0.png)
    




```python
import cv2
from PIL import Image
import numpy as np

image = np.array(image)

low_threshold = 100
high_threshold = 200

# æå–å›¾ç‰‡è¾¹ç¼˜çº¿æ¡
image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
canny_image
```




    
![png](img/output_37_0.png)
    




```python
# ä½¿ç”¨åŠç²¾åº¦èŠ‚çº¦è®¡ç®—èµ„æºï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16).to(device)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet,
                                                        torch_dtype=torch.float16).to(device)
```


    Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]


    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
    


```python
# ä½¿ç”¨é€Ÿåº¦æœ€å¿«çš„æ‰©æ•£æ¨¡å‹è°ƒåº¦å™¨UniPCMultistepScheduler
from diffusers import UniPCMultistepScheduler

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
```


```python
def image_grid(imgs, rows, cols):
    assert len(imgs) == rows * cols

    w, h = imgs[0].size
    grid = Image.new("RGB", size=(cols * w, rows * h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i % cols * w, i // cols * h))

    return grid
```


```python
prompt = ", best quality, extremely detailed"
prompt = [t + prompt for t in ["Sandra Oh", "Kim Kardashian", "rihanna", "taylor swift"]]
generator = [torch.Generator(device=device).manual_seed(2) for i in range(len(prompt))]

output = pipe(
    prompt,
    canny_image,
    negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt),
    generator=generator,
    num_inference_steps=30
)

image_grid(output.images, 2, 2)
```


      0%|          | 0/30 [00:00<?, ?it/s]


### æå–èº«ä½“å§¿æ€


```python
urls = ["yoga1.jpeg", "yoga2.jpeg", "yoga3.jpeg", "yoga4.jpeg"]
imgs = [
    load_image("https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/" + url) for url in urls
]
image_grid(imgs, 2, 2)
```




    
![png](img/output_43_0.png)
    




```python
from controlnet_aux import OpenposeDetector

model = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")

poses = [model(img) for img in imgs]
image_grid(poses, 2, 2)
```




    
![png](img/output_44_0.png)
    




```python
from diffusers import ControlNetModel, UniPCMultistepScheduler, StableDiffusionControlNetPipeline

controlnet = ControlNetModel.from_pretrained("fusing/stable-diffusion-v1-5-controlnet-openpose",
                                             torch_dtype=torch.float16).to(device)
```


```python
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    model_id,
    controlnet = controlnet,
    torch_dtype=torch.float16
).to(device)
```


    Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]


    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
    `text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
    


```python
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()
```


```python
device
```




    device(type='cuda')




```python
generator = [torch.Generator(device=device).manual_seed(10) for i in range(4)]
prompt = "super-hero character, best quality, extremely detailed"

output = pipe(
    [prompt] * 4,
    poses,
    negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * 4,
    generator=generator,
    num_inference_steps=20
)

```


      0%|          | 0/20 [00:00<?, ?it/s]



```python
image_grid(output.images, 2, 2)
```




    
![png](img/output_50_0.png)
    




```python


generator = [torch.Generator(device=device).manual_seed(10) for i in range(4)]
prompt = "Super villain, best quality, very rough. Change it"

output = pipe(
    [prompt] * 4,
    poses,
    negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * 4,
    generator=generator,
    num_inference_steps=20
)

```


      0%|          | 0/20 [00:00<?, ?it/s]


    Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.
    


```python
image_grid(output.images, 2, 2)
```




    
![png](img/output_52_0.png)
    


