# Transformer面试题解析

**1、Transformer 为何使用多头注意力机制？ （为什么不适用一个头）**

答案：

多头保证了transformer可以注意不同子空间的信息，捕捉到更加丰富的特征信息。