# 深度学习基础

> （本学习笔记来源于DataWhale-第52期组队学习：[强化学习](https://linklearner.com/datawhale-homepage/#/learn/detail/91)） , [B站视频讲解](https://www.bilibili.com/video/BV1HZ4y1v7eX) 观看地址

## 深度学习和强化学习

强化学习和深度学习的结合通常被称为深度强化学习（Deep Reinforcement Learning, DRL）。

深度学习被用来表示和学习智能体的策略或者环境模型

而强化学习负责通过与环境的交互来优化这些策略。

1. **策略表示与学习**：深度网络用于表示智能体的策略，即给定一个状态，应该采取什么行动。通过大量的训练数据，网络可以学习复杂的状态-行动映射。
2. **值函数估计**：值函数是评估给定状态或状态-行动对的好坏的方法。深度网络可以用于学习这些值函数，提供了一个强大的函数逼近器来估计值函数。
3. **模型学习**：在某些情况下，智能体可能需要学习环境的模型。深度网络可以用来学习这种模型，使智能体能够预测环境的动态。

![image-20231120151147598](.\img\image-20231120151147598.png)

## 深度学习模型

### 2.1 线性回归

线性回归是一种有监督的学习算法，旨在采用线性方法来建模因变量和自变量之间的关系。换句话说，它的目标是拟合一条最好地捕捉数据关系的线性趋势线，并且，从这条线，它可以预测目标值可能是什么。


![image-20231120152006545](.\img\image-20231120152006545.png)


**线性回归如何工作的呢?**

1. 拟合数据（如上图所示）。

2. 计算点之间的距离（图上的红点是点，绿线是距离），然后求平方，然后求和（这些值是平方的，以确保负值不会产生错误的值并阻碍计算）。这是算法的误差，或者更好地称为残差

3. 存储迭代的残差

4. 基于一个优化算法，使得该线稍微“移动”，以便该线可以更好地拟合数据。

5. 重复步骤2-5，直到达到理想的结果，或者剩余误差减小到零。

这种拟合直线的方法称为**最小二乘法**。


**线性回归数学公式**

若用 $x_i^k$表示第$𝑘$个样本的第$𝑖$个属性，则线性模型一般形式为：

$$
f(x^k) = w_1x_1^k +  w_2x_2^k + ... +  w_mx_m^k +b = \sum^{m}_{i=1}w_ix_i^k + b
$$




线性回归学习的对象就是权重向量𝑤和偏置向量𝑏。如果用最小均方 误差来衡量预测值与样本标签的差距，那么线性回归学习的目标可以表示为：

$$
(w^*，b^*) = argmin_(w,b)\sum^{n}_{k=1}(f(x^k) - y^k)^2 = argmin_(w,b)\sum^{n}{k=1}(w^Tx^k + b -y^k)^2
$$

### 2.2 梯度下降（gradient descent）


梯度下降核心内容是对自变量进行不断的更新（针对w和b求偏导），使得目标函数不断逼近最小值的过程：
$$
      w \leftarrow w - a \frac{\partial{L}}{\partial{w}}
$$

$$
b \rightarrow b - a \frac{\partial{L}}{\partial{b}}
$$


简写形式：$y=w^Tx+b$

广义线性模型：$y=g^{-1}(w^Tx_b)$，其中$g(\cdot)$时单调可微函数。

### 2.3 逻辑回归

逻辑回归（LR,Logistic Regression）是传统机器学习中的一种分类模型，由于LR算法具有简单、高效、易于并行且在线学习（动态扩展）的特点，在工业界具有非常广泛的应用。

逻辑回归通过在线性回归模型中引入Sigmoid函数，将线性回归的不确定范围的连续输出值映射到（0，1）范围内，成为一个概率预测问题。

LR目标函数：

$$
h_{\theta}(x) = g(\theta^TX)
$$

其中Sigmoid函数g(z)的定义如下：

$$
g(z) = \frac{1}{1+e^{-z}}
$$

Sigmoid函数的函数图像为：

![image-20231120152142193](.\img\image-20231120152142193.png)

### **2.4 全连接神经网络**

全连接网络（fully connected network）也称作多层感知机（fully connected network），也称作多层感知机（$\text{multi-layer perceptron，MLP}$），是最基础的深度神经网络模型。

全连接网络由多层结构的感知器递阶组成的输入值向前传播的网络，也被称为**前馈网络**或**正向传播网络**。

以三层结构的多层感知器为例，它由输入层、中间层及输出层组成

- 与M-P模型相同，中间层的感知器通过权重与输入层的各单元相连接，通过阈值函数计算中间层各单元的输出值
- 中间层与输出层之间同样是通过权重相连接




####  MLP神经网络的结构和原理

MLP包括包括三层：**输入层、隐层和输出层**

MLP神经网络不同层之间是**全连接**的（**全连接**的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。


![image-20231120153613271](C:\Users\Jochoi\AppData\Roaming\Typora\typora-user-images\image-20231120153613271.png)



神经网络三个基本要素：**权重**、**偏置**和**激活函数**

**权重:** 神经元之间的连接强度由权重表示，权重的大小表示可能性的大小

**偏置：** 偏置的设置是为了正确分类样本，是模型中一个重要的参数，即保证通过输入算出的输出值不能随便激活。

**激活函数:** 起非线性映射的作用，其可将神经元的输出幅度限制在一定范围内，一般限制在（-1 - 1）或（0 - 1）之间。最常用的激活函数是Sigmoid函数，其可将（-∞，+∞）的数映射到（0 - 1）的范围内。

### 2.5  卷积神经网络

全连接神经网络

- 参数数量太多 
- 没有利用像素之间的位置信息
- 网络层数限制
- 所需内存和计算量巨大。

卷积神经网络

- 局部连接: 每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
- 权值共享:一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数
- 下采样 : 使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。


#### 卷积（Convolution）


卷积（Convolution），也叫褶积，是分析数学中一种重要的运算．

##### 1.卷积分类:

**连续卷积：**  

$(f*g)(n)= \int^{\infty}_{-\infty}f(\tau)g(n-\tau)d\tau ， n=\tau + (n-\tau)$


**离散卷积：**  

$(f*g)(n)= \sum^{\infty}_{\tau=-\infty}f(\tau)g(n-\tau) ， n=\tau + (n-\tau)$


**卷积应用场景**

- 统计学中加权平均法
- 概率论中两个独立变量之和概率密度的计算
- 信号处理中的线性系统
- 物理学的线性系统
- 图像处理中的应用(卷积神经网络)


其他卷积：

- 转置卷积/微步卷积：低维特征映射到高维特征

- 空洞卷积：为了增加输出单元的感受野，通过给卷积核插入“空洞”来变相地增加其大小。


实际应用中卷积网络中的卷积核常见为离散卷积运算.




在信号处理或图像处理中，经常使用一维或二维卷积．

##### 2.一维卷积

一维卷积经常用在信号处理中，用于计算信号的延迟累积. 


假设一个信号发生器每个时刻 $𝑡$ 产生一个信号 $𝑥_𝑡$，其信息的衰减率为 $𝑤_𝑘$，即在 $𝑘 − 1$ 个时间步长
后，信息为原来的$𝑤_𝑘$ 倍．假设$𝑤_1 = 1, 𝑤_2 = 1/2, 𝑤_3 = 1/4$，那么在时刻 $𝑡$ 收到的信号 $𝑦_𝑡$ 为当前时刻产生的信息和以前时刻延迟信息的叠加

$$
𝑦_𝑡 = 1 × 𝑥_𝑡 + 1/2 × 𝑥_{𝑡−1} + 1/4 × 𝑥_{𝑡−2}   \\
= 𝑤_1 × 𝑥_𝑡 + 𝑤_2 × 𝑥_{𝑡−1} + 𝑤_3 × 𝑥_{𝑡−2}  \\
= \sum^{3}_{𝑘=1}𝑤_𝑘𝑥_{𝑡−𝑘+1}.
$$

我们把$𝑤_1, 𝑤_2, ⋯$称为`滤波器`（Filter）或`卷积核`（Convolution Kernel）．

假设滤波器长度为$𝐾$，它和一个信号序列$𝑥_1, 𝑥_2, ⋯$的**卷积**为


$$
𝑦_𝑡 =  \sum^{k}_{𝑘=1}𝑤_𝑘𝑥_{𝑡−𝑘+1}.
$$


##### 3 二维卷积

二维卷积运算：给定二维的图像$I$作为输入，二维卷积核$K$，卷积运算可表示为 
$$
S(i,j)=(I∗K)(i,j)=∑m∑nI(i−m,j−n)K(m,n)
$$
卷积核需要进行上下翻转和左右反转，`吴恩达的说法，RNN中二维卷积核可以不必做任何翻转,并不影响结果`


卷积通常有三种类型：full卷积、same卷积和valid卷积.这里就不多展开,想了解可以阅读书籍:[图解深度学习与神经网络：从张量到TensorFlow实现](https://github.com/XQLuck/DeepLearning-Notes/blob/master/Books/%E3%80%8A%E5%9B%BE%E8%A7%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E4%BB%8E%E5%BC%A0%E9%87%8F%E5%88%B0TensorFlow%E5%AE%9E%E7%8E%B0%E3%80%8B_%E5%BC%A0%E5%B9%B3_2018-09-01.pdf)


#### 卷积神经网络基本原理

卷积神经网络的基本结构大致包括：`卷积层`、`激活函数`、`池化层`、`全连接层`、`输出层`等。

##### 1 卷积层


卷积层的运算过程如下图，用一个卷积核扫完整张图片：

![image-20231120152813412](.\img\image-20231120152813412.png)

这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。


**步长（Stride）**

Stride的作用：是成倍缩小尺寸，而这个参数的值就是缩小的具体倍数，比如步幅为2，输出就是输入的1/2；步幅为3，输出就是输入的1/3。以此类推。

【卷积核的大小一般为奇数 * 奇数】 $1* 1，3* 3，5 * 5，7 * 7$ 都是最常见的。这是为什么呢？为什么没有 偶数 * 偶数？

```md
 奇数卷积核中心只有一列，可以满足两边对称，比较容易判断。好像跟垂直边缘和水平边缘有关。
```

**数据填充（padding）**

两种padding：

- valid padding：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界
- same padding：进行填充，允许卷积核超出原始图像边界，并使得卷积后结果的大小与原来的一致


**感受野**

在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致，这一点会在下面举例具体说明。


![image-20231120152838378](.\img\image-20231120152838378.png)


在图 2 中展现的卷积神经网络的一部分，其中的红色为输入数据，假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。其中还有一点需要注意，对应一个感受野有75个权重，这75个权重是通过学习进行更新的，所以很大程度上这些权值之间是不相等（也就对于同一个卷积核，它对于与它连接的输入的每一层的权重都是独特的，不是同样的权重重复输入层层数那么多次就可以的）。在这里相当于前面的每一个层对应一个传统意义上的卷积模板，每一层与自己卷积模板做完卷积之后，再将各个层的结果加起来，再加上偏置，注意是一个偏置，无论输入输入数据是多少层，一个卷积核就对应一个偏置。


**总结:**

卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。注意是计算量而不是参数量。

**卷积层的作用:**

1. 滤波器的作用或者说是卷积的作用
2. 可以被看做是神经元的一个输出
3. 降低参数的数量

##### 2 激活函数

激活函数是用来加入非线性因素，提高网络表达能力，卷积神经网络中最常用的是ReLU，Sigmoid使用较少。


1. ReLU函数



$$
f(z)=\left\{\begin{array}{cc} 
		z, & if z>=0\\ 
		0, & if z<0\ 
\end{array}\right.
$$



ReLU函数的优点：

- 计算速度快，ReLU函数只有线性关系，比Sigmoid和Tanh要快很多
- 输入为正数的时候，不存在梯度消失问题


ReLU函数的缺点：

- 强制性把负值置为0，可能丢掉一些特征
- 当输入为负数时，权重无法更新，导致“神经元死亡”(学习率不 要太大)

2. Parametric ReLU

$$
f(z)=\left\{\begin{array}{cc} 
		{z}, & if z>=0\\ 
		\alpha{z}, & if  z<0\ 
\end{array}\right.
$$

当 𝛼=0.01 时，称作Leaky ReLU
当 𝛼 从高斯分布中随机产生时，称为Randomized ReLU(RReLU)

PReLU函数的优点：

- 比sigmoid/tanh收敛快
- 解决了ReLU的“神经元死亡”问题

PReLU函数的缺点：需要再学习一个参数，工作量变大

3. ELU函数

$$
f(z)=\left\{\begin{array}{cc} 
		{z}, & if z>=0\\ 
		\alpha{e^z-1}, & if z<0\ 
\end{array}\right.
$$

ELU函数的优点：

- 处理含有噪声的数据有优势
- 更容易收敛

ELU函数的缺点：计算量较大，收敛速度较慢

- CNN在卷积层尽量不要使用Sigmoid和Tanh，将导致梯度消失。
- 首先选用ReLU，使用较小的学习率，以免造成神经元死亡的情况。
- 如果ReLU失效，考虑使用Leaky ReLU、PReLU、ELU或者Maxout，此时一般情况都可以解决

特征图

- 浅层卷积层：提取的是图像基本特征，如边缘、方向和纹理等特征
- 深层卷积层：提取的是图像高阶特征，出现了高层语义模式，如“车轮”、“人脸”等特征
  

##### 3 池化层

池化层（Pooling Layer）也叫子采样层（Subsampling Layer）：促进特征选择，降低特征/参数数量，避免过拟合

池化（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括


常用汇聚函数：

1. 最大汇聚（Maximum Pooling/ Max Pooling）：$y\_{m, n}^{d}=\max \_{i \in R\_{m, n}^{d}} x\_{i}$
2. 平均汇聚（Mean Pooling）：$y\_{m, n}^{d}=\frac{1}{\left|R\_{m, n}^{d}\right|} \sum\_{i \in R\_{m, n}^{d}} x\_{i}$

池化层可以看作特殊的卷积层：卷积核大小为$K\times K$，步长为$S\times S$，卷积核为max函数或mean函数