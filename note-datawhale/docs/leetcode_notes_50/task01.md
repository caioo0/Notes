# task01: 数据结构与算法简介、leetcode入门及攻略

> 关于笔记，主要来自[datawhale-Leetcode算法笔记](https://datawhalechina.github.io/leetcode-notes/#/ch01/01.01/01.01.02-Algorithm-Complexity)

## 1、数据结构与算法

**数据结构是程序的骨架，而算法则是程序的灵魂。**

**程序  = 算法 + 数据结构**

- 算法：解决问题的方法或者过程
- 数据结构：是数据的计算机表示和相应的一组操作
- 程序：算法和数据结构的具体实现

> **为什么要学习算法和数据结构？**
>
> 学习算法和数据结构，是为了学会在编程中从时间复杂度、空间复杂度方面考虑解决方案，训练自己的逻辑思维，从而写出高质量的代码，以此提升自己的编程技能，获取更高的工作回报。

### 1.1 数据结构

> 数据结构（Data Structure）： 带有结构特性的数据元素的集合。

简单而言，**数据结构** 是指 数据的**组织结构、用来组织、存储数据。**

数据结构研究的是数据的**逻辑结构、物理结构**以及它们之间的相互关系，并对这种结构定义相应的运算。

我们可以按照数据的**逻辑结构和物理结构**来进行分类

#### 1.1.1 数据的逻辑结构

> 逻辑结构（Logical Structure）: 数据元素之间的相互关系

数据的逻辑结构分为以下四种：

1. **集合结构** 

   **特点：**

   - 数据元素属于一个集合，除此之外无其他关系。
   - 集合结构中的数据元素无序、并且每个数据元素都是唯一的，集合中没有相同的数据元素。

   

![image-20230912095941605](.\img\image-20230912095941605.png)

2. **线性结构**

   **特点：**

   - 数据之间是一对一关系
   - 线性结构中的数据元素（除了第一个和最后一个元素），左侧和右侧分别只有一个数据与其相邻。线性结构类型包括：**数组、链表，以及由它们衍生出来的栈、队列、哈希表**。

   ![image-20230912101124849](.\img\image-20230912101124849.png)

3. **树形结构**

   **特点**

   - 数据元素之间是「一对多」的层次关系。
   - 最简单的树形结构是二叉树。这种结构可以简单的表示为：根， 左子树， 右子树。 左子树和右子树又有自己的子树。当然除了二叉树，树形结构类型还包括：多叉树、字典树等。

   ![image-20230912101153647](.\img\image-20230912101153647.png)

4. **图形结构**

   特点：

   - 数据元素之间是「多对多」的关系。

   - 图形结构是一种比树形结构更复杂的非线性结构，用于表示物件与物件之间的关系。一张图由一些小圆点（称为 **「顶点」** 或 **「结点」**）和连结这些圆点的直线或曲线（称为 **「边」**）组成。

     在图形结构中，任意两个结点之间都可能相关，即结点之间的邻接关系可以是任意的。图形结构类型包括：无向图、有向图、连通图等。

![image-20230912101332003](.\img\image-20230912101332003.png)

#### 1.1.2 数据的物理结构

物理结构（Physical Structure）:  数据的逻辑结构在计算机中的存储方式。

计算机内有多种存储结构，采用最多的是这两种结构：**「顺序存储结构」**、**「链式存储结构」**。



1. **顺序存储结构**

​    **定义：**将数据元素存放在一片地址连续的存储单元里，数据元素之间的逻辑关系通过数据元素的存储地址来直接反映。

​	**特点：**

	-  在顺序存储结构中，逻辑上相邻的数据元素在物理地址上也必然相邻 。
	-  简单、易理解，且实际占用最少的存储空间。
	-  需要占用一片地址连续的存储单元；并且存储分配要事先进行；另外对于一些操作的时间效率较低（移动、删除元素等操作）。

![image-20230912102143964](.\img\image-20230912102143964.png)

2. **链式存储结构**

   **定义：**将数据元素存放在任意的存储单元里，存储单元可以连续，也可以不连续。

![image-20230912140549440](.\img\image-20230912140549440.png)

链式存储结构中，逻辑上相邻的数据元素在物理地址上可能相邻，可也能不相邻。其在物理地址上的表现是随机的。

链式存储结构中，一般将每个数据元素占用的若干单元的组合称为一个链结点。每个链结点不仅要存放一个数据元素的数据信息，还要存放一个指出这个数据元素在逻辑关系的直接后继元素所在链结点的地址，该地址被称为指针。换句话说，数据元素之间的逻辑关系是通过指针来间接反映的。



**优点：**存储空间不必事先分配，在需要存储空间的时候可以临时申请，不会造成空间的浪费；一些操作的时间效率远比顺序存储结构高（插入、移动、删除元素）。

**缺点：**不仅数据元素本身的数据信息要占用存储空间，指针也需要占用存储空间，链式存储结构比顺序存储结构的空间开销大。

### 1.2 算法

算法（Algorithm）:  解决特定问题求解步骤的准确而完整的描述，在计算中表现为一系列指令的集合，算法代表着用系统的方法描述解决问题的策略机制。

简单而言，**算法**指的是解决问题的方法。

算法可以用自然语言、编程语言（Python、C、C++、Java等）描述，也可以用**伪流程、流程图**来表示。

#### 1.2.1 算法基本特性

- 输入
- 输出
- 有穷性
- 确定性
- 可行性

#### 1.2.2 算法追求的目标

不同的算法成本不同，一个优秀的算法至少应该追求以下两个目标：

1. **所需运行时间更少（时间复杂度更低）**
2. **占用内存空间更小（空间复杂度更低）**

一个好的算法还应该追求以下目标：

1. **正确性**：正确性是指算法能够满足具体问题的需求，程序运行正常，无语法错误，能够通过典型的软件测试，达到预期的需求。
2. **可读性**：可读性指的是算法遵循标识符命名规则，简洁易懂，注释语句恰当，方便自己和他人阅读，便于后期修改和调试。
3. **健壮性**：健壮性指的是算法对非法数据以及操作有较好的反应和处理。

这 3 个目标是算法的基本标准，是所有算法所必须满足的。一般我们对好的算法的评判标准就是上边提到的 **所需运行时间更少（时间复杂度更低）**、**占用内存空间更小（空间复杂度更低）**。

#### 1.2.3 算法效率评估

效率评估方法主要分为两种：**实际测试、理论估算**。

##### 实际测试： 这种评估方式能够反映真实情况，但也存在较大局限性。

理论估算：这种估算方法被称为「渐近复杂度分析 asymptotic complexity analysis」，简称「复杂度分析」。

复杂度分析体现算法运行所需的时间（空间）资源与输入数据大小之间的关系。**它描述了随着输入数据大小的增加，算法执行所需时间和空间的增长趋势**。这个定义有些拗口，我们可以将其分为三个重点来理解。

- “时间和空间资源”分别对应「时间复杂度 time complexity」和「空间复杂度 space complexity」。
- “随着输入数据大小的增加”意味着复杂度反映了算法运行效率与输入数据体量之间的关系。
- “时间和空间的增长趋势”表示复杂度分析关注的不是运行时间或占用空间的具体值，而是时间或空间增长的“快慢”。

**复杂度分析克服了实际测试方法的弊端**，体现在以下两个方面。

- 它独立于测试环境，分析结果适用于所有运行平台。
- 它可以体现不同数据量下的算法效率，尤其是在大数据量下的算法性能。



### 1.3 迭代与递归



### 1.4 数据结构与算法小结

#### 1.4.1 数据结构总结

数据结构分为 **逻辑结构** 和 **物理结构**。

- 逻辑结构分为：**集合结构**、**线性结构**、**树形结构**、**图形结构**。
- 物理结构分为：**顺序存储结构**、**链式存储结构**。

逻辑结构指的是数据之间的 **关系**，物理结构指的是这种关系 **在计算机中的表现形式**。

>  例如：线性表中的「栈」，其数据元素之间的关系是一对一的，除头和尾结点之外的每个结点都有唯一的前驱和唯一的后继，这体现的是逻辑结构。而对于栈中的结点来说，可以使用顺序存储（也就是 **顺序栈**）的方式存储在计算机中，其结构在计算机中的表现形式就是一段连续的存储空间，栈中每个结点和它的前驱结点、后继结点在物理上都是相邻的。当然，栈中的结点也可以使用链式存储（也即是 **链式栈**），每个结点和它的前驱结点、后继结点在物理上不一定相邻，每个结点是靠前驱结点的指针域来进行访问的。

#### 1.4.2 算法总结

**算法** 指的就是解决问题的方法。

算法是一系列的运算步骤，这些运算步骤可以解决特定的问题。

算法拥有 5 个基本特性：**输入**、**输出**、**有穷性**、**确定性**、**可行性**。

算法追求的目标有 5 个：**正确性**、**可读性**、**健壮性**、**所需运行时间更少（时间复杂度更低）**、**占用内存空间更小（空间复杂度更低）**。

## 2、算法复杂度

**定义：**

算法复杂度（Algorithm complexity）： 在问题的输入规模为n的条件下，程序的时间使用情况和空间使用情况。

算法所追求的就是 **所需运行时间更少（时间复杂度更低）**、**占用内存空间更小（空间复杂度更低）**

算法分析，就是从**运行时间情况、空间使用情况**两方面对算法进行分析。

比较算法的两种方法：

1. 事后统计
2. 预先统计（推荐），**不考虑编程语言，计算机运行速度，只关心随着问题规模 $n$ 扩大时，时间开销、空间开销的增长情况**。

>  **问题规模 $n：$** ：算法问题输入的数据量大小。
>
> - 排序算法中：  $n$表示需要排序的元素数量。
> - 查找算法中： $n$ 表示查找范围内的元素总数：比如数组大小、二维矩阵大小、字符串长度、二叉树节点数、图的节点数、图的边界点等。
> - 二进制计算相关算法中： $n$ 表示二进制的展开宽度。

### 2.1 时间复杂度

定义：

**时间复杂度（Time Complexity）**：在问题的输入规模为 $n$ 的条件下，算法运行所需要花费的时间，可以记作为 $T(n)$。

**基本操作** ：算法执行中的每一条语句。每一次基本操作都可在常数时间内完成。基本操作是一个运行时间不依赖于操作数的操作。

给定一个输入大小为$n$的函数：

```python
def algorithm(n):
    fact = 1                +++++ 1 
    // 循环 n 次 
    for i in range(1,n+1):  +++++ 1（每轮都执行i ++ ）
        fact *= i			+++++ 1
    return fact 			+++++ 1 
```

上述算法的执行次数： $2n+2$ ,用f(n)表示： $f(n) = 2n +2$



**时间复杂度：**$T(n) = O(f(n))$。 它表示的是随着问题规模 n 的增大，算法执行时间的增长趋势跟  $f(n)$相同。

$O$是一种渐进符号，$T(n)$称作算法的渐进时间复杂度（Asymptotic Time Complexity）,简称为时间复杂度。

时间复杂度分析本质上是计算“操作数量函数（T(n)）”的渐进上界， 具有明确的数学定义：

> 若存在正实数c和实数$n_0$,使得对于所有的$ n> n_0$ ,均有$T(n) \leqslant c.f(n)$,则可认为$f(n)$给出了$T(n)$的一个渐进上界，记为$ T(n) = 0(f(n))$。

#### 2.1.1 渐进符号

> 渐进符号（Asymptotic Symbol）:刻画函数的增长速度，并且只保留**最高阶幂**。忽略**低阶幂**、**系数**、**常量**等。

经常用到的渐进符号有三种： **Θ 渐进紧确界符号、$O$渐进上界符号、Ω 渐进下界符号**。

#### 2.1.2 推算方法

渐近上界的数学味儿有点重，如果你感觉没有完全理解，也无须担心。因为在实际使用中，我们只需要掌握推算方法，数学意义就可以逐渐领悟。

确定 $f(n)$之后，我们便可得到时间复杂度 $O(f(n))$ 。那么如何确定渐近上界 $f(n)$呢？总体分为两步：**首先统计操作数量，然后判断渐近上界。**

##### 1.  第一步：统计操作数量

针对代码，逐行从上到下计算即可。然而，由于上述$ c⋅f(n)$ 中的常数项 $c$ 可以取任意大小，**因此操作数量 $T(n)$ 中的各种系数、常数项都可以被忽略**。根据此原则，可以总结出以下计数简化技巧。

1. **忽略 $T(n)$ 中的常数项**。因为它们都与 $n$无关，所以对时间复杂度不产生影响。
2. **省略所有系数**。例如，循环 2$n$次、5$n$+1 次等，都可以简化记为 � 次，因为 � 前面的系数对时间复杂度没有影响。
3. **循环嵌套时使用乘法**。总操作数量等于外层循环和内层循环操作数量之积，每一层循环依然可以分别套用第 `1.` 点和第 `2.` 点的技巧。

给定一个函数，我们可以用上述技巧来统计操作数量。

```python
def algorithm(n: int):
    a = 1      # +0（技巧 1）
    a = a + n  # +0（技巧 1）
    # +n（技巧 2）
    for i in range(5 * n + 1):
        print(0)
    # +n*n（技巧 3）
    for i in range(2 * n):
        for j in range(n + 1):
            print(0)

```

以下公式展示了使用上述技巧前后的统计结果，两者推出的时间复杂度都为$O(n^2)$。


$$
\begin{aligned}
T(n) & = 2n(n + 1) + (5n + 1) + 2 & \text{完整统计 (-.-|||)} \newline
& = 2n^2 + 7n + 3 \newline
T(n) & = n^2 + n & \text{偷懒统计 (o.O)}
\end{aligned}
$$

##### 2. 第二步：判断渐进上界

**时间复杂度由多项式 $T(n)$ 中最高阶的项来决定**。这是因为在 $n$ 趋于无穷大时，最高阶的项将发挥主导作用，其他项的影响都可以被忽略。

一些常见的例子：

![image-20230915180849751](.\img\image-20230915180849751.png)

#### 2.1.3 常见类型

设输入数据大小为 $n$ ，常见的时间复杂度类型如图 2-9 所示（按照从低到高的顺序排列）。
$$
\begin{aligned}
O(1) < O(\log n) < O(n) < O(n \log n) < O(n^2) < O(2^n) < O(n!) \newline
\text{常数阶} < \text{对数阶} < \text{线性阶} < \text{线性对数阶} < \text{平方阶} < \text{指数阶} < \text{阶乘阶}
\end{aligned}
$$
![image-20230915181106231](.\img\image-20230915181106231.png)
$$
图为 常见的时间复杂度类型
$$

##### 1. 常数阶$O(1)$

常数阶的操作数量与输入数据大小 $n$ 无关，即不随着  $n$ 的变化而变化

在以下函数中，尽管操作数量`size`可能很大，但由于其与输入数据大小$n$无关，因此时间复杂度仍为$O(1):$

```python
def constant(n:int) -> int:
	"""常数阶"""
	count = 0 
	size  = 100000
	for _ in range(size):
		count += 1
	return count 
```

##### 2. 线性阶$0(n)$

线性阶的操作数量相对于输入数据大小$n$以线性级别增长。线性阶通常出现在单层循环中：

```python
def linear(n:int) ->int:
	"""线性阶"""
	Count = 0
	fot _ in range(n):
		Count += 1
	return Count 
```

遍历数组和遍历链表等操作的时间复杂度均为$O(n)$,其中$n$ 为数组或链表的长度：

```python
def array_traversal(nums:list[int]) -> int:
	"""线性阶（遍历数组）"""
	count = 0
	# 循环次数与数组长度成正比
	for num in nums:
		count += 1
    return count 
```

值得注意的是，**输入数据大小 $n$ 需根据输入数据的类型来具体确定**。比如在第一个示例中，变量 $n$ 为输入数据大小；在第二个示例中，数组长度 $n$ 为数据大小。

##### 3. 平方阶$O(n^2)$

平方阶的操作数量相对于输入数据大小 $n$ 以平方级别增长。平方阶通常出现在嵌套循环中，外层循环和内层循环都为$O(n)$, 因此总体为$O(n^2)$

```python
def quadratic(n:int) -> int:
	"""平方阶"""
	count = 0
	# 循环次数与数组长度成平方关系
	for i in range(n):
		for j in range(n):
			count +=1
    return count 
```

对比常数阶、线性阶和平方阶三种时间复杂度

![image-20230916103235206](.\img\image-20230916103235206.png)
$$
图为常数阶、线性阶和平方阶的时间复杂度
$$
以冒泡排序为例，外层循环执行$n-1$,内层循环$n-1,n-2,...,2,1$次，平均为$n/2$次，因此时间复杂度为$O((n-1)n/2) = O(n^2)$

```python
def bubble_sort(nums:list[int]) ->int:
	"""平方阶（冒泡排序）"""
	count = 0 # 计数器
	# 外循环：未排序区间为[0,i]
	for i in range(Len(nums) - 1, 0, -1):
		# 内循环：将未排序区间【0，i】中的最大元素交换至该区间的最右端
		for j in range(i):
			if nums[j] > nums[j+1]:
				# 交换 nums[j] 与 nums[j+1]
				tmp: int = nums[j]
				nums[j]  = nums[j+1]
                nums[j+1]  =tmp
                count +=3  # 元素交换包含了3个单位操作
    return count 
```



##### 4. 指数阶$O(2^n)$

 生物学的"细胞分裂"是指数阶增长的典型例子：初始状态为1一个细胞，分裂一轮后变成2个，分裂两轮后4个，一次类推，分类N轮后有$2^n$个细胞。

以下代码模拟了细胞分裂的过程，时间复杂度为 $O(2^n)$ 。

```python
def exponential(n:int)-> int:
	"""指数阶(循环实现)"""
	count = 0
	base = 1
	# 细胞每轮一分为二，形成数列1,2,4,8,...,$2^(n-1)$
	for _ in range(n):
        for _ in range(base):
            count += 1
        base *= 2
    # count = 1 + 2 + 4 + 8 + .. + 2^(n-1) = 2^n - 1
    return count
```

![image-20230916113902545](.\img\image-20230916113902545.png)
$$
图 指数阶的时间复杂度
$$
在实际算法中，指数阶常出现于递归函数中。例如在以下代码中，其递归地一分为二，经过 $n$ 次分裂后停止：

```python
def exp_recur(n:int) ->int:
	"""指数阶（递归实现）"""
	if n ==1
		return 1
	return exp_recur(n - 1) + exp_recur(n - 1) + 1
```

指数阶增长非常迅速，在穷举法（暴力搜索、回溯等）中比较常见。对于数据规模较大的问题，指数阶是不可接受的，通常需要使用动态规划或贪心等算法来解决。

##### 5. 对数阶$O(logn)$

与指数阶相反，对数阶反映了“每轮缩减到一半”的过程，时间复杂度为$O(log_2^n)$,简记为$O(logn)$

```
def logarithmic(n:float) ->int:
	"""对数阶（）"""
```



##### 6. 线性对数阶$O(nlogn)$

##### 7. 阶乘阶$O(n!)$



### 2.2 空间复杂度

> **空间复杂度（Space Complexity）**：在问题的输入规模为 $n$ 的条件下，算法所占用的空间大小，可以记作为$S(n)$。一般将 **算法的辅助空间** 作为衡量空间复杂度的标准。

除了执行时间的长短，算法所需储存空间的多少也是衡量性能的一个重要方面。而在「2. 时间复杂度」中提到的渐进符号，也同样适用于空间复杂度的度量。空间复杂度的函数可以表示为 $ S(n) = O(f(n))$，它表示的是随着问题规模 $n$ 的增大，算法所占空间的增长趋势跟 $f(n)$ 相同。

相比于算法的时间复杂度计算来说，算法的空间复杂度更容易计算，主要包括「局部变量（算法范围内定义的变量）所占用的存储空间」和「系统为实现递归（如果算法是递归的话）所使用的堆栈空间」两个部分。

### [2.3 算法复杂度小结](https://datawhalechina.github.io/leetcode-notes/#/ch01/01.01/01.01.02-Algorithm-Complexity?id=算法复杂度总结)

**「算法复杂度」** 包括 **「时间复杂度」** 和 **「空间复杂度」**，用来分析算法执行效率与输入问题规模 $n$ 的增长关系。通常采用 **「渐进符号」** 的形式来表示「算法复杂度」。

常见的时间复杂度有：$O(1)$、$O(\log n)$、$O(n)$、$O(n \times \log n)$、$O(n^2)$、$O(n^3)$、$O(2^n)$、$O(n!)$。

常见的空间复杂度有：$O(1)$、$O(\log n)$、$O(n)$、$O(n^2)$。

## 3、LeetCode练习题5道

#### 1. [2235. 两整数相加](https://leetcode.cn/problems/add-two-integers/)

```python
def twoSum(self, nums: List[int], target: int) -> List[int]:
    numDict = dict()
    for i in range(len(nums)):
        if target-nums[i] in numDict:
            return numDict[target-nums[i]], i
        numDict[nums[i]] = i
    return [0]
```



#### 2. [1929. 数组串联](https://leetcode.cn/problems/concatenation-of-array/)

```python
class Solution:
    def getConcatenation(self, nums: List[int]) -> List[int]:
        nums.extend(nums)
        return nums
```



#### 3.[0771. 宝石与石头](https://leetcode.cn/problems/jewels-and-stones/)

```python
class Solution(object):
    def numJewelsInStones(self, J, S):
        res = 0
        j_arr = []
        for c in J:
            j_arr.append(c)
        for c in S:
            if c in j_arr:
                res += 1
        return res

```



#### 4.[1480. 一维数组的动态和](https://leetcode.cn/problems/running-sum-of-1d-array/)

```python
class Solution(object):
    def runningSum(self, nums):
        """
        :type nums: List[int]
        :rtype: List[int]
        """
        lis = []
        sum = 0
        for i in nums:
            sum+=i
            lis.append(sum)    
        return lis
        

```



#### 5.[0709. 转换成小写字母](https://leetcode.cn/problems/to-lower-case/)

```python
class Solution:
    def toLowerCase(self, str: str) -> str:
        new_ch_list=[]
        for ch in str:
            if 65<=ord(ch)<97:
                new_ch_list.append(chr(ord(ch)+32))
            else:
                new_ch_list.append(ch)
        return "".join(new_ch_list)

```



#### 6.[1672. 最富有客户的资产总量](https://leetcode.cn/problems/richest-customer-wealth/)

```python
class Solution(object):
    def maximumWealth(self, accounts):
        """
        :type accounts: List[List[int]]
        :rtype: int
        """
        maxm = 0
        for i in accounts:
            if maxm < sum(i):
                maxm = sum(i)
        return maxm

```



## 4. 参考资料

1.[Hello-算法](https://www.hello-algo.com/chapter_computational_complexity/time_complexity/#234)

2.[datawhale-Leetcode算法笔记](https://datawhalechina.github.io/leetcode-notes/#/ch01/01.01/01.01.02-Algorithm-Complexity)

