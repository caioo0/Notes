# 机器学习思考题目——08降维

本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.降维的主要动机是什么？有哪些负面影响（drawbacks）？

【动机】
（a）提高后续的训练算法的速度（在某些情况下甚至可以去除噪音和冗余的特征，使得训练算法表现更好）；
（b）可视化数据并了解最重要的特征（特点）；
（c）节省空间（压缩）。
【负面影响】
（a）丢失了某些信息，可能会降低后续算法的表现；
（b）降维可能计算量会大；
（c）提高了机器学习pipeline复杂性；
（d）转化后的特征，常常难以解释。

## 2.什么是维度诅咒（curse of dimensionality）？

维度诅咒是指在低维空间中不存在的问题在高维空间中出现。在机器学习中，在高维空间中随机选取的向量是很稀疏的，这增加了过拟合的风险，如果没有大量的数据，很难发现数据的pattern。

## 3.数据集经过降维维度减少之后，有可能进行反向操作么（恢复原始数据）？如果可以，怎么做？如果不可以，为什么？

降维之后的数据，几乎不可能完美的复原数据，因为在降维的过程中数据丢失了。另外，有些算法（例如PCA）可以进行反向操作来重建数据集（相当接近原始数据），其他算法没有这种操作（例如T-SNE）。

## 4.PCA可以用来给一个高度非线性的数据集进行降维么？

PCA可以给多数数据集进行显著地降维，即使数据集是高度非线性的，因为它至少可以去除无用维度。
然而，如果没有无用的维度——例如，瑞士卷（Swiss roll数据）——用PCA降维会丢失很多信息。你会想展开瑞士卷，而不是压碎它。

## 5.在一个1000维的数据集上用PCA，把 explained variance ratio设置为95%。那么降维之后的数据集有多少维？

答案跟数据集有关。结果可能是1-1000中的任何一个数。画出 explained variance与维度之间的图形，可以大致了解数据集的内在维度。

## 6.在什么情况下应用：vanilla PCA（香草PCA？此处貌似应该理解为常规PCA）、增量PCA、随机PCA、核-PCA？

（a）常规PCA是默认选项，不过它需要数据集可以载入内存；
（b）增量PCA 可以处理大型数据集，不过它的速度比常规PCA要慢，因此如果数据集可以放入内存，常规PCA更合适。增量PCA也可以应用于在线任务（online tasks）：每当有一个新数据到达之后，快速的应用PCA。
（c）当你需要大大降低维度，同时数据集可以载入内存中的时候可以用随机PCA；它比常规PCA快得多。
（d）核-PCA对于非线性数据集是有用的。

## 7.怎样评价降维算法的表现（降维效果）？

降维算法表现好是指丢弃了很多维度，但是并没有丢失很多信息。
（方法一）对结果数据集进行反向操作，衡量reconstruction error，不过并非所有的降维算法都有反向操作；
（方法二）如果降维是另一个机器学习算法（例如随机森林）的预处理步骤，可以评价第二个算法（后续算法）的表现，如果降维没有损失太多信息，那么这个算法应该表现地和用原始数据一样好。

## 8.把两个不同的降维算法连接（chain）到一起使用，有意义么？

有意义。一个常见的例子是用PCA快速去除大量无用维度，然后用另一个速度慢的降维算法，例如LLE。这种两个步骤的方法效果和只用LLE差不多，但是时间上少得多。