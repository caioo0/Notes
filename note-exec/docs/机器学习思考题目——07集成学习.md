# 机器学习思考题目——07集成学习



本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.在同一个训练集上训练五个不同的模型，它们的准确率（precision）都为95%，是否有可能把它们组合起来获得更好的结果？如果可以，该怎么做？如果不可以，为什么？

可以尝试把这五个模型组合成 voting ensemble，一般会有更好的结果。当这五个模型越不同的时候（例如分别为SVM、CART、LR等）效果越好。当训练集不同的时候，效果会更好。

## 2.硬投票（hard voting）分类器和软投票（soft voting）分类器的区别是什么？

硬投票分类器统计所有分类器的投票数目，选取投票最多的类。
软投票分类器计算每个类的平均概率得分，选取概率最高的那个。这给那些高可信度（high-confidence）的投票更多权重，一般表现更好。前提是需要每个分类器可以预测类的概率（sklearn中的SVM需要设置probability=True）。

## 3.是否有可能通过分布式计算来加速bagging ensemble的训练速度？pasting ensemble、boosting ensemble、random ensemble、随机森林、stacking ensemble呢？

Bagging ensemble可以通过分布式计算来加速，因为ensemble中的每个预测器是独立的。同样的原因，对于pasting ensemble和随机森林也是可以的。boosting ensemble中的每个预测器是建立在前面预测器的基础之上的，因此训练是顺序的，不能通过分布式计算加速。对于stacking ensemble，同一层的预测器是独立的，可以并行计算。但是只有某一层的预测次都训练完才能（分布式）训练下一层的预测器。

## 4.out-of-bag evaluation的优势是什么？

用out-of-bag evaluation，bagging ensemble中的每个预测器用训练集之外的数据来评价。这使得不需要额外的validation set来对集成做一个比较公正的评价。这样可以有更多的样本来做训练，模型会表现地稍微更好些 。

## 5.Extra-Trees为什么比常规的随机森林随机性更强？这种随机性有什么帮助？Extra-Trees是比常规随机森林更快还是更慢？

当用随机森林算法生长一棵树的时候，每个节点分裂的时候考虑的是特征的一个随机子集。这对于Extra-Trees同样如此，不过它更进了一步：不是像决策树那样寻找最优的分界点（threshold），而是对每个特征用随机的临界点。这种额外的随机性的作用（某种程度上）像一种正则化：如果一个随机森林过拟合，Extra-Tree可能表现的更好。另外，由于Extra-Tree 不寻找最优的临界点，它比随机森林更快。但是在做预测的时候，它的速度和随机森林一样。

## 6.当AdaBoost集成欠拟合的时候，该怎么调节超参数？

应该增加estimator的数目或者减少正则化超参数。也可以尝试略微增加学习率。

## 7.如果Gradient Boosting过拟合了，应该增加还是减小学习率？

应该减小学习率。也可以尝试early stopping来寻找合适的predictor的数目（目前很可能太多了）。