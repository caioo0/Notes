本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.如果有一个百万量级特征的训练集，应该用哪种线性回归训练算法？

可以用随机梯度下降法（SGD）或者小批量梯度下降（Mini-batch Gradient Descent）。如果训练集能否放入内存的话也可以用批量梯度下降算法（Batch Gradient Descent）。但是不宜用标准方程（Normal Equation）的解法，因为计算复杂度随着特征数增长极快。

## 2.假如训练集中不同特征的数据级差异很大，这种情况下什么算法会受影响？该怎么处理这种情况？

这种情况下，损失函数的形状为一个细长的碗（an elongated bowl），因此梯度下降算法会花费很长时间来收敛。
为了解决这个问题，需要在训练之前对数据进行缩放（scale the data）。注意：标准方程解法在这种情况下，不需要缩放也可以工作的很好。
【SVM也需要训练之前标准化】

## 3.梯度下降法在训练逻辑回归模型的时候会卡在一个局部最小点么？

不会，因为逻辑回归的损失函数是凸的。

## 4. 如果运行足够长的时间，所有的梯度下降算法都会得到一个相同的模型么？

（a）如果优化问题是凸的（例如线性回归或逻辑回归），假如学习率（learning rate）不很大，那么所有的梯度下降算法都会接近最优解，最后得到一个非常相近（fairly similar）的模型。
（b）除非逐渐减少学习率，SGD和小批量GD（Mini-batch GD）永远不会真正收敛；它们会在最优点附近来回跳跃。这意味着让它们运行很长时间，这些梯度下降算法会产生不同（slightly different）的模型。

## 5.假设用批量梯度下降法训练，每个epoch画出验证误差（validation error）。假如发现验证误差 consistently goes up，很可能发生了什么？该怎么解决这个问题？

如果验证误差在每个epoch结束之后上升，一个可能是学习率太大了，算法发散（diverging）：
（a）如果训练误差同时也上升了，这就很明确，需要减小学习率
（b）如果训练误差没有变大，说明模型过拟合了，需要停止训练。

## 6.当验证误差变大的时候就立刻停止小批量梯度下降（Mini-batch GD）是好主意么？

由于包含随机性，不论SGD还是Mini-batch GD，都不能保证误差每一步都减小。如果发现验证误差增大，立刻停止训练，可能停止地过早了，算法还没有找到最优解。
一个更好的选择是每隔一定步数保存模型，当有一段时间模型效果没有提高的时候，可以回去找到保存的最好的模型。

## 7.(在SGD，Mini-batch GD,Batch GD中)哪种梯度下降算法会最快到达最优解的附近？ 哪种会实际收敛？对于其他的几个，怎样使它们收敛？

SGD（或Mini-batch）会最快地到达最优解附近，因为它每次只考虑一个（或若干个）训练样本，因此训练时迭代最快。
当给予足够的时间，只有Batch GD会实际上收敛。除非逐渐减少学习率，SGD和Mini-batch GD 会在最优解附近来回跳跃。

## 8.当用多项式回归的时候，如果学习曲线中训练误差和验证误差有一个大的的间隔，原因是什么？有哪些方法可以解决这个问题？

如果验证误差比训练误差大得多，可能是因为模型过拟合了。
解决办法：
（a）减少多项式的阶数：低阶多项式不容易过拟合。
（b）可以对模型进行正则化：在损失函数中添加L2惩罚（Ridge）或L1惩罚（Lasso）。这也会减少模型的自由度的阶数。
（c）可以尝试增加数据集的数量。

## 9.当用岭回归（Ridge Regression）时，训练误差和验证误差几乎相等且都相当高。这个模型是偏差大还是方差大？是应该增大正则化超参数α还是减小它？

此模型可能是欠拟合，这意味着模型偏差大；应该减小α。

## 10.什么情况下：（a）岭回归比线性回归效果好？ （b）Lasso比岭回归好？ （c）Elastic Net比Lasso好？

（a）有正则化的模型一般都会比没有正则化的模型表现好，因此岭回归一般都比纯线性回归效果好；
（b）Lasso回归用了L1惩罚项，这会倾向于把权重变为0。这会生成一个稀疏模型——除了一些最重要的权重，其他的都为0。因此当只有部分特征重要的时候，这是一种自动的选择特征的方法。当你不确定（只有部分特征重要的）时候，用岭回归。
（c）Elastic Net一般都会比Lasso从表现好，因为Lasso在某些问题中会表现异常（例如有若干特征之间相关性很强或者特征数目多于样本数目）。而且Lasso引入了一个超参数。

## 11.要把图片分为 outdoor/indoor和 daytime/nighttime，应该训练两个逻辑回归分类器还是一个Softmax分类器？

由于类之间可能有交叉（not exlusive）,因此训练两个逻辑回归分类器即可。