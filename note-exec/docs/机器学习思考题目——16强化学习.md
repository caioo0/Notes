# 机器学习思考题目——16强化学习

本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.什么 是强化学习（Reinforcement Learning）？它和常规的监督学习、无监督学习的区别是什么？

（1）强化学习是机器学习中的一个领域，它的目的是建立agent，agent能在环境（environment）中采取动作（action）使得一段时间内的回报（reward）最大化。
（2）RL和常规的监督学习、无监督学习的差别很多：
　　（a）在监督和无监督学习中，目标一般是从数据中发现pattern（模式）。强化学习中目标是找到好的policy（策略）。
　　（b）与监督学习不同的是，agent没有被明确告诉什么是正确的答案（right answer），它必须自己通过尝试和错误（trial and error）来学习。
　　（c）与无监督学习不同的是，强化学习通过得到reward获得某种形式的‘监督’。
不告诉agent怎么执行任务，但是当它进步或者失败的时候告诉它。
　　（d）强化学习agent需要在探索（explore）环境（寻找获得reward的新的方式）和利用(exploit)已知方法(获得reward的方法)之间寻找平衡。而监督学习和无监督学习系统一般不需要考虑探索(exploration）,它们依赖于训练数据。
　　（e）在监督学习和无监督学习中，训练样本一般是独立的。在RL中，连续的观测值（consecutive observations）往往是不是独立的。一个agent在继续前进之前可能在一个区域停留一段时间，因此连续的观察值相关性可能会很强。在某些例子中，重播记忆（replay memory）可以用来确保训练算法获得相对独立的观察值。

## 2.能否列举出三个强化学习的应用例子？对于这些例子，环境（environment）是什么，agent是什么？

可能的应用场景如下：
（a）音乐个性化（Music personalization）
environment是用户个性化的网络电台。agent是决定给用户推送下一首歌的软件，它的可能的action是播放列表中的任一首歌（它需要选择用户喜欢的歌）或者播放一段广告（它需要选择用户感兴趣的广告）。用户听了一首歌，agent获得一个小的reward；用户听了一段广告，agent获得一个大的reward；用户跳过了一首歌或者一段广告，agent获得一个负的reward；用户如果离开了，agent获得一个很大的负reward。
（b）营销活动（Marketing）
environment是公司的营销部门。agent是决定需要给哪些客户发送邮件（已经给定客户的资料和购买记录，对于每个客户有两个action：发送或不发送）。发邮件会得到一个负的reward，发邮件产生的预计收入（【客户购买产品产生】）作为一个正的reward。
（c）产品送货（Product delivery）
agent控制一队送货卡车，决定它们在仓库装上什么，送到哪里去等等。如果它们准时送到，会获得正的reward，如果送货有延迟，则获得负的reward。

## 3.什么是discount rate（折扣率）？当修改discount rate的时候最优策略会有变化么？

（1）当估计action的值（reward）的时候，强化学习算法一般把这个action导致的所有reward进行求和，求和的时候给immediate rewards（【即时奖励，时间上较近的奖励】）更大的权重， later rewards（【时间上较远的奖励】）更小的权重（认为一个动作对未来一小段时间的影响大于未来一大段时间的影响）。为了对这种情况进行建模，每个时间步应用一个discount rate。例如，discount rate设为0.9，a reward of 100 that is received two time steps later is counted as only 100×0.9×0.9=0.81。可以把discount rate想成用来度量未来相对于现在的价值：如果它接近1，未来被认为和现在有差不多的价值；如果它接近0，只有 immediate rewards（【当前或者最邻近的未来】）是重要的。
（2）修改discount rate对最优策略影响很大：如果你觉得未来重要，为了最后获得reward，你可能会近期承受很多pain；然而如果你觉得未来不重要，你可能会更看重 immediate rewards（【眼前利益】），而不是未来的投资。

## 4.怎样评价强化学习agent的表现？

可以简单的把agent获得的所有reward求和。在一个模拟环境中，可以运行很多episode（【次】）来观察agent获得的平均的总收益（也可以观察min、max、标准差等参数）。

## 5.什么是 credit assignment problem（信用分配问题）？它什么时候会出现？怎样缓解它？

（1）信用分配问题是指：当强化学习agent获得一个reward，它不知道是它过去的哪个action导致了这个reward。这个问题一般当action和得到的reward有较大的延迟的时候会出现（例如，在Atari的乒乓球游戏中，在agent击球和agent得分之间可能隔着几十个时间步）。
（2）一种缓解的方法是如果可能的话，给agent提供短期reward。这一般需要这个任务的先验知识。例如，如果我们设计一个agent来学下棋（chess），并不是只有它取胜之后才给它一个reward，而是它吃掉对方的棋子之后就给它一个reward。

## 6.使用replay memory（重播记忆）有什么意义？

一个agent常常会在环境的同一个区域内停留一段时间，因此它的经验在这段时间内会非常相似。这会给学习算法引入一些偏差（bias）。它会为了环境中的这个区域调整策略，但是当agent不在这个区域之后效果就不好了。为了解决这个问题，可以用replay memory；不是用最近的经验来学习，而是用过去的经验（最近的但不是那么近的，recent and not so recent）（这或许就是为什么我们晚上做梦：重播白天的记忆，更好的从中学习）。

## 7.什么是 off-policy强化学习算法？

off-policy算法学习最优策略的得分（i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally【打折之后的】），与agent实际的动作无关。Q-Learning是这种算法很好的例子。
on-policy算法学习agent实际执行的得分，同时包括exploration和exploitation。