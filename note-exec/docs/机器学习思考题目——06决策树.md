# 机器学习思考题目——06决策树

本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.用一百万个样本的训练集训练出的决策树（没有限制）大约有多深？

一棵匀称（well-balanced）的二叉树，如果共有m个叶子，则它的深度为log2(m)（取整）。一棵二元决策树（binary Decision Tree）训练结束时，或多或少平衡，每个叶子代表一个样本（训练时没有限制的情况下）。因此如果包含一百万个样本，那么输的深度会是log2(1000000)≈20（实际中会略多，因为树一般不会如此平衡）。

## 2.（决策树的）一个节点的Gini不纯度一般会比它的父节点的小还是大？一般而言，还是总是这样？

一个节点的Gini不纯度一般比它的父节点的小。这是由CART训练算法的损失函数来保证的——在进行分裂的时候，最小化它的子节点的不纯度的和。
然而，它仍然可能比它的父节点有更高的Gini不纯度 。例如，假设一个节点包含四个A类样本，1个B类样本。它的Gin不纯度为1-（1/5）2-（4/5）2=0.32。现在假设数据集是一维数据集，样本按以下顺序排列：A，B，A，A，A。可以验证，算法在第二个样本之后进行分裂，产生包含A、B的子节点和包含A、A、A的子节点。第一个子节点的Gini不纯度为1-（1/2）2-（1/2）2=0.5，比它的父节点大。因为另一个节点是纯节点，所以（这个增长）被补偿了，总的Gini不纯度为 （2/5）*0.32+（3/5）*0=0.2，这比它的父节点的Gini不纯度小。

## 3.如果决策树过拟合，减小深度是否会解决这个问题？

会，因为这会限制模型，regularizing it。

## 4.如果决策树欠拟合，scale 输入特征是否有用？

决策树不关系训练数据是否 scaled 或 centered；这是决策树的一个优点。因此此时scale输入是浪费时间。

## 5. 一百万样本的训练集，训练一个决策树模型需要一个小时，那么在一千万样本的训练集上大概需要多少时间？【此处应该假设样本特征数目相同】

训练决策树的计算复杂度为O(n×mlog(m))，其中m为样本数，n为特征数。训练集的数据是原来的十倍，训练时间应该乘以K = (n × 10m × log(10m)) / (n × m × log(m)) = 10 × log(10m) / log(m)。当m=1000000时，代入得 K ≈ 11.7，因此训练时间大概为11.7小时。

## 6. 如果训练集有1000,000个样本，设置 presort=True（sklearn中）是否会加速训练？

只有当训练集不超过几千的时候（smaller than a few thousand）预排序才会加速训练。如果有100,000个样本，设置presort=True会considerably减缓训练。