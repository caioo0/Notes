{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1385819B03174E38AD530BFCFAF646D3","trusted":true,"mdEditEnable":false},"source":"## NLP操练101 (使用造好的轮子，少许修改~)\n原作【nlp-exercises 101 】是Shrivarsheni 写的，发布在 https://www.machinelearningplus.com/nlp/nlp-exercises/  。由于其中的代码绝大部分是针对英文来操作的，不太适合中文，笔者对其中的内容进行了选取，并针对中文做了适配，改动幅度大于90%。\n \n自然语言处理是人工智能理解人类语言的技术。NLP任务如文本分类、总结、情感分析、翻译等被广泛使用。本帖旨在为基础和高级NLP任务提供些许参考。篇幅有限，很多原理没有来得及铺陈开来，直接上的代码。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"33AACB46DA614BCE86742115F993515B","trusted":true},"source":"### 1. 如何在nltk中下载 \"stopwords（停用词）\" and \"punkt（标点符号）\" 这两个工具包？\n难度等级 : L1\n\nQ. 载入nltk及其必要的工具包"},{"cell_type":"code","execution_count":1,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"64EE1C6B6DA749CBA3D781C3E485FC1B","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n","[nltk_data] Error loading stop: Package 'stop' not found in index\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"# Downloading packages and importing\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stop')\nnltk.download('stopwords')\n\n#> [nltk_data] Downloading package punkt to /root/nltk_data...\n#> [nltk_data]   Unzipping tokenizers/punkt.zip.\n#> [nltk_data] Error loading stop: Package 'stop' not found in index\n#> [nltk_data] Downloading package stopwords to /root/nltk_data...\n#> [nltk_data]   Unzipping corpora/stopwords.zip.\n#> True"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0702B81F26EA4D4E86959A10FC370159","trusted":true},"source":"### 2. 如何在spacy导入语言模型（language model） ？  \n难度等级 : L1\n\nQ. 导入spacy库并加载'en_core_web_sm'模型以支持英语语言模型。加载'xx_ent_wiki_sm'以支持多语言。"},{"cell_type":"code","execution_count":2,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FD9F77FCC71D496AB7E38BAE335893AA","trusted":true},"outputs":[{"data":{"text/plain":["<spacy.lang.en.English at 0x24e7a5d4e10>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"# Import and load model\n\nimport spacy\nnlp=spacy.load(\"en_core_web_sm\")\nnlp\n# More models here: https://spacy.io/models"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D12D33FC5A3A4D628E945BB5C64DCBA7","trusted":true},"source":"\n### 3. 如何对给定的文本进行词条化（tokenization）?\n难度等级 : L1\n\nQ. 对给定文本词条化后的词条（token）进行打印。"},{"cell_type":"code","execution_count":9,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"60278D7765DB4A69863C4ECE9B1194F0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['通过', '对', '全网', '主流', '媒体', '及', '社交', '媒体', '平台', '进行', '实时', '数据', '抓取', '和', '深度', '处理', '，', '可以', '帮助', '政府', '/', '企业', '及时', '、', '全面', '、', '精准', '地', '从', '海量', '的', '数据', '中', '了解', '公众', '态度', '、', '掌控', '舆论', '动向', '、', '聆听', '用户', '声音', '、', '洞察', '行业', '变化', '。']\n"]}],"source":"import jieba\n\ntext ='''通过对全网主流媒体及社交媒体平台进行实时数据抓取和深度处理，可以帮助政府/企业及时、全面、精准地从海量的数据中了解公众态度、掌控舆论动向、聆听用户声音、洞察行业变化。'''\n\ntext_segment = jieba.lcut(text)\n\nprint(text_segment )"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5B92EE629D6143DAB1CACA032273771B","trusted":true},"source":"\n### 4. 如何对给定的文档进行语句划分?   \n\n难度等级 : L1\n\nQ. 打印经过语句分割后的文档。"},{"cell_type":"code","execution_count":15,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"299AD79662BB4E9BB83DD2DE97BA9391","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["社会化聆听可以7*24小时全天侯对全网进行实时监测，采用文本挖掘技术对民众意见反馈、领导发言、政策文件进行监测分析。\n","通过这些先进的nlp技术的处理，可以帮助政府及时的了解社会各阶层民众对社会现状和发展所持有的情绪、态度、看法、意见和行为倾向 。\n","最终实现积极主动的响应处理措施和方案，对于互联网上一些错误的、失实的舆论做出正确的引导作用，控制舆论发展方向。\n"]}],"source":"from pyltp import SentenceSplitter\n\ndocs = '''社会化聆听可以7*24小时全天侯对全网进行实时监测，采用文本挖掘技术对民众意见反馈、领导发言、政策文件进行监测分析。通过这些先进的nlp技术的处理，可以帮助政府及时的了解社会各阶层民众对社会现状和发展所持有的情绪、态度、看法、意见和行为倾向 。最终实现积极主动的响应处理措施和方案，对于互联网上一些错误的、失实的舆论做出正确的引导作用，控制舆论发展方向。'''\n\nsentences = SentenceSplitter.split(docs)\n\nprint('\\n'.join(list(sentences)))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4AB2791CDAA148D5891C475888E26B4F","trusted":true},"source":"### 5. 如何利用 `transformers` 对文本进行词条化 ?   \n难度等级 : L1\n\nQ. 使用Huggingface的 transformers库直接对文本进行词条化处理，不需要分词。"},{"cell_type":"code","execution_count":18,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"28882754D6D449BF8CF1D99B2A33D9BC","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[101, 2190, 1062, 1385, 1501, 4277, 6822, 6121, 6566, 7481, 5644, 2658, 2141, 3198, 4664, 3844, 8024, 752, 816, 4028, 1359, 6633, 1232, 7564, 3844, 8024, 7564, 6356, 2141, 3198, 6239, 6809, 8024, 2376, 1221, 1062, 1385, 2356, 1767, 1350, 1501, 4277, 6956, 7305, 5018, 671, 3198, 7313, 1355, 4385, 6566, 7481, 5644, 2658, 8024, 1350, 3198, 2418, 2190, 1314, 3322, 1062, 1068, 8024, 2971, 1169, 5644, 6389, 6624, 1403, 8024, 7344, 3632, 1501, 4277, 1358, 2938, 119, 102]\n","[CLS] 对 公 司 品 牌 进 行 负 面 舆 情 实 时 监 测 ， 事 件 演 变 趋 势 预 测 ， 预 警 实 时 触 达 ， 帮 助 公 司 市 场 及 品 牌 部 门 第 一 时 间 发 现 负 面 舆 情 ， 及 时 应 对 危 机 公 关 ， 控 制 舆 论 走 向 ， 防 止 品 牌 受 损. [SEP]\n"]}],"source":"# 下载和导入transfromers\n!pip install transformers\nfrom transformers import AutoTokenizer\n\ntext = '''对公司品牌进行负面舆情实时监测，事件演变趋势预测，预警实时触达，帮助公司市场及品牌部门第一时间发现负面舆情，及时应对危机公关，控制舆论走向，防止品牌受损。'''\n\n# 初始化tokenizer\ntokenizer=AutoTokenizer.from_pretrained('bert-base-chinese')\n\n# 使用tokenizer对文本进行编码\ninputs=tokenizer.encode(text)\nprint(inputs)\n# 使用tokenizer对文本编码进行解码\noutputs = tokenizer.decode(inputs)\nprint(outputs)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F5A00FD4976842EA8E75D1FDD6E77A2D","trusted":true},"source":"\n### 6. 如何在对文本词条化时将停用词（stopwords）用作短语区隔符号 ?   \n难度等级 : L2\n\nQ. 有时把停用词用作区隔符号，将会保留有意义的短语（ meaningful phrases），这对于很多场景，如文本分类、主题建模和文本聚类都很有用。"},{"cell_type":"code","execution_count":182,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4B8D55D8B1A547C3A424E9F99F612A9C","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['文本挖掘', '功能', '达观数据', '多年', '自然语言处理技术经验', '掌握从词语短串到篇章分析', '层面', '分析技术', '基础', '提供', '文本挖掘功能', '涉黄涉政检测', '文本内容', '涉黄涉政检测', '满足相应政策要求', '垃圾评论过滤', '在论坛发言', '用户评论中', '过滤文本', '垃圾广告', '提升文本总体质量', '情感分析', '用户评论', '文本内容', '情感分析', '指导决策与运营', '自动标签提取', '自动提取文本重要内容生成关键性标签', '基础', '拓展', '功能形式', '文本自动分类', '文本内容', '分析', '给出文本所属', '类别和置信度', '支持二级分类', '正常政治言论', '被过滤掉', '达观', '涉政内容', '返回', '“反动”权值', '取值范围0到1', '涉政内容', '反动权值接近“1”时', '文本', '反动倾向', '根据客户要求', '直接过滤掉', '反动权值接近“0”时', '文本为正常政治言论', '几率', '非常高', '客户', '反动权值控制审核松紧程度', '黄反内容', '垃圾广告形式多样', '处理', '传统', '方法', '配词典', '方式来解决', '方法', '变形文本时命中率', '造成严重', '漏盘', '而且需要人工', '断更新词典', '效率', '达观数据', '机器学习', '方法智能识别', '种变形变换', '内容', '同时根据最新', '样本数据实时更新运算模型', '自动学习更新', '保证检测', '效果', '实时', '弹幕能够', '处理', '达观数据文本挖掘系统支持高并发大数据量实时处理', '完全', '支持实时弹幕', '处理', '实现', '弹幕文本', '筛除涉黄', '涉政', '垃圾评论', '广告内容', '检测', '标签自动提取', '非热门行业适用', '达观标签自动提取功能', '利用行业数据', '模型训练和调整', '在接入', '非热门行业服务', '行业', '规范文本', '训练样本', '模型训练', '模型更新', '适应此行业', '个性化需求', '而且在后期应用', '过程模型', '更新迭代保证提取', '结果与行业', '发展保持同步']\n"]}],"source":"text = '''\n文本挖掘主要有哪些功能\n达观数据拥有多年的自然语言处理技术经验，掌握从词语短串到篇章分析各层面的分析技术，在此基础之上提供以下文本挖掘功能：\n* 涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求；\n* 垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量；\n* 情感分析：对用户评论等文本内容做情感分析，指导决策与运营；\n* 自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式；\n* 文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。\n正常政治言论也会被过滤掉吗？\n不会，达观对涉政内容会返回一个“反动”权值，取值范围0到1。当涉政内容的反动权值接近“1”时，文本的反动倾向很高，根据客户要求可以直接过滤掉，当反动权值接近“0”时，则文本为正常政治言论的几率就非常高，客户可通过反动权值控制审核松紧程度。\n黄反内容、垃圾广告形式多样怎么处理？\n传统的方法更多的是通过配词典的方式来解决。但是这种方法遇到变形文本时命中率很低，造成严重的漏盘，而且需要人工不断更新词典，效率很低。\n达观数据通过机器学习的方法智能识别各种变形变换的内容，同时根据最新的样本数据实时更新运算模型，自动学习更新，保证检测的效果。\n实时的弹幕能够做处理吗？\n可以，达观数据文本挖掘系统支持高并发大数据量实时处理，完全可以支持实时弹幕的处理，实现对弹幕文本做筛除涉黄、涉政、垃圾评论、广告内容等的检测。\n标签自动提取对于非热门行业适用吗？\n达观标签自动提取功能可以利用行业数据进行模型训练和调整，在接入一个非热门行业服务之前，我们会以此行业的规范文本作为训练样本做模型训练，新的模型更新之后会适应此行业的个性化需求，而且在后期应用的过程模型会不断的更新迭代保证提取的结果与行业的发展保持同步。'''\n\nfor r in ['主要有','那些','哪些','\\n','或','拥有','。','，','之前','以下','对于','；','：','、','会','我们','在此','之上',\n          '*','各','从''而且','一个','以此','作为','之后','当','进行','？','怎么','更多','可以',\n          '不','通过','吗', '也','可','但是','这种','遇到','则','就','对','等','很','做','中的','的'\n         ]:\n    \n    text = text.replace(r, 'DELIM')\n\nwords = [t.strip() for t in text.split('DELIM')]\nwords_filtered = list(filter(lambda a: a not in [''] and len(a) >1, words))\n\nprint(words_filtered)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F743F1686B3D45E4B9EB867BA1A15D15","trusted":true},"source":"### 7. 如何移除文本中的停用词 ?    \n难度等级 : L1\n\nQ. 从文本中移除类似“我们”、“，”、“想要”、“一个”这样的无意义词汇和标点。"},{"cell_type":"code","execution_count":30,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"048AEE13E91B4D4996B859FF34143F7B","trusted":true},"outputs":[{"data":{"text/plain":["'达观 数据 客户 意见 洞察 平台 品牌 负面 舆情 实时 监测 演变 趋势 预测 预警 实时 触达 品牌 部门 时间 负面 舆情 应对 危机 公关 控制 舆论 走向 品牌 受损'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":"import jieba\n\ntext = \"达观数据客户意见洞察平台对公司品牌进行负面舆情实时监测，事件演变趋势预测，预警实时触达，帮助公司市场及品牌部门第一时间发现负面舆情，及时应对危机公关，控制舆论走向，防止品牌受损。\"\n\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\nnew_tokens=[]\n\n# Tokenization using word_tokenize()\nall_tokens=jieba.lcut(text)\n\nfor token in all_tokens:\n  if token not in my_stopwords:\n    new_tokens.append(token)\n\n\n\" \".join(new_tokens)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1CA72CC773384F638465783B79F812BF","trusted":true},"source":"\n### 8. 如何进行词干化（stemming）？  \n难度等级 : L2\n\nQ. 在给定的文本中，将每个词条（token）转换为它的词根形式（root form）。"},{"cell_type":"code","execution_count":35,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3FFDE05730B14171937CFE67427EB0C9","trusted":true},"outputs":[{"data":{"text/plain":["'“ 哇，这个新来的boy真的sup handsom ! ” , “ 这个timetable做的not veri good ” , “ coffe 我们需要meet一下然后看看tomorrow怎么安排 ” ,'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":"# 使用nltk自带的PorterStemmer进行Stemming \n\nfrom nltk.stem import PorterStemmer \nstemmer=PorterStemmer()\nstemmed_tokens=[]\n\ntext= ''' \n “哇，这个新来的boy真的super handsome!”,\n “这个timetable做的not very good”,\n “coffee 我们需要meet一下然后看看tomorrow怎么安排”,'''\n\n\n\nfor token in nltk.word_tokenize(text):\n  stemmed_tokens.append(stemmer.stem(token))\n\n\" \".join(stemmed_tokens)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FB3E887ABDA4F47833884FBAAED06D3","trusted":true},"source":"\n### 9. 如何从邮箱号中抽取出姓名 ?   \n难度等级 : L2\n\nQ. 利用正则从邮箱地址中抽取出姓名。"},{"cell_type":"code","execution_count":101,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CA0854FDE9D246CFB963765CFAA75800","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Scottish_folds_meow']\n"]}],"source":"# 使用正则表达式从邮箱号里抽取姓名 \nimport re  \n\ntext = '我的邮箱号是Scottish_folds_meow@gmail.com。'\n     \n    \nusernames= re.findall('([^\\u4E00-\\u9FA5]\\w.*)@', text,re.M|re.I)  #[^表示除...以外\n\nprint(usernames) "},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"67B54C93BDC34F0D87352788BAB8798C","trusted":true},"source":"### 10. 如何在排除停用词的情况下找出文中最常见的词汇？\n难度等级 : L2\n\nQ. 在启用停用词的情况下，抽取给定文本段落中的TOP10高频词汇。"},{"cell_type":"code","execution_count":113,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0FDE430B6CBB4E3EA676DBEE29BD6562","trusted":true},"outputs":[{"data":{"text/plain":["[('文本', 16),\n"," ('内容', 9),\n"," ('自动', 6),\n"," ('行业', 6),\n"," ('达观', 5),\n"," ('数据', 5),\n"," ('分析', 5),\n"," ('提取', 5),\n"," ('反动', 5),\n"," ('模型', 5),\n"," ('检测', 4),\n"," ('垃圾', 4),\n"," ('评论', 4),\n"," ('过滤', 4),\n"," ('标签', 4),\n"," ('权值', 4),\n"," ('更新', 4),\n"," ('挖掘', 3),\n"," ('功能', 3),\n"," ('涉政', 3)]"]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":"import jieba\n\ntext = '''\n文本挖掘主要有哪些功能\n达观数据拥有多年的自然语言处理技术经验，掌握从词语短串到篇章分析个层面的分析技术，在此基础之上提供以下文本挖掘功能：\n* 涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求；\n* 垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量；\n* 情感分析：对用户评论等文本内容做情感分析，指导决策与运营；\n* 自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式；\n* 文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。\n正常政治言论也会被过滤掉吗？\n不会，达观对涉政内容会返回一个“反动”权值，取值范围0到1。当涉政内容的反动权值接近“1”时，文本的反动倾向很高，根据客户要求可以直接过滤掉，当反动权值接近“0”时，则文本为正常政治言论的几率就非常高，客户可通过反动权值控制审核松紧程度。\n黄反内容、垃圾广告形式多样怎么处理？\n传统的方法更多的是通过配词典的方式来解决。但是这种方法遇到变形文本时命中率很低，造成严重的漏盘，而且需要人工不断更新词典，效率很低。\n达观数据通过机器学习的方法智能识别各种变形变换的内容，同时根据最新的样本数据实时更新运算模型，自动学习更新，保证检测的效果。\n实时的弹幕能够做处理吗？\n可以，达观数据文本挖掘系统支持高并发大数据量实时处理，完全可以支持实时弹幕的处理，实现对弹幕文本做筛除涉黄、涉政、垃圾评论、广告内容等的检测。\n标签自动提取对于非热门行业适用吗？\n达观标签自动提取功能可以利用行业数据进行模型训练和调整，在接入一个非热门行业服务之前，我们会以此行业的规范文本作为训练样本做模型训练，新的模型更新之后会适应此行业的个性化需求，而且在后期应用的过程模型会不断的更新迭代保证提取的结果与行业的发展保持同步。'''\n\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\nnew_tokens=[]\n\n# 使用jieba分词对语句进行词条化（Tokenization ） \nall_tokens=jieba.lcut(text.strip(' ').replace('\\n',''))\n\nfor token in all_tokens:\n    if len(token) >1: #仅关注词长大于1的词汇\n        if token not in my_stopwords:\n         new_tokens.append(token)\n\nfreq_dict={}\n\n# 计算词频（word frequency count）\nfor word in new_tokens:\n  if word not in freq_dict:\n    freq_dict[word]=1\n  else:\n    freq_dict[word]+=1\n\nsorted(freq_dict.items(),key = lambda x:x[1],reverse=True)[:20]  #按照键进行降序排列"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EFC597023A124E6A8EF315609AE45C0A","trusted":true},"source":"### 11. 如何对给定文本进行拼写纠错 ?   \n难度等级 : L2\n\nQ. 通过训练具体领域的语言模型来纠正该领域文本中的拼写错误。"},{"cell_type":"code","execution_count":186,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7B03BB2BEBB94D1DA43D27E550B87602","trusted":true},"outputs":[],"source":"import re, collections\n\ndef words(text): \n    return re.findall('[\\u4e00-\\u9fa5_a-zA-Z]+', text.lower())  #仅抽取文本中的中英文字符\n\ndef train(features):\n    model = collections.defaultdict(lambda: 1)\n    for f in features:\n        model[f] += 1\n    return model\n\nNWORDS = train(words(open('car_reviews.txt',encoding='utf-8').read()))  # 载入汽车领域的评论语料，尽量大，可更换为其他领域的文本\n\nhanzi = open('hanzi.txt',encoding='utf-8').read()  #汉字字符全集（不含标点），可在网上搜\n\ndef edits1(word):   \n   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n   deletes    = [a + b[1:] for a, b in splits if b]\n   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n   replaces   = [a + c + b[1:] for a, b in splits for c in hanzi if b]\n   inserts    = [a + c + b   for a, b in splits for c in hanzi]\n   return set(deletes + transposes + replaces + inserts)\n\ndef known_edits2(word):\n    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n\ndef known(words): \n    return set(w for w in words if w in NWORDS)\n\ndef correct(word):  #主函数\n    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n    return max(candidates, key=NWORDS.get)\n "},{"cell_type":"code","execution_count":187,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9284C10296E04EE796D44740D9CFAA68","trusted":true},"outputs":[{"data":{"text/plain":["'后备箱'"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":"correct(\"后箱备\")"},{"cell_type":"code","execution_count":188,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1EE4A1F2CEB940219EA23B6E5DD21B3D","trusted":true},"outputs":[{"data":{"text/plain":["'油量'"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":"correct(\"油蚝量\")"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1E87C2578695474488F5B11E5725FC5E","trusted":true},"source":"###  12. 如何度量若干文本之间的余弦相似度（cosine similarity）?   \n难度等级 : L3\n\nQ. 用sklearn中的文本特征抽取器CountVectorizer和TfidfVectorizer来实现文本相似度计算。"},{"cell_type":"code","execution_count":202,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D22003ECA0CF4B8F859737E3D74143F7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.85602883 0.86584201 0.85405964 0.86644858]\n"," [0.85602883 1.         0.91662846 0.90239949 0.91971415]\n"," [0.86584201 0.91662846 1.         0.89376154 0.92826114]\n"," [0.85405964 0.90239949 0.89376154 1.         0.90961569]\n"," [0.86644858 0.91971415 0.92826114 0.90961569 1.        ]]\n"]}],"source":"import jieba\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n\ntext1 = '涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求'\ntext2 = '垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量'\ntext3 = '情感分析：对用户评论等文本内容做情感分析，指导决策与运营'\ntext4 = '自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式'\ntext5 = '文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。'\n\ndocuments=[text1,text2,text3,text4,text5]\n\n\nvectorizer = CountVectorizer(\n    stop_words= my_stopwords,\n    tokenizer =lambda x : ' '.join(jieba.lcut(x)))\n\n\n\nmatrix=vectorizer.fit_transform(documents)\n\n# Obtaining the document-word matrix\ndoc_term_matrix=matrix.todense()\ndoc_term_matrix\n\n# Computing cosine similarity\ndf = pd.DataFrame(doc_term_matrix)\n\n\nprint(cosine_similarity(df,df))\n"},{"cell_type":"code","execution_count":203,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F13BA27D193F4B6A8D47C6F5CB72F372","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.60569101 0.62043724 0.59192176 0.62451717]\n"," [0.60569101 1.         0.75529928 0.71314498 0.75758034]\n"," [0.62043724 0.75529928 1.         0.69155057 0.78177146]\n"," [0.59192176 0.71314498 0.69155057 1.         0.73255858]\n"," [0.62451717 0.75758034 0.78177146 0.73255858 1.        ]]\n"]}],"source":"vectorizer = TfidfVectorizer(\n    stop_words= my_stopwords,\n    tokenizer =lambda x : ' '.join(jieba.lcut(x)))\n\n\n\nmatrix=vectorizer.fit_transform(documents)\n\n# 获取文档-词汇矩阵（document-word matrix）\ndoc_term_matrix=matrix.todense()\ndoc_term_matrix\n\n# 基于上述矩阵计算文档间的余弦相似度\ndf = pd.DataFrame(doc_term_matrix)\n\n\nprint(cosine_similarity(df,df))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"98EC61253B964CFEBE2C04BD924F1920","trusted":true},"source":"### 13. 如何计算文本间的soft cosine similarity（\"软性\"余弦值） ?   \n难度等级 : L4\n\nQ. 利用gensim中的计算文档间的\"软性\"余弦值。"},{"cell_type":"code","execution_count":225,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"12C293747B7B496883DD19CAA472395F","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3.8.0\n"]}],"source":"import gensim\nimport jieba\nfrom gensim.models import KeyedVectors\nfrom gensim.similarities import termsim\nfrom gensim import corpora\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings \nfilterwarnings('ignore') \nprint(gensim.__version__)"},{"cell_type":"code","execution_count":206,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"152DB3EF02A547BFB59D161E63681D49","trusted":true},"outputs":[],"source":"text1 = '涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求'\ntext2 = '垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量'\ntext3 = '情感分析：对用户评论等文本内容做情感分析，指导决策与运营'\ntext4 = '自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式'\ntext5 = '文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。'\n\ndocuments=[text1,text2,text3,text4,text5]"},{"cell_type":"code","execution_count":211,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"95528FACCE8A4F9798333C5309554BA6","trusted":true},"outputs":[],"source":"model = KeyedVectors.load_word2vec_format(r'E:\\bpemb模型文件\\zh\\zh.wiki.bpe.vs200000.d300.w2v.bin', binary=True)"},{"cell_type":"code","execution_count":226,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E9AD7FCD8EE84F9E8886A104ADFD5E8A","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.2575362240810525\n"]}],"source":"def simple_preprocess(text):\n    return jieba.lcut(text)\n\n#准备dictionary和corpus.\ndictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n\n# 计算文档相似度矩阵（similarity matrix）\nsimilarity_matrix = model.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n\n# 将上述文档转化为基于词袋表示的向量 \nsent_1 = dictionary.doc2bow(simple_preprocess(text1))\nsent_2 = dictionary.doc2bow(simple_preprocess(text2))\nsent_3 = dictionary.doc2bow(simple_preprocess(text3))\nsent_4 = dictionary.doc2bow(simple_preprocess(text4))\nsent_5 = dictionary.doc2bow(simple_preprocess(text5))\n \n\nsentences = [sent_1, sent_2, sent_3, sent_4, sent_5]\n\n# 计算其中两个语句的soft cosine similarity\nprint(softcossim(sent_1, sent_2, similarity_matrix))"},{"cell_type":"code","execution_count":231,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D1740E70AD6D40DBB03BD47B8A9FAE0B","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.00</td>\n","      <td>0.26</td>\n","      <td>0.36</td>\n","      <td>0.27</td>\n","      <td>0.46</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.26</td>\n","      <td>1.00</td>\n","      <td>0.37</td>\n","      <td>0.33</td>\n","      <td>0.43</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.36</td>\n","      <td>0.37</td>\n","      <td>1.00</td>\n","      <td>0.26</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.27</td>\n","      <td>0.33</td>\n","      <td>0.26</td>\n","      <td>1.00</td>\n","      <td>0.39</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.46</td>\n","      <td>0.43</td>\n","      <td>0.50</td>\n","      <td>0.39</td>\n","      <td>1.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0     1     2     3     4\n","0  1.00  0.26  0.36  0.27  0.46\n","1  0.26  1.00  0.37  0.33  0.43\n","2  0.36  0.37  1.00  0.26  0.50\n","3  0.27  0.33  0.26  1.00  0.39\n","4  0.46  0.43  0.50  0.39  1.00"]},"execution_count":231,"metadata":{},"output_type":"execute_result"}],"source":"#一口气算出所有文档之间的\"软性\"余弦值\n\ndef create_soft_cossim_matrix(sentences):\n\n    len_array = np.arange(len(sentences))\n    xx, yy = np.meshgrid(len_array, len_array)\n    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n    return cossim_mat\n\ncreate_soft_cossim_matrix(sentences)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2EC89960CE7343A2B4D46DA4AF670F37","trusted":true,"mdEditEnable":false},"source":"### 14.如何不通过词典找到某个词的相关词?   \n难度等级：L3   \nQ.利用词嵌入的手段找到某个词汇的相关词（近义词、反义词或者同语境词汇）。"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B75100BCB40B40C691981DB2F3514589","trusted":true},"outputs":[],"source":"from pymagnitude import *\nvectors  = Magnitude(r'E:\\2020.09.04 基于wiki的词嵌入wikipedia2vec-master\\百度百科.magnitude')"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7F25C0AF48E9496C8E2D35588F468BB3","trusted":true},"outputs":[{"data":{"text/plain":["[('语义', 0.8291653),\n"," ('超文本', 0.82207274),\n"," ('语言信息', 0.8203382),\n"," ('逻辑系统', 0.8198495),\n"," ('函数式', 0.8194275),\n"," ('关系数据库', 0.8177376),\n"," ('语言模型', 0.8131796),\n"," ('语义分析', 0.80611384),\n"," ('描述语言', 0.8052611),\n"," ('查询语言', 0.8032667)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"vectors.most_similar('自然语言')"},{"metadata":{"id":"5C4B1E2AE4A94BA29E0278E16C4CD51B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### pymagnitude的使用方法详见[<【实用】词嵌入模型利器---Pymagnitude的使用>](https://www.kesci.com/home/project/5e91f18ae7ec38002d01fdf3)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0F98B4A8BEAF4E668156FC4F7E4499B0","trusted":true,"mdEditEnable":false},"source":"### 15.如何度量两个词汇之间的相似度  \n难度等级：L3   \nQ.利用词汇之间的词向量的余弦值来度量词汇相似度。"},{"cell_type":"code","execution_count":20,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C9F2D9D6D8A64DC7B828CEF8EEFD676E","trusted":true},"outputs":[],"source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom pymagnitude import *\nvectors  = Magnitude(r'E:\\2020.09.04 基于wiki的词嵌入wikipedia2vec-master\\百度百科.magnitude')"},{"cell_type":"code","execution_count":21,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E763982FAA914B368BC572385FD1D981","trusted":true},"outputs":[],"source":"def sklearn_cosine(x, y):\n    return cosine_similarity(x, y)"},{"cell_type":"code","execution_count":27,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3E832135E4F746A4998640CE490E8984","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["'倚马千言'和'才思敏捷'之间的余弦相似度: 0.6707871\n","'倚马千言'和'倚门倚闾'之间的余弦相似度: 0.40876597\n","'倚门倚闾'和'才思敏捷'之间的余弦相似度: 0.34079546\n","'倚门千言'和'倚马七纸'之间的余弦相似度: 0.8365689950912402\n","'才思敏捷'和'倚马七纸'之间的余弦相似度: 0.6443239906284672\n","'倚门倚闾'和'倚马七纸'之间的余弦相似度: 0.5506272158696582\n"]}],"source":"word1 =  '倚马千言'\nword2 =  '才思敏捷'\nword3 =  '倚门倚闾'\nword4 =  '倚马七纸'\n\nvector1 = vectors.query(word1).reshape(1,-1)  \nvector2 = vectors.query(word2).reshape(1,-1) \nvector3 = vectors.query(word3).reshape(1,-1)\nvector4 = vectors.query(word4).reshape(1,-1)\n\nprint(\"'倚马千言'和'才思敏捷'之间的余弦相似度:\",sklearn_cosine(vector1 ,vector2)[0][0])\nprint(\"'倚马千言'和'倚门倚闾'之间的余弦相似度:\",sklearn_cosine(vector1 ,vector3)[0][0])\nprint(\"'倚门倚闾'和'才思敏捷'之间的余弦相似度:\",sklearn_cosine(vector3 ,vector2)[0][0])\nprint(\"'倚门千言'和'倚马七纸'之间的余弦相似度:\",sklearn_cosine(vector1 ,vector4)[0][0])\nprint(\"'才思敏捷'和'倚马七纸'之间的余弦相似度:\",sklearn_cosine(vector4 ,vector2)[0][0])\nprint(\"'倚门倚闾'和'倚马七纸'之间的余弦相似度:\",sklearn_cosine(vector3 ,vector4)[0][0])"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"78C511A56C844F2C8B3077AFB6B1DFD8","trusted":true},"source":"### 16.如何度量两句话之间的相似度？\n难度等级：L3   \nQ.通过度量两个文档向量之间的余弦夹角值来度量其语义相似度。"},{"cell_type":"code","execution_count":28,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"28B4EA6B5CE2403CB45758E901468A4B","trusted":true},"outputs":[],"source":"text1 = '文本挖掘实录：用文本挖掘剖析54万首诗歌'\ntext2 = '数据挖掘实操｜用文本挖掘剖析近5万首《全唐诗》'\ntext3 = '以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程'\ntext4 = '干货｜ 如何利用Social Listening从在线垂直社区提炼有价值的信息？'"},{"cell_type":"code","execution_count":32,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7631EFD8AAC74F32901A227148FEE9D1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.         0.90711536 0.80459625 0.70148463]\n"," [0.90711536 1.         0.80541531 0.73332658]\n"," [0.80459625 0.80541531 1.         0.72434468]\n"," [0.70148463 0.73332658 0.72434468 1.        ]]\n"]},{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', '#', '—', '③', '⑩', '万', '下', '两', '主', '乌', '二', '云', '些', '人', '什', '仅', '令', '位', '体', '余', '作', '使', '例', '便', '候', '假', '儿', '先', '光', '免', '关', '具', '再', '况', '切', '办', '加', '单', '反', '句', '叮', '后', '否', '呼', '哒', '哧', '唷', '啪', '喔', '嗡', '固', '外', '天', '好', '始', '家', '少', '尔', '尚', '岂', '年', '开', '徒', '怕', '总', '恰', '惟', '愿', '慢', '成', '抑', '拘', '换', '接', '方', '旁', '无', '旦', '易', '末', '极', '果', '样', '根', '次', '毋', '漫', '点', '然', '特', '独', '甚', '番', '登', '直', '相', '省', '看', '真', '知', '紧', '结', '继', '综', '罢', '肯', '致', '般', '莫', '见', '言', '许', '设', '话', '说', '诸', '越', '身', '达', '进', '述', '通', '遵', '鄙', '里', '鉴', '问', '间', '限', '面', '首', '\\ufeff', '［', '］'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]}],"source":"# 使用 sklearn中的CountVectorizer来获取文档的向量化表示\n\n\nimport jieba\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndocuments=[text1,text2,text3,text4]\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\nvectorizer = TfidfVectorizer(\n        stop_words= my_stopwords,\n        tokenizer =lambda x : ' '.join(jieba.lcut(x)))\n\n\nmatrix=vectorizer.fit_transform(documents)\n\n# 获取文档-词频矩阵 \ndoc_term_matrix=matrix.todense()\ndoc_term_matrix\n\n# 计算余弦相似值\ndf=pd.DataFrame(doc_term_matrix)\n\n\nprint(cosine_similarity(df,df))\n\n###两两相似度结果：\n\n# '文本挖掘实录：用文本挖掘剖析54万首诗歌'，'数据挖掘实操｜用文本挖掘剖析近5万首《全唐诗》' 0.90711536\n# '文本挖掘实录：用文本挖掘剖析54万首诗歌'，'以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程' 0.80459625\n# '数据挖掘实操｜用文本挖掘剖析近5万首《全唐诗》'，'以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程' 0.80541531\n# '以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程'，'干货｜ 如何利用Social Listening从在线垂直社区提炼有价值的信息？'0.72434468\n# '文本挖掘实录：用文本挖掘剖析54万首诗歌'，'干货｜ 如何利用Social Listening从在线垂直社区提炼有价值的信息？' 0.70148463"},{"metadata":{"id":"5B39024F4F1246CA8693214B7E6F17E1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"上述标题对应的文章详情见于微信公众号【Social Listening与文本挖掘】"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8A296F319B6C4833898F80672ECDE64A","trusted":true,"mdEditEnable":false},"source":"### 17. 如何计算给定的两段文本间的WMD距离（Word mover distance）?   \n难度等级: L3\n\nQ. 通过词嵌入模型计算两段文本间的WMD（word mover distance）值 。"},{"cell_type":"code","execution_count":55,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"89A1BEFD754C4C81842E427810D14B5D","trusted":true},"outputs":[],"source":"#通过gensim载入训练好的Word2Vec模型\nimport jieba    \nimport itertools  #用来进行两两语句对组合，且不重复\nfrom gensim.models import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format(r'E:\\bpemb模型文件\\zh\\zh.wiki.bpe.vs200000.d300.w2v.bin', binary=True)\n\nsent1 = '印媒:印度借边境局势紧张之际 在拉达克测试新武器'\nsent2 = \"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"\nsent3 = '印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”'\nsent4 = '印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中'\nsent5 = '印度提高边境桥梁承重级别 能向拉达克调动更多T90'\nsent6 ='四面楚歌!解放军在中印边境架大喇叭播放印度歌曲。'\n"},{"cell_type":"code","execution_count":56,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FE8F423BE1C542DA82BD87FEDE44F22D","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\"印媒:印度借边境局势紧张之际 在拉达克测试新武器\"和\"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"之间的WMD距离为：  6.296912388553118\n","\"印媒:印度借边境局势紧张之际 在拉达克测试新武器\"和\"印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”\"之间的WMD距离为：  6.919785525226737\n","\"印媒:印度借边境局势紧张之际 在拉达克测试新武器\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  5.452807771630273\n","\"印媒:印度借边境局势紧张之际 在拉达克测试新武器\"和\"印度提高边境桥梁承重级别 能向拉达克调动更多T90\"之间的WMD距离为：  5.425519964174957\n","\"印媒:印度借边境局势紧张之际 在拉达克测试新武器\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  5.452807771630273\n","\"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"和\"印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”\"之间的WMD距离为：  6.435367473178606\n","\"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  4.719169062967202\n","\"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"和\"印度提高边境桥梁承重级别 能向拉达克调动更多T90\"之间的WMD距离为：  6.6598001702179275\n","\"印度方向传来震耳枪响,一名士兵倒在血泊!军官承认已无能为力\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  4.719169062967202\n","\"印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  6.57703320804335\n","\"印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”\"和\"印度提高边境桥梁承重级别 能向拉达克调动更多T90\"之间的WMD距离为：  6.962845611805521\n","\"印度女演员宣称“为国家呐喊”,制片人怼“带几个人去打中国”\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  6.57703320804335\n","\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"和\"印度提高边境桥梁承重级别 能向拉达克调动更多T90\"之间的WMD距离为：  5.948305329113334\n","\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  0.0\n","\"印度提高边境桥梁承重级别 能向拉达克调动更多T90\"和\"印度边境附近军营传出枪响 一士兵举枪自尽倒在血泊中\"之间的WMD距离为：  5.948305329113334\n"]}],"source":"origin_sents = [sent1,sent2,sent3,sent4,sent5,sent4]\nsents = [jieba.lcut(i) for i in origin_sents]\n\n# 计算上述6个文档间两两文档的 WMD距离（word mover distance），数值越大，表明两个语句间的语义相似度越低\nfor i,j in zip(itertools.combinations(sents, 2),itertools.combinations(origin_sents, 2)):\n    print('\"{}\"和\"{}\"之间的WMD距离为： '.format(j[0],j[1]),model.wmdistance(*i))      "},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E4C86FF9B1914BF6939F5C500D4A7749","trusted":true},"source":"### 18.如何使用LSA模型抽取文本中的主题词？  \n \n难度等级 : L3\n\nQ. 通过sklearn中的TruncatedSVD进行LSA（潜在语义分析，Latent Semantic Analysis）操作，从大段文本/大量文档中抽取主题关键词(topic keywords) 。"},{"cell_type":"code","execution_count":44,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4AF1209C77DF4214AC91091EA18589A3","trusted":true},"outputs":[],"source":"import jieba\nfrom pyltp import SentenceSplitter\ndocs = '''9月9日，就在日本安倍政权接近尾声、新首相呼之欲出之际，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》，旨在通过印军和日本自卫队的后勤保障合作，加强双方协同能力和防务关系。根据协定，日本海上自卫队舰艇可以使用印度安达曼-尼科巴群岛的军事基地，印度海军则可以使用日本设在非洲之角国家吉布提的后勤保障基地。\n9月9日，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》。\n印度向来视印度洋为自家“后院”，伸入大洋的印度半岛有利于拓展其在印度洋的海上存在。连通苏伊士运河-红海-曼德海峡与马六甲海峡的北印度洋海域，又是海上航运繁忙的国际水道，贸易和战略价值十分重要，而印度恰好卡在这条航线的要冲地带。\n在北印度洋，印度本土位于中间位置，并在东部安达曼-尼科巴群岛上设有司令部和军事基地，扼守马六甲海峡的西端。而在西部方向上，印度此前并没有什么抓手。印日后勤保障协议达成后，印度海军就可以利用吉布提的日本自卫队后勤基地，实现在曼德海峡-亚丁湾这一关键水域的常态化存在。\n对日本来说，上述东西向航线中的任何一个节点，都事关海上贸易和能源安全，而这两项有都是关乎日本经济和国家安全的核心利益。能够利用印度安达曼-尼科巴群岛的基地，除了确保马六甲海峡航运安全，还可以利用其战略位置监视亚太地区其他大国西进印度洋的海上活动，进而充分发挥日本海上自卫队的实力，加强日本在美国印太战略中的关键作用。\n《相互提供物资与劳务协定》是印日相互借力，以兑现自身远洋战略诉求的一次利益交换。类似的交换，印度在今年6月也有过一次，合作方是澳大利亚。6月初，印度总理莫迪与澳大利亚总理莫里森举行视频会晤，将双边关系提升为全面战略伙伴关系，发表了涉及印太地区海上合作愿景的联合声明，签署了包括《后勤相互保障协定》在内的7项协议。该协议允许双方舰机在对方港口和基地补充燃料、进行维修。\n印澳、印日先后签署的后勤保障协议，被视为印太地区一个更广泛战略的组成部分，即以美国印太战略为总纲，以美日澳三角同盟关系为基础，通过美日澳印四角关系来构成印太战略的四大支点。\n如果没有印度参与，印太战略就会存在明显短板，是个“瘸腿”战略，因而特朗普政府近年来不断加强美印军事合作，试图将美日澳同盟拓展为美日澳印准军事联盟，而这正合印度心意。\n早在2016年8月，历经12年对话谈判，印度与美国签署《物流交换备忘录协定》。据此，印度可用美国设在吉布提、印度洋中部迪戈加西亚群岛、西太平洋关岛和菲律宾苏比克湾的基地，进行军事人员和装备的补给、维修和休整；美军舰机在必要时可使用印度的机场或港口。\n2018年9月初，印美外长和防长“2+2会谈”期间，双方签署《通信兼容与安全协议》，为美国向印度出口加密通信安全设备铺平道路，包括在出口印度的武器装备上安装美军通信系统。近日有报道称，印度已批准与美国签署《共享地理空间国防情报协议》。另外，再加上印美之间的《基本交流与合作协议》，四大军事合作协议使得印美两国形成了事实上的准军事联盟关系。\n特朗普政府的印太战略，其实是奥巴马时期“亚太再平衡”战略的扩展升级版，即突破亚太区域范畴，西进印度洋。这与印度近些年来推行的“东进战略”擦出火花，印度也一直想增加在亚太地区的存在感，尤其是通过插手地区热点敏感事务来提升自身影响力，以此彰显所谓大国地位。\n印美这种战略上的一拍即合，促使双方得以迅速推进军事合作，连带着印日、印澳军事关系也显著提升，美日澳印还定期举行的“马拉巴尔”海上联合军演，这四国也已近乎形成准军事联盟。在这组关系中，印度舰艇和军机的活动范围得到扩展，还能从美国等国买到更先进的武器装备，进一步充实被称为“大杂烩”的印军装备。\n2017年11月，时任新加坡国防部长黄永宏（后排左）访问印度，与时任印度国防部长西塔拉曼一起见证了两国《海上安全合作协议》的签署。\n为了对付歼-20，印度斥巨资从法国引进一批“阵风”战机，但仍然无法与歼-20对抗。“阵风”是一款多用途双发中型战斗机，航程、机动性本就不如歼-20，多达14个武器挂载点更说明完全不具备隐身能力，在与歼-20对阵时很有可能还没发现对方就被击落。放眼全球，能支持印度空军与歼-20对抗的只有F-35，印度为何不采购呢？\n根据JSF初始计划，F-35战机只能出售给参与研发的国家，按照财务支援、转移科技数量和分包合约确定获得战机的顺序。实现量产后，美国宣布扩大F-35出售范围，不仅项目参与国可以购买，一些未参与的友好国家也可获得购买资格，如印度、乌克兰。最新消息称，印度正在考虑引进F-35的利弊。\n印度是俄罗斯军火的忠实客户，空军建设也以俄制战机为主，2016年才与法国达成引进“阵风”战机协议，从未装备或使用过美制战机。从性价比看，印度引进“阵风”非常不划算，单架战机价格达到2.4亿美元，几乎是F-35量产后的三倍，这笔资金完全可以引进大约100架F-35。另外，“阵风”还无法与歼-20对抗，战斗力仅相当于歼-16，因此引进F-35是个非常不错的选择。\n不过，美国就出口F-35提出了一个非常苛刻的条件。由于印度未参与研发计划，需要在购机基础上增加一笔专利费，单机价格可能达到1.5亿美元，加上配套的武器、配件和地勤系统，以及训练飞行员的费用、运转费用，价格和采购“阵风”战机差不多。这是印度正在考虑的原因之一.\n原因之二是，引进F-35后，印度空军维护机型种类将达到8种，覆盖俄制、法制、美制和国产四国机型，还需要额外建立一条维护体系和人员培养系统，会给后勤保障系统增加更大的压力。\n原因之三是，印度正在进行AMCA战机研制计划。这是一款单座双发第五代隐形战斗机，用于取代现役的“幻影”2000和米格-29战机，前期已投入30亿美元研发费用，目前已经制造出模型，预计在2030年左右试飞。外形上，AMCA战机完全借鉴F-35，作战定位是填补LCA战机与苏-30MKI之间的空白，与F-35也非常接近。引进F-35意味着，印度必须在二者之间放弃一个，毫无疑问AMCA战机会被放弃。但是，印度对AMCA战机寄予厚望，在它身上投注了太多心血，突然间要放弃肯定是无法接受的。\n不过，印度的准军事联盟朋友圈不止于美日澳，近年来印度还与法国、韩国、新加坡等国签署了类似的后勤保障协议。比如，印新2017年11月签署了《海上安全合作协议》，相互提供海军设施和后勤支持，这样一来印度就可以利用位于马六甲海峡东端的新加坡樟宜海军基地进行补给休整，从而实现从西端到东端对马六甲海峡的全监控，并可借此插手南海。\n此外，印度与俄罗斯周年的《后勤互助协议》预计近期有望签署，这样一来印度就有可能利用俄方在北极地区的设施。印度与英国、越南的类似协议也在讨论中。但印度也有顾忌，这些军事合作既不能破坏自身外交自主和独立性，也不愿因此打破自己发起并奉行数十年的不结盟政策，同时还要在美俄等大国之间找平衡。'''\n\n#从大段落中划分出若干语句\nsentences = list(SentenceSplitter.split(docs))\n\n#载入停用词\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\n\n#分词和去停用词处理\nsentence_pro = [' '.join([w.strip() for w in jieba.lcut(s) if w not in my_stopwords]) for s in sentences if len(s) >1]"},{"cell_type":"code","execution_count":57,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"27DD7424B1564B6189B9D9500121E13C","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['③⑩'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]},{"name":"stdout","output_type":"stream","text":["Topic 0: \n","日本 战略 相互 签署 协定 协议 海上 提供 合作 印太  \n","Topic 1: \n","日本 相互 劳务 物资 协定 提供 大使 库马尔 秘书 铃木  \n","Topic 2: \n","印度洋 群岛 安达曼 尼科巴 使用 基地 马六甲海峡 海上 日本 军事基地  \n","Topic 3: \n","战略 印太 亚太 政府 特朗普 印度洋 澳印 美日 西进 日本  \n","Topic 4: \n","类似 交换 一次 今年 合作方 澳大利亚 英国 讨论 越南 战略  \n"]}],"source":"# 从sklearn中导入Tf-idf文本特征抽取器 \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# 定义Tf-idf文本特征抽取器\nvectorizer = TfidfVectorizer(stop_words=my_stopwords, max_features= 1000,  max_df = 0.4, smooth_idf=True)\n\n# 通过.fit_transform()方法将tokens转化为文档-词汇矩阵\nmatrix= vectorizer.fit_transform(sentence_pro)\n\n# 使用截断式SVD对文档-词汇矩阵进行分解，减小噪音\nfrom sklearn.decomposition import TruncatedSVD\nSVD_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=1000, random_state=2020)\nSVD_model.fit(matrix)\n\n# 获取词组（即terms，字、词或者复合词） \nterms = vectorizer.get_feature_names()\n\n# 对每个topic中的主题词进行轮询\nfor i, comp in enumerate(SVD_model.components_):\n    terms_comp = zip(terms, comp)\n    # 找到每个主题中最为重要的10主题关键词  \n    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n    print(\"Topic \"+str(i)+\": \")\n    #打印各个topic下的主题关键词\n    for t in sorted_terms:\n        print(t[0],end=' ')\n    print(' ')\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"70389B31DE7D4E05A0872DDE0ECB4936","trusted":true},"source":"### 19.如何利用LDA抽取文本中的主题关键词?  \n难度等级 : L3  \n\nQ.通过gensim中的LdaModel方法进行LDA（隐含狄利克雷分配，Latent dirichlet algorithm）文档主题生成建模，从大段文本/大量文档中抽取主题关键词(topic keywords) 。\n\n"},{"cell_type":"code","execution_count":61,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"099B3464C2964F94A2985A91F3FE2A07","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[(0,\n","  '0.042*\"印度\" + 0.040*\"海上\" + 0.035*\"马六甲海峡\" + 0.029*\"利用\" + 0.027*\"安全\" + '\n","  '0.020*\"东端\" + 0.020*\"月\" + 0.018*\"印度洋\"'),\n"," (1,\n","  '0.039*\"军事\" + 0.033*\"合作\" + 0.033*\"印美\" + 0.024*\"联盟\" + 0.024*\"准军事\" + '\n","  '0.024*\"关系\" + 0.024*\"形成\" + 0.024*\"海上\"'),\n"," (2,\n","  '0.054*\"印度\" + 0.030*\"9\" + 0.028*\"签署\" + 0.019*\"日本\" + 0.019*\"合作\" + 0.018*\"协定\" '\n","  '+ 0.018*\"相互\" + 0.016*\"协议\"'),\n"," (3,\n","  '0.053*\"战略\" + 0.037*\"印太\" + 0.032*\"关系\" + 0.021*\"-\" + 0.020*\"美国\" + 0.018*\"印度\" '\n","  '+ 0.018*\"印日\" + 0.016*\"协议\"'),\n"," (4,\n","  '0.065*\"印度\" + 0.029*\"战略\" + 0.019*\"存在\" + 0.016*\"基地\" + 0.016*\"吉布提\" + '\n","  '0.015*\"后勤保障\" + 0.014*\"群岛\" + 0.014*\"-\"')]\n"]}],"source":"# 载入 gensim, nltk\nimport gensim\nfrom gensim import models, corpora\nimport nltk\nfrom pprint import pprint\nfrom nltk.corpus import stopwords\nfrom gensim.models.ldamodel import LdaModel #针对文档-词频矩阵进行LDA主题建模\n\n\n\n# 直接使用上面case的文本数据\nall_tokens=[]\nfor text in sentence_pro:\n  tokens=[]\n  raw=nltk.wordpunct_tokenize(text)\n  for token in raw:\n    if token not in my_stopwords:\n        tokens.append(token)\n        all_tokens.append(tokens)\n\n# 创建一个字典（dictionary）和 矩阵（matrix）\ndictionary = corpora.Dictionary(all_tokens)\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in all_tokens]\n\n\nmodel = LdaModel(doc_term_matrix, num_topics=5, id2word = dictionary,passes=40)\n\npprint(model.print_topics(num_topics=6,num_words=8))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"11ADBA44965F4ABEA08C34928DA3BBEF","trusted":true},"source":"### 20. 如何利用NMF抽取主题关键词?\n难度等级: L3\n\n \nQ.通过sklearn中的NMF方法进行NMF(Non-negative Matrix Factorization method，即非负矩阵分解)主题建模，从大段文本/大量文档中抽取主题关键词(topic keywords) 。"},{"cell_type":"code","execution_count":65,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E3758E827EC74D0A9F59B01807228CAE","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Topic 1:\n","[('战略', 0.5710465480307177), ('印太', 0.4120080240327396), ('军事', 0.3799192311980819), ('合作', 0.3271671937070113), ('关系', 0.3244847080719927)]\n","Topic 2:\n","[('相互', 0.3148373405473465), ('劳务', 0.2821644203207381), ('物资', 0.2821644203207381), ('提供', 0.2615871113174627), ('协定', 0.25767147376352045)]\n","Topic 3:\n","[('日本', 0.3363009438607815), ('印度洋', 0.2472161316472932), ('群岛', 0.24382836527923335), ('海上', 0.2364101363497987), ('使用', 0.22996049774825428)]\n","Topic 4:\n","[('协议', 0.2810945801617884), ('后勤', 0.2216594458961478), ('签署', 0.19737396877859897), ('利用', 0.1873629686759039), ('东端', 0.17171924044548237)]\n","Topic 5:\n","[('类似', 0.49178867905266244), ('交换', 0.26342159884783045), ('越南', 0.25140768475820496), ('英国', 0.25140768475820496), ('讨论', 0.25140768475820496)]\n"]},{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['③⑩'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]}],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# 定义Tf-idf文本特征抽取器\nvectorizer = TfidfVectorizer(stop_words=my_stopwords, max_features= 1000,  max_df = 0.4, smooth_idf=True)\n\n# 通过.fit_transform()方法将tokens转化为文档-词汇矩阵\nnmf_matrix= vectorizer.fit_transform(sentence_pro)\n\nnmf_model = NMF(n_components=5)\nnmf_model.fit(nmf_matrix)\n\n# 定一个打印主题关键词的方法\ndef print_topics_nmf(model, vectorizer, top_n=5):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx+1))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]])\n        \nprint_topics_nmf(nmf_model,vectorizer)\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"59F0891C8BF549C489D48175E47880AC","trusted":true,"mdEditEnable":false},"source":"### 21.如何方便快捷的对一句话进行情感正负面分析？  \n难度等级 : L1\n\nQ. 使用SnowNLP对一句话进行情感正负面倾向分析。"},{"cell_type":"code","execution_count":73,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DF6CA070060C47B48322F37C179368EA","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["【印度卢比获选“最差货币”！千亿投资撤离，经济恐倒退20年！】的情感倾向为 *正面* \n"]}],"source":"from snownlp import SnowNLP\n\nsentence = '印度卢比获选“最差货币”！千亿投资撤离，经济恐倒退20年！'\ns = SnowNLP(sentence)\n\n\n\nprint('【{}】的情感倾向为 *{}* '.format(sentence,['正面' if s.sentiments>0.5 else '负面'][0]))   \n\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2FE57D54F11A4EEF82FBCE9C0193E89F","trusted":true},"source":"### 22.如何使用Doc2Vec来获取语句表示?  \n难度等级: L2\n\nQ.使用gensim中的Doc2Vec训练模型，来获取优于word2vec的语句表示。 "},{"cell_type":"code","execution_count":75,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"689B5C68DDF6446A88FC68F16E5892EE","trusted":true},"outputs":[],"source":"import jieba\nfrom pyltp import SentenceSplitter\ndocs = '''9月9日，就在日本安倍政权接近尾声、新首相呼之欲出之际，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》，旨在通过印军和日本自卫队的后勤保障合作，加强双方协同能力和防务关系。根据协定，日本海上自卫队舰艇可以使用印度安达曼-尼科巴群岛的军事基地，印度海军则可以使用日本设在非洲之角国家吉布提的后勤保障基地。\n9月9日，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》。\n印度向来视印度洋为自家“后院”，伸入大洋的印度半岛有利于拓展其在印度洋的海上存在。连通苏伊士运河-红海-曼德海峡与马六甲海峡的北印度洋海域，又是海上航运繁忙的国际水道，贸易和战略价值十分重要，而印度恰好卡在这条航线的要冲地带。\n在北印度洋，印度本土位于中间位置，并在东部安达曼-尼科巴群岛上设有司令部和军事基地，扼守马六甲海峡的西端。而在西部方向上，印度此前并没有什么抓手。印日后勤保障协议达成后，印度海军就可以利用吉布提的日本自卫队后勤基地，实现在曼德海峡-亚丁湾这一关键水域的常态化存在。\n对日本来说，上述东西向航线中的任何一个节点，都事关海上贸易和能源安全，而这两项有都是关乎日本经济和国家安全的核心利益。能够利用印度安达曼-尼科巴群岛的基地，除了确保马六甲海峡航运安全，还可以利用其战略位置监视亚太地区其他大国西进印度洋的海上活动，进而充分发挥日本海上自卫队的实力，加强日本在美国印太战略中的关键作用。\n《相互提供物资与劳务协定》是印日相互借力，以兑现自身远洋战略诉求的一次利益交换。类似的交换，印度在今年6月也有过一次，合作方是澳大利亚。6月初，印度总理莫迪与澳大利亚总理莫里森举行视频会晤，将双边关系提升为全面战略伙伴关系，发表了涉及印太地区海上合作愿景的联合声明，签署了包括《后勤相互保障协定》在内的7项协议。该协议允许双方舰机在对方港口和基地补充燃料、进行维修。\n印澳、印日先后签署的后勤保障协议，被视为印太地区一个更广泛战略的组成部分，即以美国印太战略为总纲，以美日澳三角同盟关系为基础，通过美日澳印四角关系来构成印太战略的四大支点。\n如果没有印度参与，印太战略就会存在明显短板，是个“瘸腿”战略，因而特朗普政府近年来不断加强美印军事合作，试图将美日澳同盟拓展为美日澳印准军事联盟，而这正合印度心意。\n早在2016年8月，历经12年对话谈判，印度与美国签署《物流交换备忘录协定》。据此，印度可用美国设在吉布提、印度洋中部迪戈加西亚群岛、西太平洋关岛和菲律宾苏比克湾的基地，进行军事人员和装备的补给、维修和休整；美军舰机在必要时可使用印度的机场或港口。\n2018年9月初，印美外长和防长“2+2会谈”期间，双方签署《通信兼容与安全协议》，为美国向印度出口加密通信安全设备铺平道路，包括在出口印度的武器装备上安装美军通信系统。近日有报道称，印度已批准与美国签署《共享地理空间国防情报协议》。另外，再加上印美之间的《基本交流与合作协议》，四大军事合作协议使得印美两国形成了事实上的准军事联盟关系。\n特朗普政府的印太战略，其实是奥巴马时期“亚太再平衡”战略的扩展升级版，即突破亚太区域范畴，西进印度洋。这与印度近些年来推行的“东进战略”擦出火花，印度也一直想增加在亚太地区的存在感，尤其是通过插手地区热点敏感事务来提升自身影响力，以此彰显所谓大国地位。\n印美这种战略上的一拍即合，促使双方得以迅速推进军事合作，连带着印日、印澳军事关系也显著提升，美日澳印还定期举行的“马拉巴尔”海上联合军演，这四国也已近乎形成准军事联盟。在这组关系中，印度舰艇和军机的活动范围得到扩展，还能从美国等国买到更先进的武器装备，进一步充实被称为“大杂烩”的印军装备。\n2017年11月，时任新加坡国防部长黄永宏（后排左）访问印度，与时任印度国防部长西塔拉曼一起见证了两国《海上安全合作协议》的签署。\n为了对付歼-20，印度斥巨资从法国引进一批“阵风”战机，但仍然无法与歼-20对抗。“阵风”是一款多用途双发中型战斗机，航程、机动性本就不如歼-20，多达14个武器挂载点更说明完全不具备隐身能力，在与歼-20对阵时很有可能还没发现对方就被击落。放眼全球，能支持印度空军与歼-20对抗的只有F-35，印度为何不采购呢？\n根据JSF初始计划，F-35战机只能出售给参与研发的国家，按照财务支援、转移科技数量和分包合约确定获得战机的顺序。实现量产后，美国宣布扩大F-35出售范围，不仅项目参与国可以购买，一些未参与的友好国家也可获得购买资格，如印度、乌克兰。最新消息称，印度正在考虑引进F-35的利弊。\n印度是俄罗斯军火的忠实客户，空军建设也以俄制战机为主，2016年才与法国达成引进“阵风”战机协议，从未装备或使用过美制战机。从性价比看，印度引进“阵风”非常不划算，单架战机价格达到2.4亿美元，几乎是F-35量产后的三倍，这笔资金完全可以引进大约100架F-35。另外，“阵风”还无法与歼-20对抗，战斗力仅相当于歼-16，因此引进F-35是个非常不错的选择。\n不过，美国就出口F-35提出了一个非常苛刻的条件。由于印度未参与研发计划，需要在购机基础上增加一笔专利费，单机价格可能达到1.5亿美元，加上配套的武器、配件和地勤系统，以及训练飞行员的费用、运转费用，价格和采购“阵风”战机差不多。这是印度正在考虑的原因之一.\n原因之二是，引进F-35后，印度空军维护机型种类将达到8种，覆盖俄制、法制、美制和国产四国机型，还需要额外建立一条维护体系和人员培养系统，会给后勤保障系统增加更大的压力。\n原因之三是，印度正在进行AMCA战机研制计划。这是一款单座双发第五代隐形战斗机，用于取代现役的“幻影”2000和米格-29战机，前期已投入30亿美元研发费用，目前已经制造出模型，预计在2030年左右试飞。外形上，AMCA战机完全借鉴F-35，作战定位是填补LCA战机与苏-30MKI之间的空白，与F-35也非常接近。引进F-35意味着，印度必须在二者之间放弃一个，毫无疑问AMCA战机会被放弃。但是，印度对AMCA战机寄予厚望，在它身上投注了太多心血，突然间要放弃肯定是无法接受的。\n不过，印度的准军事联盟朋友圈不止于美日澳，近年来印度还与法国、韩国、新加坡等国签署了类似的后勤保障协议。比如，印新2017年11月签署了《海上安全合作协议》，相互提供海军设施和后勤支持，这样一来印度就可以利用位于马六甲海峡东端的新加坡樟宜海军基地进行补给休整，从而实现从西端到东端对马六甲海峡的全监控，并可借此插手南海。\n此外，印度与俄罗斯周年的《后勤互助协议》预计近期有望签署，这样一来印度就有可能利用俄方在北极地区的设施。印度与英国、越南的类似协议也在讨论中。但印度也有顾忌，这些军事合作既不能破坏自身外交自主和独立性，也不愿因此打破自己发起并奉行数十年的不结盟政策，同时还要在美俄等大国之间找平衡。'''\n\n#从大段落中划分出若干语句\nsentences = list(SentenceSplitter.split(docs))\n\n#载入停用词\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\n\n#分词和去停用词处理\nsentence_pro = [' '.join([w.strip() for w in jieba.lcut(s) if w not in my_stopwords]) for s in sentences if len(s) >1]"},{"cell_type":"code","execution_count":78,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F67EB3FA6A8A4703BD3F4BEA4316123E","trusted":true},"outputs":[],"source":"# 载入模型\nfrom gensim.models import Doc2Vec\n\n# 按Doc2vec适配的数据格式进行训练，需要定义一个处理方法\ndef tagged_document(list_of_list_of_words):\n   for i, list_of_words in enumerate(list_of_list_of_words):\n      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n        \nmy_data = list(tagged_document(sentence_pro))\nmodel=Doc2Vec(my_data)\n\nsentence_representation = model.infer_vector(jieba.lcut('美日澳印还定期举行的“马拉巴尔”海上联合军演，这四国也已近乎形成准军事联盟'))"},{"cell_type":"code","execution_count":79,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AC4B0FF51C6641A6862CBA9632684F9B","trusted":true},"outputs":[{"data":{"text/plain":["array([-5.55146951e-03, -4.09128377e-04,  9.98128671e-03, -1.61631079e-03,\n","       -1.09040793e-02, -2.49802368e-03, -4.91932034e-03, -5.54430950e-03,\n","        1.29141295e-02, -9.19324253e-03,  4.41139610e-03, -6.34658104e-03,\n","       -8.95782746e-03, -6.13171235e-03,  8.09948146e-03,  4.25658328e-03,\n","        1.22247692e-02, -8.24749749e-03, -4.12754162e-05, -8.87597352e-03,\n","        2.86492519e-03,  6.13890064e-04, -7.60324649e-04, -4.54546791e-03,\n","        1.02009121e-02,  4.47121332e-04,  7.08227337e-04, -1.25148706e-03,\n","        1.30278906e-02, -9.68346093e-03,  2.10329308e-04, -4.23507160e-03,\n","       -1.01349987e-02,  5.45600243e-03,  2.20704190e-02, -1.26401130e-02,\n","        3.71520477e-03, -1.00920983e-02, -1.47518131e-03, -5.70378965e-03,\n","        8.39843298e-04,  3.75504629e-03,  4.44985367e-03,  1.81169121e-03,\n","       -6.83331955e-03, -1.48959563e-03, -3.53126298e-03, -5.08019421e-03,\n","       -6.06982736e-03,  4.50496003e-03,  8.71601794e-03,  1.12138491e-03,\n","        2.54865806e-03,  1.75907463e-03, -4.15994693e-03, -1.13981096e-02,\n","       -5.00217220e-03, -4.77492157e-03, -8.95258319e-03,  3.34604760e-03,\n","        1.09082181e-02,  6.95198122e-03, -6.30796349e-05, -5.02276700e-03,\n","       -5.85626205e-03,  3.67805548e-03, -2.08411249e-03,  1.30086055e-03,\n","       -2.58965592e-04, -1.42816349e-03, -1.95064058e-03,  9.74527095e-03,\n","       -3.28157144e-03,  1.66226830e-03,  3.49604385e-03,  2.82776961e-03,\n","        1.39003107e-03, -2.24572457e-02, -1.22418255e-02,  3.95327806e-03,\n","        5.22937579e-03,  5.18370327e-03,  1.05808033e-02,  1.48341735e-03,\n","        1.12122595e-02,  8.94360244e-03, -8.18125065e-03, -4.56358725e-03,\n","        1.04168681e-02,  1.48906596e-02, -9.77839809e-03,  1.18711200e-02,\n","        5.68252802e-03,  8.32767691e-03,  3.52392090e-03,  8.74217972e-03,\n","        7.91305490e-03, -6.71930658e-03,  6.49949070e-05, -1.00128038e-03],\n","      dtype=float32)"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":"sentence_representation"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8044B7BB854446768800002356D0F241","trusted":true},"source":"### 23.如何抽取某段文本的TF-IDF矩阵 ?\n难度等级 : L3\n\nQ. 使用sklearn或者gensim抽取文本的TF-IDF (Term Frequency -Inverse Document Frequency，即词频-逆向文件频率) 矩阵。"},{"cell_type":"code","execution_count":92,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F130FE9156D14D8B87F5B4EF992F7109","trusted":true},"outputs":[],"source":"import jieba\nfrom pyltp import SentenceSplitter\ndocs = '''9月9日，就在日本安倍政权接近尾声、新首相呼之欲出之际，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》，旨在通过印军和日本自卫队的后勤保障合作，加强双方协同能力和防务关系。根据协定，日本海上自卫队舰艇可以使用印度安达曼-尼科巴群岛的军事基地，印度海军则可以使用日本设在非洲之角国家吉布提的后勤保障基地。\n9月9日，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》。\n印度向来视印度洋为自家“后院”，伸入大洋的印度半岛有利于拓展其在印度洋的海上存在。连通苏伊士运河-红海-曼德海峡与马六甲海峡的北印度洋海域，又是海上航运繁忙的国际水道，贸易和战略价值十分重要，而印度恰好卡在这条航线的要冲地带。\n在北印度洋，印度本土位于中间位置，并在东部安达曼-尼科巴群岛上设有司令部和军事基地，扼守马六甲海峡的西端。而在西部方向上，印度此前并没有什么抓手。印日后勤保障协议达成后，印度海军就可以利用吉布提的日本自卫队后勤基地，实现在曼德海峡-亚丁湾这一关键水域的常态化存在。\n对日本来说，上述东西向航线中的任何一个节点，都事关海上贸易和能源安全，而这两项有都是关乎日本经济和国家安全的核心利益。能够利用印度安达曼-尼科巴群岛的基地，除了确保马六甲海峡航运安全，还可以利用其战略位置监视亚太地区其他大国西进印度洋的海上活动，进而充分发挥日本海上自卫队的实力，加强日本在美国印太战略中的关键作用。\n《相互提供物资与劳务协定》是印日相互借力，以兑现自身远洋战略诉求的一次利益交换。类似的交换，印度在今年6月也有过一次，合作方是澳大利亚。6月初，印度总理莫迪与澳大利亚总理莫里森举行视频会晤，将双边关系提升为全面战略伙伴关系，发表了涉及印太地区海上合作愿景的联合声明，签署了包括《后勤相互保障协定》在内的7项协议。该协议允许双方舰机在对方港口和基地补充燃料、进行维修。\n印澳、印日先后签署的后勤保障协议，被视为印太地区一个更广泛战略的组成部分，即以美国印太战略为总纲，以美日澳三角同盟关系为基础，通过美日澳印四角关系来构成印太战略的四大支点。\n如果没有印度参与，印太战略就会存在明显短板，是个“瘸腿”战略，因而特朗普政府近年来不断加强美印军事合作，试图将美日澳同盟拓展为美日澳印准军事联盟，而这正合印度心意。\n早在2016年8月，历经12年对话谈判，印度与美国签署《物流交换备忘录协定》。据此，印度可用美国设在吉布提、印度洋中部迪戈加西亚群岛、西太平洋关岛和菲律宾苏比克湾的基地，进行军事人员和装备的补给、维修和休整；美军舰机在必要时可使用印度的机场或港口。\n2018年9月初，印美外长和防长“2+2会谈”期间，双方签署《通信兼容与安全协议》，为美国向印度出口加密通信安全设备铺平道路，包括在出口印度的武器装备上安装美军通信系统。近日有报道称，印度已批准与美国签署《共享地理空间国防情报协议》。另外，再加上印美之间的《基本交流与合作协议》，四大军事合作协议使得印美两国形成了事实上的准军事联盟关系。\n特朗普政府的印太战略，其实是奥巴马时期“亚太再平衡”战略的扩展升级版，即突破亚太区域范畴，西进印度洋。这与印度近些年来推行的“东进战略”擦出火花，印度也一直想增加在亚太地区的存在感，尤其是通过插手地区热点敏感事务来提升自身影响力，以此彰显所谓大国地位。\n印美这种战略上的一拍即合，促使双方得以迅速推进军事合作，连带着印日、印澳军事关系也显著提升，美日澳印还定期举行的“马拉巴尔”海上联合军演，这四国也已近乎形成准军事联盟。在这组关系中，印度舰艇和军机的活动范围得到扩展，还能从美国等国买到更先进的武器装备，进一步充实被称为“大杂烩”的印军装备。\n2017年11月，时任新加坡国防部长黄永宏（后排左）访问印度，与时任印度国防部长西塔拉曼一起见证了两国《海上安全合作协议》的签署。\n为了对付歼-20，印度斥巨资从法国引进一批“阵风”战机，但仍然无法与歼-20对抗。“阵风”是一款多用途双发中型战斗机，航程、机动性本就不如歼-20，多达14个武器挂载点更说明完全不具备隐身能力，在与歼-20对阵时很有可能还没发现对方就被击落。放眼全球，能支持印度空军与歼-20对抗的只有F-35，印度为何不采购呢？\n根据JSF初始计划，F-35战机只能出售给参与研发的国家，按照财务支援、转移科技数量和分包合约确定获得战机的顺序。实现量产后，美国宣布扩大F-35出售范围，不仅项目参与国可以购买，一些未参与的友好国家也可获得购买资格，如印度、乌克兰。最新消息称，印度正在考虑引进F-35的利弊。\n印度是俄罗斯军火的忠实客户，空军建设也以俄制战机为主，2016年才与法国达成引进“阵风”战机协议，从未装备或使用过美制战机。从性价比看，印度引进“阵风”非常不划算，单架战机价格达到2.4亿美元，几乎是F-35量产后的三倍，这笔资金完全可以引进大约100架F-35。另外，“阵风”还无法与歼-20对抗，战斗力仅相当于歼-16，因此引进F-35是个非常不错的选择。\n不过，美国就出口F-35提出了一个非常苛刻的条件。由于印度未参与研发计划，需要在购机基础上增加一笔专利费，单机价格可能达到1.5亿美元，加上配套的武器、配件和地勤系统，以及训练飞行员的费用、运转费用，价格和采购“阵风”战机差不多。这是印度正在考虑的原因之一.\n原因之二是，引进F-35后，印度空军维护机型种类将达到8种，覆盖俄制、法制、美制和国产四国机型，还需要额外建立一条维护体系和人员培养系统，会给后勤保障系统增加更大的压力。\n原因之三是，印度正在进行AMCA战机研制计划。这是一款单座双发第五代隐形战斗机，用于取代现役的“幻影”2000和米格-29战机，前期已投入30亿美元研发费用，目前已经制造出模型，预计在2030年左右试飞。外形上，AMCA战机完全借鉴F-35，作战定位是填补LCA战机与苏-30MKI之间的空白，与F-35也非常接近。引进F-35意味着，印度必须在二者之间放弃一个，毫无疑问AMCA战机会被放弃。但是，印度对AMCA战机寄予厚望，在它身上投注了太多心血，突然间要放弃肯定是无法接受的。\n不过，印度的准军事联盟朋友圈不止于美日澳，近年来印度还与法国、韩国、新加坡等国签署了类似的后勤保障协议。比如，印新2017年11月签署了《海上安全合作协议》，相互提供海军设施和后勤支持，这样一来印度就可以利用位于马六甲海峡东端的新加坡樟宜海军基地进行补给休整，从而实现从西端到东端对马六甲海峡的全监控，并可借此插手南海。\n此外，印度与俄罗斯周年的《后勤互助协议》预计近期有望签署，这样一来印度就有可能利用俄方在北极地区的设施。印度与英国、越南的类似协议也在讨论中。但印度也有顾忌，这些军事合作既不能破坏自身外交自主和独立性，也不愿因此打破自己发起并奉行数十年的不结盟政策，同时还要在美俄等大国之间找平衡。'''\n\n#从大段落中划分出若干语句\nsentences = list(SentenceSplitter.split(docs))\n\n#载入停用词\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\n\n#分词和去停用词处理\ndoc_tokenized = [[w.strip() for w in jieba.lcut(s) if w not in my_stopwords] for s in sentences if len(s) >1]"},{"cell_type":"code","execution_count":99,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"130FDE9889DC4F93B1A4B1A14515DD2D","trusted":true},"outputs":[{"data":{"text/plain":["[(8, 0.0170149740424658),\n"," (37, 0.11546380233219157),\n"," (45, 0.09690055545816822),\n"," (54, 0.1198127494227414),\n"," (62, 0.22106225197145943),\n"," (63, 0.18189359767938695),\n"," (64, 0.22106225197145943),\n"," (65, 0.22106225197145943),\n"," (66, 0.22106225197145943),\n"," (67, 0.22106225197145943),\n"," (68, 0.22106225197145943),\n"," (69, 0.10355628909524209),\n"," (70, 0.18189359767938695),\n"," (71, 0.22106225197145943),\n"," (72, 0.22106225197145943),\n"," (73, 0.18189359767938695),\n"," (74, 0.22106225197145943),\n"," (75, 0.22106225197145943),\n"," (76, 0.18189359767938695),\n"," (77, 0.18189359767938695),\n"," (78, 0.22106225197145943),\n"," (79, 0.22106225197145943),\n"," (80, 0.18189359767938695),\n"," (81, 0.22106225197145943),\n"," (82, 0.22106225197145943),\n"," (83, 0.22106225197145943),\n"," (84, 0.14272494338731456)]"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":"# Method 1-使用gensim\nimport numpy as np\nfrom gensim import corpora\nfrom gensim.utils import simple_preprocess\n\ndictionary = corpora.Dictionary()\n\n# Creating the Bag of Words from the docs\nBoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n\n\ntfidf = models.TfidfModel(BoW_corpus)\n\n#获取其中一个语句的TF-IDF句向量表示\ntfidf[BoW_corpus[4]] "},{"cell_type":"code","execution_count":89,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"62FF96074B1544C69CE98378EB12B82C","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 97)\t0.14076663372332268\n","  (0, 493)\t0.19982631806229104\n","  (0, 401)\t0.18071125647638306\n","  (0, 130)\t0.19982631806229104\n","  (0, 154)\t0.15662912693506145\n","  (0, 123)\t0.18071125647638306\n","  (0, 164)\t0.1289187812744869\n","  (0, 170)\t0.14076663372332268\n","  (0, 301)\t0.18071125647638306\n","  (0, 138)\t0.18071125647638306\n","  (0, 302)\t0.19982631806229104\n","  (0, 131)\t0.14076663372332268\n","  (0, 124)\t0.16714890444630287\n","  (0, 351)\t0.16714890444630287\n","  (0, 282)\t0.15662912693506145\n","  (0, 360)\t0.1480338428603949\n","  (0, 379)\t0.11535642924440674\n","  (0, 491)\t0.18071125647638306\n","  (0, 203)\t0.18071125647638306\n","  (0, 239)\t0.18071125647638306\n","  (0, 370)\t0.18071125647638306\n","  (0, 181)\t0.16714890444630287\n","  (0, 140)\t0.12203006738951785\n","  (0, 48)\t0.19982631806229104\n","  (0, 175)\t0.19982631806229104\n","  :\t:\n","  (48, 455)\t0.4947400398888607\n","  (48, 415)\t0.4947400398888607\n","  (48, 381)\t0.4138356571598592\n","  (48, 132)\t0.2676127228185662\n","  (48, 140)\t0.15106408653616524\n","  (49, 468)\t0.240594214433829\n","  (49, 293)\t0.240594214433829\n","  (49, 32)\t0.240594214433829\n","  (49, 295)\t0.240594214433829\n","  (49, 208)\t0.240594214433829\n","  (49, 158)\t0.240594214433829\n","  (49, 266)\t0.240594214433829\n","  (49, 29)\t0.240594214433829\n","  (49, 353)\t0.240594214433829\n","  (49, 404)\t0.240594214433829\n","  (49, 198)\t0.240594214433829\n","  (49, 365)\t0.240594214433829\n","  (49, 33)\t0.240594214433829\n","  (49, 504)\t0.240594214433829\n","  (49, 236)\t0.21757936198240077\n","  (49, 47)\t0.20125006429932474\n","  (49, 103)\t0.1782352118478965\n","  (49, 204)\t0.20125006429932474\n","  (49, 164)\t0.15522035939646828\n","  (49, 140)\t0.07346311658441326\n"]}],"source":"# Method 2- 使用sklearn中TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the vectorizer to our text documents\nvectorizer = TfidfVectorizer()\nmatrix = vectorizer.fit_transform(sentence_pro)\nprint(matrix)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8B75883062E24AB5975E59646B5D54DA","trusted":true},"source":"### 24.如何使用gensim中的Phraser来抽取bigrams（双词组合） ?  \n难度等级: L3\n\nQ. 基于统计方法从大段文本中抽取常用词组（至少两个词汇复合而成）表达。"},{"cell_type":"code","execution_count":102,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"015B5241ACA44097A9538C089018C4E5","trusted":true},"outputs":[],"source":"import jieba\nfrom pyltp import SentenceSplitter\ndocs = '''9月9日，就在日本安倍政权接近尾声、新首相呼之欲出之际，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》，旨在通过印军和日本自卫队的后勤保障合作，加强双方协同能力和防务关系。根据协定，日本海上自卫队舰艇可以使用印度安达曼-尼科巴群岛的军事基地，印度海军则可以使用日本设在非洲之角国家吉布提的后勤保障基地。\n9月9日，印度国防秘书库马尔与日本驻印度大使铃木哲签署了《相互提供物资与劳务协定》。\n印度向来视印度洋为自家“后院”，伸入大洋的印度半岛有利于拓展其在印度洋的海上存在。连通苏伊士运河-红海-曼德海峡与马六甲海峡的北印度洋海域，又是海上航运繁忙的国际水道，贸易和战略价值十分重要，而印度恰好卡在这条航线的要冲地带。\n在北印度洋，印度本土位于中间位置，并在东部安达曼-尼科巴群岛上设有司令部和军事基地，扼守马六甲海峡的西端。而在西部方向上，印度此前并没有什么抓手。印日后勤保障协议达成后，印度海军就可以利用吉布提的日本自卫队后勤基地，实现在曼德海峡-亚丁湾这一关键水域的常态化存在。\n对日本来说，上述东西向航线中的任何一个节点，都事关海上贸易和能源安全，而这两项有都是关乎日本经济和国家安全的核心利益。能够利用印度安达曼-尼科巴群岛的基地，除了确保马六甲海峡航运安全，还可以利用其战略位置监视亚太地区其他大国西进印度洋的海上活动，进而充分发挥日本海上自卫队的实力，加强日本在美国印太战略中的关键作用。\n《相互提供物资与劳务协定》是印日相互借力，以兑现自身远洋战略诉求的一次利益交换。类似的交换，印度在今年6月也有过一次，合作方是澳大利亚。6月初，印度总理莫迪与澳大利亚总理莫里森举行视频会晤，将双边关系提升为全面战略伙伴关系，发表了涉及印太地区海上合作愿景的联合声明，签署了包括《后勤相互保障协定》在内的7项协议。该协议允许双方舰机在对方港口和基地补充燃料、进行维修。\n印澳、印日先后签署的后勤保障协议，被视为印太地区一个更广泛战略的组成部分，即以美国印太战略为总纲，以美日澳三角同盟关系为基础，通过美日澳印四角关系来构成印太战略的四大支点。\n如果没有印度参与，印太战略就会存在明显短板，是个“瘸腿”战略，因而特朗普政府近年来不断加强美印军事合作，试图将美日澳同盟拓展为美日澳印准军事联盟，而这正合印度心意。\n早在2016年8月，历经12年对话谈判，印度与美国签署《物流交换备忘录协定》。据此，印度可用美国设在吉布提、印度洋中部迪戈加西亚群岛、西太平洋关岛和菲律宾苏比克湾的基地，进行军事人员和装备的补给、维修和休整；美军舰机在必要时可使用印度的机场或港口。\n2018年9月初，印美外长和防长“2+2会谈”期间，双方签署《通信兼容与安全协议》，为美国向印度出口加密通信安全设备铺平道路，包括在出口印度的武器装备上安装美军通信系统。近日有报道称，印度已批准与美国签署《共享地理空间国防情报协议》。另外，再加上印美之间的《基本交流与合作协议》，四大军事合作协议使得印美两国形成了事实上的准军事联盟关系。\n特朗普政府的印太战略，其实是奥巴马时期“亚太再平衡”战略的扩展升级版，即突破亚太区域范畴，西进印度洋。这与印度近些年来推行的“东进战略”擦出火花，印度也一直想增加在亚太地区的存在感，尤其是通过插手地区热点敏感事务来提升自身影响力，以此彰显所谓大国地位。\n印美这种战略上的一拍即合，促使双方得以迅速推进军事合作，连带着印日、印澳军事关系也显著提升，美日澳印还定期举行的“马拉巴尔”海上联合军演，这四国也已近乎形成准军事联盟。在这组关系中，印度舰艇和军机的活动范围得到扩展，还能从美国等国买到更先进的武器装备，进一步充实被称为“大杂烩”的印军装备。\n2017年11月，时任新加坡国防部长黄永宏（后排左）访问印度，与时任印度国防部长西塔拉曼一起见证了两国《海上安全合作协议》的签署。\n为了对付歼-20，印度斥巨资从法国引进一批“阵风”战机，但仍然无法与歼-20对抗。“阵风”是一款多用途双发中型战斗机，航程、机动性本就不如歼-20，多达14个武器挂载点更说明完全不具备隐身能力，在与歼-20对阵时很有可能还没发现对方就被击落。放眼全球，能支持印度空军与歼-20对抗的只有F-35，印度为何不采购呢？\n根据JSF初始计划，F-35战机只能出售给参与研发的国家，按照财务支援、转移科技数量和分包合约确定获得战机的顺序。实现量产后，美国宣布扩大F-35出售范围，不仅项目参与国可以购买，一些未参与的友好国家也可获得购买资格，如印度、乌克兰。最新消息称，印度正在考虑引进F-35的利弊。\n印度是俄罗斯军火的忠实客户，空军建设也以俄制战机为主，2016年才与法国达成引进“阵风”战机协议，从未装备或使用过美制战机。从性价比看，印度引进“阵风”非常不划算，单架战机价格达到2.4亿美元，几乎是F-35量产后的三倍，这笔资金完全可以引进大约100架F-35。另外，“阵风”还无法与歼-20对抗，战斗力仅相当于歼-16，因此引进F-35是个非常不错的选择。\n不过，美国就出口F-35提出了一个非常苛刻的条件。由于印度未参与研发计划，需要在购机基础上增加一笔专利费，单机价格可能达到1.5亿美元，加上配套的武器、配件和地勤系统，以及训练飞行员的费用、运转费用，价格和采购“阵风”战机差不多。这是印度正在考虑的原因之一.\n原因之二是，引进F-35后，印度空军维护机型种类将达到8种，覆盖俄制、法制、美制和国产四国机型，还需要额外建立一条维护体系和人员培养系统，会给后勤保障系统增加更大的压力。\n原因之三是，印度正在进行AMCA战机研制计划。这是一款单座双发第五代隐形战斗机，用于取代现役的“幻影”2000和米格-29战机，前期已投入30亿美元研发费用，目前已经制造出模型，预计在2030年左右试飞。外形上，AMCA战机完全借鉴F-35，作战定位是填补LCA战机与苏-30MKI之间的空白，与F-35也非常接近。引进F-35意味着，印度必须在二者之间放弃一个，毫无疑问AMCA战机会被放弃。但是，印度对AMCA战机寄予厚望，在它身上投注了太多心血，突然间要放弃肯定是无法接受的。\n不过，印度的准军事联盟朋友圈不止于美日澳，近年来印度还与法国、韩国、新加坡等国签署了类似的后勤保障协议。比如，印新2017年11月签署了《海上安全合作协议》，相互提供海军设施和后勤支持，这样一来印度就可以利用位于马六甲海峡东端的新加坡樟宜海军基地进行补给休整，从而实现从西端到东端对马六甲海峡的全监控，并可借此插手南海。\n此外，印度与俄罗斯周年的《后勤互助协议》预计近期有望签署，这样一来印度就有可能利用俄方在北极地区的设施。印度与英国、越南的类似协议也在讨论中。但印度也有顾忌，这些军事合作既不能破坏自身外交自主和独立性，也不愿因此打破自己发起并奉行数十年的不结盟政策，同时还要在美俄等大国之间找平衡。'''\n\n#从大段落中划分出若干语句\nsentences = list(SentenceSplitter.split(docs))\n\n#载入停用词\nmy_stopwords =  [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()]\n\n#分词和去停用词处理\nsentence_stream = [[w.strip() for w in jieba.lcut(s) if w not in my_stopwords] for s in sentences if len(s) >1]"},{"cell_type":"code","execution_count":114,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"993990A28E6F469997A2C9B8CF3EA3C6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['9_月', '9_日', '印度_国防', '秘书_库马尔', '日本_驻', '印度_大使', '铃木_哲', '签署_相互', '提供_物资', '劳务_协定']\n","['日本_海上', '印度_安达曼', '-_尼科巴', '印度_海军']\n","['9_月', '9_日', '印度_国防', '秘书_库马尔', '日本_驻', '印度_大使', '铃木_哲', '签署_相互', '提供_物资', '劳务_协定']\n","['印度洋_海上']\n","['曼德_海峡', '北_印度洋']\n","['北_印度洋', '安达曼_-', '尼科巴_群岛']\n","[]\n","['后勤保障_协议', '印度_海军', '曼德_海峡']\n","[]\n","['印度_安达曼', '-_尼科巴', '西进_印度洋', '日本_海上', '美国_印太']\n","['相互_提供', '物资_劳务']\n","[]\n","['印太_地区']\n","[]\n","['后勤保障_协议', '印太_地区', '美国_印太', '美日_澳印', '印太_战略']\n","['印太_战略', '特朗普_政府', '军事_合作', '美日_澳印', '准军事_联盟']\n","['2016_年', '美国_签署']\n","[]\n","[]\n","[]\n","['称_印度', '美国_签署']\n","['合作_协议', '军事_合作', '准军事_联盟']\n","['特朗普_政府', '印太_战略', '西进_印度洋']\n","[]\n","['军事_合作', '美日_澳印', '准军事_联盟']\n","[]\n","['2017_年', '11_月', '海上_安全', '合作_协议']\n","['歼_-', '阵风_战机', '无法_歼', '-_20']\n","['歼_-', '歼_-']\n","['印度_空军', '歼_-', '20_对抗', 'F_-']\n","['F_-', '参与_研发']\n","['量产_后', 'F_-', '未_参与']\n","['称_印度', '正在_考虑', '引进_F', '-_35']\n","['印度_俄罗斯', '2016_年', '引进_阵风']\n","['引进_阵风', 'F_-', '量产_后', 'F_-']\n","['无法_歼', '-_20', '歼_-', '引进_F', '-_35']\n","['F_-']\n","['未_参与', '阵风_战机']\n","['印度_正在']\n","['引进_F', '-_35', '印度_空军']\n","['印度_正在', 'AMCA_战机']\n","[]\n","['AMCA_战机', 'F_-', 'F_-', '35_非常']\n","['引进_F', '-_35', 'AMCA_战机']\n","['AMCA_战机']\n","['准军事_联盟', '后勤保障_协议']\n","['2017_年', '11_月', '海上_安全', '合作_协议', '相互_提供', '这样一来_印度']\n","['印度_俄罗斯', '这样一来_印度']\n","[]\n","['军事_合作']\n"]}],"source":"# 从gensim中导入Phraser\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\n\n\n# 创建一个bigram phraser\nbigram = Phrases(sentence_stream, min_count=1, threshold=8, delimiter=b'_')\nbigram_phraser = Phraser(bigram)\n\nfor sent in sentence_stream:\n    tokens_ = bigram_phraser[sent]\n    print([i.strip() for i in tokens_ if i.find('_')>0])  #看看经由bigram phraser产生了哪些新的词组"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"271898A334C74AA98143401DE2130B80","trusted":true},"source":"### 25.如实使用nltk中的ngrams来创建bigrams、trigrams?\n难度等级: L3\n\nQ. 利用nktk中华的ngrams方法从大段文本中抽取bigrams 和trigrams（三个词汇复合而成的词汇） 。"},{"cell_type":"code","execution_count":121,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B797E88DBAE14BE789A672356969F6C8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" Bigrams are:\n"," [('协定', '日本'), ('日本', '海上'), ('海上', '自卫队'), ('自卫队', '舰艇'), ('舰艇', '使用'), ('使用', '印度'), ('印度', '安达曼'), ('安达曼', '-'), ('-', '尼科巴'), ('尼科巴', '群岛'), ('群岛', '军事基地'), ('军事基地', '印度'), ('印度', '海军'), ('海军', '使用'), ('使用', '日本'), ('日本', '设在'), ('设在', '非洲之角'), ('非洲之角', '国家'), ('国家', '吉布提'), ('吉布提', '后勤保障'), ('后勤保障', '基地')]\n","------------------------------------------------------------------------------------------------------------------------\n"," Trigrams are:\n"," [('协定', '日本', '海上'), ('日本', '海上', '自卫队'), ('海上', '自卫队', '舰艇'), ('自卫队', '舰艇', '使用'), ('舰艇', '使用', '印度'), ('使用', '印度', '安达曼'), ('印度', '安达曼', '-'), ('安达曼', '-', '尼科巴'), ('-', '尼科巴', '群岛'), ('尼科巴', '群岛', '军事基地'), ('群岛', '军事基地', '印度'), ('军事基地', '印度', '海军'), ('印度', '海军', '使用'), ('海军', '使用', '日本'), ('使用', '日本', '设在'), ('日本', '设在', '非洲之角'), ('设在', '非洲之角', '国家'), ('非洲之角', '国家', '吉布提'), ('国家', '吉布提', '后勤保障'), ('吉布提', '后勤保障', '基地')]\n"]}],"source":"# 创建 bigrams and trigrams\nfrom nltk import ngrams\nbigram=list(ngrams(sentence_stream[1],2))\ntrigram=list(ngrams(sentence_stream[1],3))\n\nprint(\" Bigrams are:\\n\",bigram)\nprint(\"---\"*40)\nprint(\" Trigrams are:\\n\", trigram)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AE4C96ACB70A420282E7B25DD86CC83E","trusted":true},"source":"### 26.如何利用bert抽取语句表示进行语句相似度比较   \n难度等级：L6   \nQ.使用BERT模型抽取文本中各个token的嵌入表示，取其中隐藏层的特征，将其平均，得到句向量。"},{"cell_type":"code","execution_count":9,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"180906818392453BBE92FFBEBB2EFF07","trusted":true},"outputs":[],"source":"import numpy as np\nimport torch\nfrom transformers import BertModel, BertTokenizer\nfrom scipy.spatial.distance import cosine\n\ndef get_word_indeces(tokenizer, text, word):\n    '''\n    确定 \"text \"中与 \"word \"相对应的标记的index或indeces。`word`可以由多个字词复合而成，如 \"数据分析（数据+分析）\"。\n    \n    确定indeces是很棘手的，因为一个词汇可能会被分解成多个token。\n    我用一种比较迂回的方法解决了这个问题--我用一定数量的`[MASK]`的token代替`word`，然后在词条化（tokenization）结果中找到这些token。\n    '''\n    # 将'word'词条化--它可以被分解成多个词条(token)或子词（subword）\n    word_tokens = tokenizer.tokenize(word)\n\n    # 创建一个\"[MASK]\"词条序列来代替 \"word\"\n    masks_str = ' '.join(['[MASK]']*len(word_tokens))\n\n    #将\"word\"替换为 mask词条\n    text_masked = text.replace(word, masks_str)\n\n    # `encode`环节同时执行如下功能:\n    #   1. 将文本词条化\n    #   2. 将词条映射到其对应的id\n    #   3. 增加特殊的token，主要是 [CLS] 和 [SEP]  \n    input_ids = tokenizer.encode(text_masked)\n\n    # 使用numpy的`where`函数来查找[MASK]词条的所有indeces\n    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n\n    return mask_token_indeces\n\ndef get_embedding(b_model, b_tokenizer, text, word=''):\n    '''\n    使用指定的model和tokenizer对喂进来的文本和词进行句嵌入或者词汇语境嵌入输出。\n    '''\n\n    # 如果提供了一个词，找出与之对应的token\n    if not word == '':\n        word_indeces = get_word_indeces(b_tokenizer, text, word)\n\n    # 对文本进行编码，添加(必要的!)特殊token，并转换为PyTorch tensors\n    encoded_dict = b_tokenizer.encode_plus(\n                        text,                      # 待encode的文本\n                        add_special_tokens = True, # 增加特殊token ，加在句首和句尾添加'[CLS]' 和 '[SEP]'\n                        return_tensors = 'pt',     # 返回的数据格式为pytorch tensors \n                )\n\n    input_ids = encoded_dict['input_ids']\n    \n    b_model.eval()\n\n    # 通过模型运行经编码后的文本以获得hidden states\n    bert_outputs = b_model(input_ids)\n    \n    # 通过BERT运行经编码后的文本，集合所有12层产生的所有隐藏状态\n    with torch.no_grad():\n\n        outputs = b_model(input_ids)\n\n        # 根据之前`from_pretrained`调用中的配置方式，评估模型将返回不同数量的对象。\n        # 在这种情况下，因为我们设置了`output_hidden_states = True`，\n        # 第三项将是所有层的隐藏状态。更多细节请参见文档。\n        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n        \n        hidden_states = outputs[2]\n\n    # `hidden_states`的shape是 [13 x 1 x <文本长度> x 768]\n\n    # 选择第二层到最后一层的嵌入，`token_vecs` 是一个形如[<文本长度> x 768]的tensor\n    token_vecs = hidden_states[-2][0]\n\n    # 计算所有token向量的平均值\n    sentence_embedding = torch.mean(token_vecs, dim=0)\n\n    # 将上述token平均嵌入向量转化为numpy array\n    sentence_embedding = sentence_embedding.detach().numpy()\n\n   # 如果提供了`word`，计算其token的嵌入。\n    if not word == '':\n        # 假如是词长大于等于2的词汇，取`word`T中token嵌入的平均值\n        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)\n\n        # 转化为numpy array \n        word_embedding = word_embedding.detach().numpy()\n    \n        return (sentence_embedding, word_embedding)\n    else:\n        return sentence_embedding"},{"cell_type":"code","execution_count":36,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8E44E228FF4740198A8B5ACB0C13EDA0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","BERT Similarity:\n","  sim(query, A): 0.9736\n","  sim(query, B): 0.7728\n","  sim(query, C): 0.922\n"]}],"source":"bert_model = BertModel.from_pretrained(r\"E:\\2020.09.07 pytorch_pretrained_models\\bert-sim-chinese\",\n                                  output_hidden_states=True)\nbert_tokenizer = BertTokenizer.from_pretrained(r\"E:\\2020.09.07 pytorch_pretrained_models\\bert-sim-chinese\")\n\n \n\ntext_query = \"如何针对用户群体进行数据分析\"\n\ntext_A = \"基于25W+知乎数据，我挖掘出这些人群特征和内容偏好\"\ntext_B = \"揭开微博转发传播的规律：以“人民日报”发布的G20文艺晚会微博为例\"\ntext_C ='''不懂数理和编程，如何运用免费的大数据工具获得行业洞察？'''\n\n\n# 使用BERT获取各语句的向量表示\nemb_query = get_embedding(bert_model, bert_tokenizer, text_query)\nemb_A = get_embedding(bert_model, bert_tokenizer, text_A)\nemb_B = get_embedding(bert_model, bert_tokenizer, text_B)\nemb_C = get_embedding(bert_model, bert_tokenizer, text_C)\n\n# 计算query和各语句的相似余弦值（cosine similarity）\nsim_query_A = 1 - cosine(emb_query, emb_A)\nsim_query_B = 1 - cosine(emb_query, emb_B)\nsim_query_C = 1 - cosine(emb_query, emb_C)\n\nprint('')\nprint('BERT Similarity:')\nprint('  sim(query, A): {:.4}'.format(sim_query_A))\nprint('  sim(query, B): {:.4}'.format(sim_query_B))\nprint('  sim(query, C): {:.4}'.format(sim_query_C))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0F08126EA2444E7B8BD62A2C875FD17E","trusted":true},"source":"### 27.如何对古代汉语进行分词？\n难度等级： L3   \nQ.使用jiayan（甲言）对古代汉语（包括诗歌、文言文）进行分词，其中涉及利用无监督、无词典的N元语法和隐马尔可夫模型进行古汉语自动分词。  \n同时，利用词库构建功能产生的文言词典，基于有向无环词图、句子最大概率路径和动态规划算法进行分词。  "},{"cell_type":"code","execution_count":122,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B7B7C0B9A70B4A0FA6F5D1FF32D3D6EA","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['万物', '生', '芸芸', '，', '与我', '本', '同', '气', '。', '氤氲', '随', '所', '感', '，', '形体', '偶然', '异', '。', '丘岳', '孰', '为', '高', '，', '尘', '粒', '孰', '为', '细', '。', '忘', '物', '亦', '忘我', '，', '优游', '何', '所', '觊', '。']\n"]}],"source":"from jiayan import load_lm\nfrom jiayan import CharHMMTokenizer\n \ntext = '''万物生芸芸，与我本同气。氤氲随所感，形体偶然异。丘岳孰为高，尘粒孰为细。忘物亦忘我，优游何所觊。'''\n\nlm = load_lm(r'D:\\jiayan_models\\jiayan.klm')\ntokenizer = CharHMMTokenizer(lm)\nprint(list(tokenizer.tokenize(text)))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6F529F3417C2466480EF48F3C76D1BAB","trusted":true},"source":"### 28.如何对古代汉语进行自动化词库构建？\n难度等级：L5\nQ.jiayan中利用无监督的双字典树、点互信息以及左右邻接熵进行古代汉语词库自动化构建。  "},{"cell_type":"code","execution_count":124,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CE4F1FC08D7948D18A3784F7C47064BE","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trie building time: 2.5439980030059814\n","Computation time: 2.946000099182129\n","Word filtering: 0.33702635765075684\n"]}],"source":"from jiayan import PMIEntropyLexiconConstructor\n\nconstructor = PMIEntropyLexiconConstructor()\nlexicon = constructor.construct_lexicon(r'C:\\Users\\Administrator\\Desktop\\【精华】诗歌文本挖掘项目集\\data\\poems_clean.txt')\nconstructor.save(lexicon, '诗歌词库自动构建.csv')  #输出词库"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"351AB0D216F2403F8AE15F7DFDD82467","trusted":true},"source":"### 29.如何自动生成诗歌？\n难度等级：L2  \nQ.如何使用transformers中的pipeline（基于GPT2）生成诗歌。"},{"cell_type":"code","execution_count":12,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D1ABB1A0F6FE422D96306BB51CD90A96","trusted":true},"outputs":[],"source":"from transformers import GPT2LMHeadModel, BertTokenizerFast,pipeline \nmodel = GPT2LMHeadModel.from_pretrained('gaochangkuan/model_dir', pad_token_id=tokenizer.eos_token_id)\ntokenizer = BertTokenizerFast.from_pretrained( \"gaochangkuan/model_dir\")  #自动下载vocab.txt\n\nnlp =pipeline('text-generation', model = model,tokenizer =tokenizer  ) #自动下载模型\n"},{"cell_type":"code","execution_count":132,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C864C72DF21A48B288AB06BD7F92E7AA","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"]},{"data":{"text/plain":["[{'generated_text': '<s>阳光明媚雪初消，天上人间第一桥。万国衣冠朝北极，九重宫阙拱东朝。风云际会瞻龙衮，日月昭回听凤箫。圣主恩深何以报，愿将歌颂答舜尧。</s>'}]"]},"execution_count":132,"metadata":{},"output_type":"execute_result"}],"source":"nlp('阳光明媚')     #七言律诗，64个字（含标点），超过该字数则无效"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E555BB9989314219931676749B2E0BAC","trusted":true},"source":"### 30.如何基于关键词的布尔逻辑组合（与、或、非）来精确检索信息？\n难度等级：L4  \nQ.利用eldar进行关键词组合来检索查询信息。"},{"cell_type":"code","execution_count":1,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B66855E9EB9546DA99213C4303FE3C4A","trusted":true},"outputs":[{"data":{"text/plain":["['科普:人工智能和大数据是如何联系在一起的?',\n"," '大数据,为人工智能提供了深度学习的数据,促进人工智能快速发展',\n"," 'ai的“新基建”的高科技主导权必须掌握在中国人手里',\n"," '全国人大代表刘庆峰:让大数据和ai一起为师生减负',\n"," '华米科技ai创新大会公布!借力ai,华米将在健康领域继续发力',\n"," 'ai to decode future!华米科技将召开,ai创新大会解构未来']"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"from eldar import build_query\n\n\n# build list\ndocuments = [\n    \"打造人工智能产业高地 泉山区牵手矿大开启校地合作新篇章\",\n    \"全国人工智能版图:广东一枝独秀,山东胜过浙江\",\n    \"科普:人工智能和大数据是如何联系在一起的?\",\n    \"联动六城 共享科技盛宴!2020科大讯飞人工智能云展会重磅亮相\",\n    \"大数据,为人工智能提供了深度学习的数据,促进人工智能快速发展\",\n    '人工智能学习如何实现从0到1?刘鹏教授主编的《人工智能》是好帮手!',\n    '政协委员刘伟:人工智能“新基建”的主导权必须掌握在中国人手里',\n    'ai的“新基建”的高科技主导权必须掌握在中国人手里',\n    '全国人大代表刘庆峰:让大数据和AI一起为师生减负',\n    '华米科技AI创新大会公布!借力AI,华米将在健康领域继续发力',\n    'AI to Decode Future!华米科技将召开,AI创新大会解构未来',\n    '有ai加持，浙江绍兴的智慧城市建设会更加顺畅~'\n]\n\nkeywords ='(\"大数据\"|\"数据\"|\"科技\") + ( \"AI\"|\"人工智能\") '  #关键词组合逻辑\nfilter_word =  '(\"浙江\")'  #排除词逻辑\n\neldar = build_query(keywords.replace('|',' OR ').replace('+',' AND ') + 'NOT ' + filter_word.replace('|',' OR '))\n\n\ndocuments = [i.lower() for i in documents]\neldar.filter(documents)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EDE4C1A79F6B499AA0A6CC2E5C33BEDF","trusted":true,"mdEditEnable":false},"source":"### 催更请留言、评论\n# To be continued~"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C7D0848A6D9E4EB780910ACDB534BF65","trusted":true,"mdEditEnable":false},"source":"### 鄙喵的微信公众号为【Social Listening与文本挖掘】，欢迎关注！"},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1B236FCCEBCD4EC1B231EA78FD07A918","trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}