本文直译自《hands on ML》课后题。（有改动的以【】表示）。

## 1.用He-initialization随机得到一个值之后，所有的weight都用这个值进行初始化，这样是否可以？

不可以，所有的weight需要独立地采样，不能有同样的初始值。随机采样weight值的一个重要目的是打破对称性；如果所有的weight都有相同的初始值，即使这个值不是0，它们的对称性依然存在（即所某一层的所有神经元是相同的），反向传播无法打破这种情况（同一层权重相同）。这意味着同一层里的所有神经元总是有相同的权重。这就像每一层只有一个神经元，而且更慢。这种设置几乎不可能得到一个好的结果。

## 2.可以把bias初始化为0么？

可以。有些人喜欢像初始化weight一样初始化bias，这也可以，二者没有太大的区别。

## 3.ELU激活函数相对于ReLU激活函数有哪些优势？

（a）ELU可以取到负值，因此任一层的神经元的平均输出比用ReLU（从不输出负值）更接近于0。这缓和了梯度消失（vanishing gradients）问题。
（b）ELU总是有非0导数，这避免了可以影响ReLU的dying units问题。
（c）ELU函数的任何位置都平滑（smooth），而ReLU的斜率在z=0处从0直接跳到1.这个突变会减缓梯度下降因为算法会在z=0附近跳跃。

## 4.什么情况下应该用以下激活函数：ELU、leaky ReLU（和它的变种）、ReLU、tanh、logistics、softmax？

（a）ELU是一个好的默认选项。
（b）如果希望神经网络尽量快，可以用leaky ReLU的变种之一（例如用默认超参数的leaky ReLU）。
（c）ReLU的简洁性使得它很受欢迎，虽然它的效果一般不如ELU和leaky ELU。不过ReLU可以精确输出0的能力在某些情况下有用。
（d）tanh当输出层需要输出-1~1之间值的时候有用，但是现在它在隐层中已经用的不多了。
（e）logistics函数在输出层需要输出概率的时候有用（例如二分类），但是在隐层中用的也很少（变分自编码器的coding layer除外）。
（f）softmax在输出层需要输出互斥类别的概率的时候有用，除此之外隐层中很少用。

## 5.当用一个MomentumOptimizer的时候，如果动量超参数（momentum hyperparameter）过于接近1（例如0.99999），结果会怎样？

算法可能会提高很多速度，向全局最低点方向移动，但是由于它的动量，它会越过最低点。然后它会减慢速度然后返回来，再次加速再次超过最低点。收敛之前会重复多次，因此总的来说比一个更小的动量值花费更多时间。

## 6.有哪些方法可以得到一个稀疏的模型（即多数weight等于0）？

（a）正常训练模型，然后把那些很小的权重置为0。
（b）如果需要更稀疏，可以在训练中应用L1正则项，这会使模型稀疏化。
（c）把L1正则化和dual averaging相结合，用tensorflow的FTRLOptimizer类。

## 7.dropout会减慢训练么？会减慢推断（inference：新样本上预测）么？

dropout会减慢训练过程，一般（训练时间）大约会是两倍。然而，它对inference没有影响，因为只是在训练中才turned on。