# 机器学习思考题目——10神经网络基础

本文直译自《hands on ML》课后题。（有改动的以【】表示）

## 1.用原始人工神经元（为组件）画出一个ANN的示意图，来实现操作 A ⊕ B ：⊕表示异或操作XOR。提示：A ⊕ B = (A∧ ¬ B) ∨ (¬ A ∧ B).

下图是用原始人工神经元为基础的神经网络，实现异或操作，基于公式A ⊕ B = (A ∧ ¬ B)
∨ (¬ A ∧ B)。图形还有其他若干解法，例如基于公式 A ⊕ B =(A ∨ B) ∧ ¬(A ∧ B)或者 A ⊕ B = (A ∨ B) ∧ (¬ A ∨ ∧ B)等等。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190311190241856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2ODEwMzk4,size_16,color_FFFFFF,t_70)【补充——画图方法——不同情况示意图】
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190311190256777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2ODEwMzk4,size_16,color_FFFFFF,t_70)

## 2.一般而言，为什么Logistics Regression分类器比经典感知机（i.e.，classical Perceptron，a single layer of linear threshold units trained using the Perceptron training algorithm）更好？怎样调整（tweak）感知机，使其等价于Logistics Regression分类器？

（1）经典感知机只有在数据集线性可分的情况下才收敛，而且它不能估计类的概率。数据集线性不可分的时候，LR分类器也可以收敛到一个好的解，同时LR会输出类的概率。
（2）如果把感知机的激活函数改为logistics函数（如果有多个输出神经元，则改为softmax函数），用梯度下降来训练（或者其他能最小化损失函数（例如交叉熵）的优化算法），这样感知机就等价于LR分类器。

## 3.为什么logistics激活函数是训练MLP的关键成分（key ingredient）？

logistics激活函数是训练MLP中的关键成分，是因为它的导数总是非零的，因此梯度下降总可以从斜坡（slope）向下滑动（roll down the slope）。当激活函数是阶梯函数的时候，梯度下降不能移动，因为根本就没有斜坡（slope）。
【说明：原题干是训练 ‘first MLPs’，没理解，所以作了修改】

## 4.列举三种常用的激活函数，你能画出图形么？

（a）阶梯函数（step function）
（b）logistics函数：公式为 σ(z) =1 / (1 + exp(–z))
（c）双曲正切函数（hyperbolic tangent）：
tanh (z) = 2σ(2z) – 1=2 / (1+exp(-2z))-1=(1-exp(-2z)) / (1+exp(-2z))
（d）整流线性单元（ReLU函数）：ReLU (z) = max (0, z)（简化版公式）
（e）ELU函数
（f ）LeakyReLU
部分激活函数及其导数图形如下（只包含阶梯函数、logistics、Tanh、ReLU）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190311190310471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2ODEwMzk4,size_16,color_FFFFFF,t_70)

## 5.假设一个MLP组成如下：输入层有10个输入神经元（passthrough neurons），后面有50个神经元的隐层，最后是3个神经元的输出层。所有的神经元（不包括输入层的）都是用ReLU激活函数：

**（a）输入矩阵（input matrix）X的形状是什么？
（b）隐层weight向量Wh的形状是什么？隐层bias向量bh的形状是什么？
（c）输出层weight向量Wo的形状是什么？输出层bias向量bo形状是什么？
（d）这个网络的输出矩阵Y的形状是什么？
（e）计算Y的关于 X、Wh、bh、Wo、bo的计算公式是什么？**

***ANSWER:***
（a）输入矩阵X的形状是 m × 10，m表示训练batch中样本数量；
（b）Wh形状是 10 × 50；bh的长度是50；
（c）Wo的形状是 50 × 3，bo长度是3；
（d）输出矩阵Y的形状是 m × 3；
（e）Y=ReLU(ReLU(X*Wh+bh)*Wo+bo)【原答案是Y=(X*Wh+bh)*Wo+bo,貌似有点问题】

## 6.如果想把邮件分为垃圾邮件和正常邮件，输出层中需要几个神经元，输出层中应该用什么激活函数？如果是处理MNIST数据集，输出层中应该用几个神经元，什么激活函数？对于房价预测问题，输出层中该用几个神经元，什么激活函数？

（1）邮件分为为二分类：输出层需要1个神经元，可以用logistics激活函数；
（2）MNIST是多（10）分类：输出层需要10个神经元，用softmax激活函数；
（3）房价预测是回归问题：输出层需要1个神经元，输出层中不需要激活函数。说明：如果目标值之间数量级跨度很大，可以对目标值取对数之后的值作为目标值，对于模型的输出，可以再取指数得到原值，即利用exp ( log ( v ) )=v 。

## 7.什么是反向传播（backpropagation），它是怎么工作的？反向传播和reverse-mode自动微分的区别是什么？

【反向传播】反向传播是用来训练人工神经网络（ANN）的技术。它首先计算损失函数关于每个模型参数（所有的weight和bias）的梯度，然后用这些梯度进行梯度下降。（模型训练）一般需要用很多训练batch进行上千次甚至百万次反向传播，直到模型参数收敛到损失函数的最小点（理想上的）。
【Reverse-mode自动微分】为了计算梯度，反向传播运用了reverse-mode自动微分（尽管当反向传播发明的时候，它的称呼并不是这个，而且目前它已经被改造过若干次）。Reverse-mode自动微分向前穿过计算图，计算每个节点的在当前训练batch下的数值，然后它反向穿过计算图，一次计算出所有的梯度。
【二者的区别】那他们的区别是什么呢？反向传播是指应用多次[反向传播步骤]训练神经网络的全过程，每一次[反向传播步骤]中计算梯度然后用它们来进行梯度下降。reverse-mode自动微分只是高效计算梯度的一种技术，它恰巧被反向传播所用。

## 8. 能否列出MLP中所有可以调节的超参数？怎样调节超参数来解决过拟合问题？

（1）可以调节的超参数：
（a）隐层的数目；
（b）每个隐层的神经元的数目；
（c）所有隐层和输出层的激活函数（一般而言：RELU是隐层激活函数好的默认选项；二分类用logistics激活函数；多分类用softmax；回归问题不需要激活函数）；
（2）过拟合的时候可以尝试减少隐层的数目和每个隐层的神经元的数目。