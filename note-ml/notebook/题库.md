[BERT-based Models](http://localhost:3000/#/./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT?id=_3-bert-based-models) ：

1.预处理训练模型 BertForPreTraining 模型源码的如何MLM 和 NSP

\2. [ BertForSequenceClassification](http://localhost:3000/#/./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT?id=_32-bertforsequenceclassification)：用于句子分类（也可以是回归）任务 代码的研究本地运行

\3. [BertForMultipleChoice](http://localhost:3000/#/./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT?id=_33-bertformultiplechoice)：用于多项选择，如 RocStories/SWAG 任务 

\4. [BertForTokenClassification](http://localhost:3000/#/./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT?id=_34-bertfortokenclassification)：序列标注（词分类），如 NER 任务代码学习:

\5. [BertForQuestionAnswering](http://localhost:3000/#/./篇章3-编写一个Transformer模型：BERT/3.2-如何应用一个BERT?id=_35-bertforquestionanswering)：解决问答任务，例如 SQuAD 任务代码的学习

问题2: BERT模型使用哪种分词方式？

 回答：主要有两个分词器：BasicTokenizer 和 WordpieceTokenizer，另外一个 FullTokenizer 是这两个的结合：先进行 BasicTokenizer 得到一个分得比较粗的 token 列表，然后再对每个 token 进行一次 WordpieceTokenizer，得到最终的分词结果。

- 问题3: 如何理解BERT模型输入的type ids？
- 问题4: Hugginface代码中的BasicTokenizer作用是？
- 问题5: WordPiece分词的好处是什么？
- 问题6: BERT中的warmup作用是什么？