# 梯度下降（Gradient Descent）

在求解机器学习算法的模型参数，即[无约束优化问题](https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/opt-uncons.html)时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。



## 1. 梯度

在微积分里面，对多元函数的参数求$∂$偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是**梯度**。

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f\(x,y\), 分别对x,y求偏导数，求得的梯度向量就是$$\left ( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y} \right )^T$$,简称grad $$f(x,y)$$。对于在点$$(x_0,y_0)$$的具体梯度向量就是$$\left ( \frac{\partial f}{\partial x_0},\frac{\partial f}{\partial y_0} \right )^T$$.，如果是3个参数的向量梯度，就是$$\left ( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z}\right )^T$$,以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f\(x,y\),在点$$(x_0,y_0)$$，沿着梯度向量的方向就是$$\left ( \frac{\partial f}{\partial x_0},\frac{\partial f}{\partial y_0} \right )^T$$的方向是f\(x,y\)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$$-\left ( \frac{\partial f}{\partial x_0},\frac{\partial f}{\partial y_0} \right )^T$$的方向，梯度减少最快，也就是更加容易找到函数的最小值。

## 2. 梯度下降与梯度上升

机器学习算法中

- 求最小化损失函数，- 通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。

- 求最大化损失函数  - 通过梯度上升法来迭代求解。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数$$f(\theta)$$的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 $$-f(\theta)$$的最大值，这时梯度上升法就派上用场了。

## 3. 梯度下降法算法详解

### 3.1 梯度下降的直观解释

首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。



![img](.\images\1042406-20161017221342935-1872962415.png)

### 3.2 梯度下降的相关概念

---

在详细了解梯度下降的算法之前，我们先看看相关的一些概念。

1. 步长(Learning rate): 步长决定了梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。
2. 特征（feature）: 指的是样本中输入部分，比如样本$(x_0,y_0),(x_1,y_1)$,则样本特征为$X$，样本输出为$Y$。
3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$$h_{\theta}(x)$$。比如对于样本$$(x_i,y_i)(i=1,2,...n)$$,可以采用拟合函数如下：


$$
   h_{\theta}(x) = \theta_0+\theta_1x
$$

4. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本$$ (x_i,y_i)(i=1,2,...n)$$,采用线性回归，损失函数为：


$$
    J(\theta_0,\theta_1)=\sum_{i=1}^{m}(h_\theta(x_i)-y_i)^2
$$


其中$$x_i$$表示样本特征x的第i个元素，$$y_i$$表示样本输出y的第i个元素，$$h_{\theta}(x_i)$$为假设函数。

### 3.3 梯度下降的详细算法

梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。

#### 3.3.1 梯度下降法的代数方式描述

梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。

#### 3.3.1 梯度下降法的代数方式描述

1. 先决条件： 确认优化模型的假设函数和损失函数。  
   比如对于线性回归，假设函数表示为


$$
   h_\theta(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
$$


, 其中$$\theta_i (i = 0,1,2... n)$$为模型参数，$$x_i(i = 0,1,2... n)$$为每个样本的n个特征值。这个表示可以简化，我们增加一个特征$$x_0=1$$ ，这样


$$
   h_\theta(x_0,x_1...x_n)=\sum_{i=0}^{n}\theta_ix_i
$$


。同样是线性回归，对应于上面的假设函数，损失函数为：

1. 算法相关参数初始化：主要是初始化$$\theta_0,\theta_1...,\theta_n$$,算法终止距离ε以及步长$$\alpha$$。在没有任何先验知识的时候，我喜欢将所有的$$\theta$$初始化为0， 将步长初始化为1。在调优的时候再优化。

2. 算法过程：  
   1）确定当前位置的损失函数的梯度，对于$$\theta$$,其梯度表达式如下：
   $$
      \frac{\partial }{\partial \theta_i}J(\theta_0,\theta_1...,\theta_n)
   $$
   
   
   2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即
   
   
   $$
      \alpha \frac{\partial }{\partial \theta_i}J(\theta_0,\theta_1,....\theta_n)
   $$
   
   
   对应于前面登山例子中的某一步。  
      3）确定是否所有的$$\theta_i$$,梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的$$\theta_i(i=0,1,...n)$$即为最终结果。否则进入步骤4.  
      4）更新所有的$$\theta$$，对于$$\theta_i$$，其更新表达式如下。更新完毕后继续转入步骤1.
   
   
   $$
       \theta_i=\theta_i-\alpha \frac{\partial}{\partial \theta_i}J(\theta_0,\theta_1,....\theta_n)
   $$
   
   
   下面用线性回归的例子来具体描述梯度下降。假设我们的样本是
   
   
   $$
   (x^{(0)}_1,x^{(0)}_2,...x^{(0)}_n,y_0),(x^{(1)}_1,x^{(1)}_2,...x^{(1)}_n,y_1),...(x^{(m)}_1,x^{(m)}_2,...x^{(m)}_n,y_n)
   $$



,损失函数如前面先决条件所述：


$$
   J(\theta_0,\theta_1...,\theta_n)=\sum_{i=0}^{m}(h_\theta(x_0,x_1,...x_n)-y_i)^2
$$


。  
   则在算法过程步骤1中对于$$\theta_i$$ 的偏导数计算如下:


$$
   \frac {\partial } {\theta_i}J(\theta_0,\theta_1...,\theta_n)=\frac {1}{m}\sum_{j=0}^{m}(h_\theta(x^j_0,x^j_1,...x^j_n)-y^j)x^j_i
$$


由于样本中没有

$$x_0$$上式中令所有的$$x^j_0$$为1.

步骤4中$$\theta_i$$的更新表达式如下：


$$
\theta_i=\theta_i-\alpha \frac{1}{m}\sum_{j=0}^{m}(h_\theta(x^j_0,x^j_1,...x^j_n)-y_j)x^j_i
$$


从这个例子可以看出当前点的梯度方向是由所有的样本决定的，加1/m 是为了好理解。由于步长也为常数，他们的乘机也为常数，所以这里α1/m可以用一个常数表示。

在下面第4节会详细讲到的梯度下降法的变种，他们主要的区别就是对样本的采用方法不同。这里我们采用的是用所有样本。

#### 3.3.2 梯度下降法的矩阵方式描述

1. 先决条件：  
   对于线性回归，假设函数


$$
   h_\theta(x_1,x_2,...x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
$$


的矩阵表达方式为：


$$
   h_\theta(x)=X\theta
$$


，其中， 假设函数$$h_\theta(X)$$为mx1的向量,$$\theta$$为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。  
   损失函数的表达式为：


$$
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
$$
, 其中Y是样本的输出向量，维度为mx1.

1. 算法相关参数初始化:  
   θ向量可以初始化为默认值，或者调优后的值。算法终止距离ε，步长α。

2. 算法过程：  
   1）确定当前位置的损失函数的梯度，对于θ向量,其梯度表达式如下：$$\frac {\partial}{\theta}J(\theta)$$

2）用步长乘以损失函数的梯度，得到当前位置下降的距离，即$$\alpha\frac {\partial}{\theta}J(\theta)$$对应于前面登山例子中的某一步。  
   3）确定θ向量里面的每个值,梯度下降的距离都小于ε，如果小于ε则算法终止，当前θ向量即为最终结果。否则进入步骤4.  
   4）更新θ向量，其更新表达式如下。更新完毕后继续转入步骤1.


$$
   \theta=\theta-\alpha\frac {\partial}{\theta}J(\theta)
$$

还是用线性回归的例子来描述具体的算法过程。

损失函数对于θ向量的偏导数计算如下：


$$
\frac {\partial}{\theta}J(\theta)=X^T(X\theta-Y)
$$


步骤4中θ向量的更新表达式如下：


$$
\theta=\theta-\alpha X^T(X\theta-Y)
$$


可以看到矩阵法要简洁很多。这里面用到了矩阵求导链式法则，和两个矩阵求导的公式。

公式1：$$\frac {\partial}{X}(XX^T)=2X$$  
公式2：$$\frac {\partial}{\theta}(X\theta)=X^T$$

如果需要熟悉矩阵求导建议参考张贤达的《矩阵分析与应用》一书。

## 资料

​    1 .  https://shunliz.gitbooks.io/machine-learning/content/math/analytic/gradient_descent.html

2. https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/opt-uncons.html#opt-uncons-graddesc